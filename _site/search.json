[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Statistical Analysis",
    "section": "",
    "text": "Welcome to this R workshop reference website! Here you will find all information covered in the R workshop in both a website form and a downloadable PDF form for easy reference. I hope you found the R workshop helpful and I encourage everyone to provide feedback so that I can improve should I run other R workshops in the future."
  },
  {
    "objectID": "data_clean.html",
    "href": "data_clean.html",
    "title": "Data Visualization and Management",
    "section": "",
    "text": "install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\nThe above code is how you want to start any R script. You always want to install and load in any packages that you may need in order to run analyses. For this part of the R workshop we will be working with what is called the tidyverse package. It’s essentially the go to array of packages in R for data science needs (and therefore a good portion of our needs as well)\n\n\n\n\n\n\nA Note On Packages & library() Function\n\n\n\nYou only need to install a package once (unless you update your version of R). The package gets stored on your local computer. A library() function call imports the installed package from your local storage. Further, you only need to call the library() function once per R script"
  },
  {
    "objectID": "data_clean.html#installing-the-tidyverse-package",
    "href": "data_clean.html#installing-the-tidyverse-package",
    "title": "Data Visualization and Management",
    "section": "",
    "text": "install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\nThe above code is how you want to start any R script. You always want to install and load in any packages that you may need in order to run analyses. For this part of the R workshop we will be working with what is called the tidyverse package. It’s essentially the go to array of packages in R for data science needs (and therefore a good portion of our needs as well)\n\n\n\n\n\n\nA Note On Packages & library() Function\n\n\n\nYou only need to install a package once (unless you update your version of R). The package gets stored on your local computer. A library() function call imports the installed package from your local storage. Further, you only need to call the library() function once per R script"
  },
  {
    "objectID": "data_clean.html#working-with-the-dplyr-package-data-manipulation",
    "href": "data_clean.html#working-with-the-dplyr-package-data-manipulation",
    "title": "Data Visualization and Management",
    "section": "Working With The dplyr Package (Data Manipulation)",
    "text": "Working With The dplyr Package (Data Manipulation)\n\n1library(tidyverse)\nlibrary(skimr)\n\n2dplyr_data &lt;- dplyr::starwars\n\n\n1\n\nCall the tidyverse packages\n\n2\n\nWe will be using the starwars data set for the dplyr tutorial. I’ve assigned it to the variable dplyr_data here.\n\n\n\n\n\n1skim(dplyr_data)\n\n\n1\n\nWe can view some of the key variable data using the skim()\n\n\n\n\n\nData summary\n\n\nName\ndplyr_data\n\n\nNumber of rows\n87\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nlist\n3\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nname\n0\n1.00\n3\n21\n0\n87\n0\n\n\nhair_color\n5\n0.94\n4\n13\n0\n12\n0\n\n\nskin_color\n0\n1.00\n3\n19\n0\n31\n0\n\n\neye_color\n0\n1.00\n3\n13\n0\n15\n0\n\n\nsex\n4\n0.95\n4\n14\n0\n4\n0\n\n\ngender\n4\n0.95\n8\n9\n0\n2\n0\n\n\nhomeworld\n10\n0.89\n4\n14\n0\n48\n0\n\n\nspecies\n4\n0.95\n3\n14\n0\n37\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\nfilms\n0\n1\n24\n1\n7\n\n\nvehicles\n0\n1\n11\n0\n2\n\n\nstarships\n0\n1\n17\n0\n5\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nheight\n6\n0.93\n174.36\n34.77\n66\n167.0\n180\n191.0\n264\n▁▁▇▅▁\n\n\nmass\n28\n0.68\n97.31\n169.46\n15\n55.6\n79\n84.5\n1358\n▇▁▁▁▁\n\n\nbirth_year\n44\n0.49\n87.57\n154.69\n8\n35.0\n52\n72.0\n896\n▇▁▁▁▁\n\n\n\n\n\n\n1head(dplyr_data)\n\n\n1\n\nWe can also use the head() function which simply gives you a print out of the first 5 rows of a data set.\n\n\n\n\n\nRecoding Variables\nOne variable when looking at the starwars data set might be sex. Here we can see it is coded as both a character and as either male, female or NA. For a simple recode we might wish to\n\nTransform the variables into a factor\nChange the naming convention to maybe 1, 0 and Unknown\n\nWe can achieve this with the code below\n\ndplyr_data &lt;- dplyr_data %&gt;% \n1  mutate(sex = as.factor(sex),\n2         sex = recode(sex,\n                      'male' = '0',\n                      'female' = '1'))\n\n\n1\n\nTo recode the variable sex we need to use the mutate() function and as.factor() functions as shown above\n\n2\n\nTo recode the values for male and female to 0 and 1 respectively, we need to use the recode() function as shown here. ::: {.callout-tip} ### A Note On %&gt;% Operator You may have noticed this %&gt;% operator. This is a handy operator that essentially takes the data on the left hand side and “pipe”s it into whatever is on the right as the first argument. This is most effective when the right hand function is expecting some form of a data set :::\n\n\n\n\n\n\n\n\n\n\nA Note On dplyr::recode() Function\n\n\n\nThe recode() function in the dplyr package uses what is called OLD to NEW syntax. This just means that when renaming variables as shown here, you want to list the original variable name followed by you new desired variable name\n\n\n\n\nCreating Variables\nCreating variables in R can be done a couple of ways. One is a little clunky (from a code perspective) and the other is more elegant. I’ll cover the more clunky way first followed by the more elegant way second. I’ll illustrate this by creating a variable that takes the mass variable from the starwars data set and reduces it by 10 units\n\n1dplyr_data$mass_10a &lt;- dplyr_data$mass - 10\n\n\n2dplyr_data &lt;- dplyr_data %&gt;%\n  mutate(mass_10b = mass - 10)\n\n\n1\n\nThis ways is relatively simply because you can think of it as a simple formula notation. However, it’s a little clunky because typically adding a $ operator is considered poor coding practice\n\n2\n\nThe more elegant way to create a variable is to simply again use the mutate() function\n\n\n\n\n\n\n\n\n\n\nA Note On the $ Operator\n\n\n\nThe $ operator simply says from the data set on the left of the operator, please find (or create) the variable on the right. In this case, from the dplyr_data data, create the variable mass_10a\n\n\n\n\nFiltering Variables\nKeeping with the starwars data set, we might wish to revisit our earlier mutate of the male and female sex variable categories. Suppose for an analysis we wish to only include the male and female starwars characters? For this we might wish to filter so that our data only contains males and females. The code below will illustrate exactly how to do this\n\n1dplyr_data_mf &lt;- dplyr_data %&gt;%\n  filter(sex == 1 | sex == 0)\n\n\n1\n\nHere we have a filter() function that takes an argument for which conditions to include [==]. In this case we have when sex = 1 OR [|] when sex = 0.\n\n\n\n\n\n\n\n\n\n\nA Note on Syntax\n\n\n\nThe filter() function uses the notation == to serve as “equals”. You may also tell filter() what NOT to include with the notation !=\n\n\n\n\nReverse Coding\nIt is not uncommon for many of you to work with scales that might require some form of reverse coding. This can be accomplished using the following syntax. What is left will be the original dataframe with added columns for the items that we’ve reverse coded. They will have a “_R” variable name for ease of use\n\nlibrary(tidyverse)\n1library(psych)\n\n2df &lt;- data.frame(Q1 = c(1,3,4,5,6,7),\n                 Q2 = c(3,4,5,5,7,7),\n                 Q3 = c(1,2,2,4,1,1))\n\n3reverse_key &lt;- c(1,-1,1)\n\n4df_R &lt;- data.frame(reverse.code(keys = reverse_key,\n5                   items = df[,c(\"Q1\",\"Q2\",\"Q3\")],\n6                   mini = 1,\n7                   maxi = 7)) %&gt;%\n8  rename(\"Q2_R\" = \"Q2.\")\n\n9df &lt;- right_join(df,df_R,\n10                keep = FALSE)\n\nprint(df)\n\n\n1\n\nThe psych package contains a reverse.code() function for scale items\n\n2\n\nThere are no convenient pre-built data sets for this so I’ve created a quick toy one called df with the variables Q1, Q2, and Q3\n\n3\n\nThe reverse.code() function requires a keys argument which is essentially a numerical vector of length of the reverse coded items that correspond sequentially to which items are (-1) and aren’t (1) reverse coded\n\n4\n\nThis is the start of the reverse.code() function within a new dataframe\n\n5\n\nI’ve subset (only included) the scale items here using this notation\n\n6\n\nMini refers to the lowest possible value for the scale (i.e., 1)\n\n7\n\nMaxi refers to the highest possible value for the scale (i.e., 7)\n\n8\n\nI’ve added a rename() function to rename the reverse coded items from “ItemX.” to “ItemX_R” so we can track which items are the reverse coded one’s later\n\n9\n\nJoining the two data frames into the original one so we only have to worry about the original data set\n\n10\n\nRefers to keeping the keys used to join the two data frames (i.e., unique identifiers). We don’t want to keep them here\n\n\n\n\n  Q1 Q2 Q3 Q2_R\n1  1  3  1    5\n2  3  4  2    4\n3  4  5  2    3\n4  5  5  4    3\n5  6  7  1    1\n6  7  7  1    1"
  },
  {
    "objectID": "data_clean.html#working-with-the-stringr-package-working-w-strings",
    "href": "data_clean.html#working-with-the-stringr-package-working-w-strings",
    "title": "Data Visualization and Management",
    "section": "Working With The stringr Package (Working w/ Strings)",
    "text": "Working With The stringr Package (Working w/ Strings)\nThe stringr package is primarily used when working with what are known as strings of data. Essentially text box types of free response options. For example maybe in a Qualtrics form you allow someone to list “Other” as their religious belief system but ask them upon that selection choice to type out a better word. Same might be true for gender for example. Below we’ll use the words data set to some basic text manipulation with the first 10 rows of data. On the right, we will see our original data set. However, on the right we will see that data set ultimately filtered by whether or not there is an ab in the words variable for a given observation.\n\nText Detection With stringr Package\n\nlibrary(tidyverse)\n\n1stringr_data &lt;- data.frame(stringr::words %&gt;%\n                             head(10)) %&gt;%\n2  rename(\"words\" = \"stringr..words.....head.10.\")\n\n3stringr_data_original &lt;- stringr_data %&gt;%\n4  mutate(match = str_detect(words,pattern = \"ab\"))\n\n\n5stringr_data_new &lt;- stringr_data_original %&gt;%\n  filter(match == TRUE)\n\n\n1\n\nHere I am converting the stringr data into a data frame and selecting the first 10 observations for simplicity.\n\n2\n\nI’m also using the rename() function to change the preset variable name to “words”.\n\n3\n\nI’m “piping” the stringr_data into the mutate() function\n\n4\n\nThis line shows that I am creating a variable called match that will output a TRUE or FALSE if in the column words there is a pattern of \"ab\".\n\n5\n\nI am filtering the column match by whether or not it is TRUE (i.e., whether an observation consists of the pattern “ab”)\n\n\n\n\n\n\n\n\nOriginal Output\n\n\n      words match\n1         a FALSE\n2      able  TRUE\n3     about  TRUE\n4  absolute  TRUE\n5    accept FALSE\n6   account FALSE\n7   achieve FALSE\n8    across FALSE\n9       act FALSE\n10   active FALSE\n\n\n\n\n\nNew Output\n\n\n     words match\n1     able  TRUE\n2    about  TRUE\n3 absolute  TRUE\n\n\n\n\n\n\nText Replacement With The stringr Package\nWhile we’ve seen how to pull out matching observations using text responses, maybe we want to actually modify the responses. We can do that as well. We will demonstrate using the new data frame consisting of 3 words. Let’s as an example replace the pattern “ab” with nothing. We see how to do that below\n\nstringr_data_new &lt;- stringr_data_new %&gt;% \n1  mutate(across(.cols=\"words\",\n2                .fns=str_replace,\n3                pattern = \"ab\",\n4                replacement = \"\"))\n\nprint(stringr_data_new)\n\n\n1\n\nHere I am specifing that I wish to apply a function to the words column\n\n2\n\nThe function I wish to apply is the str_replace function which takes two arguments (pattern and replacement which I’m about to specify)\n\n3\n\nI specify the pattern I’m looking for as \"ab\"\n\n4\n\nI specify what I would like to replace that pattern with. In this case I don’t want anything so I just put \"\"\n\n\n\n\n   words match\n1     le  TRUE\n2    out  TRUE\n3 solute  TRUE"
  },
  {
    "objectID": "data_clean.html#working-with-the-lubridate-package-date-data",
    "href": "data_clean.html#working-with-the-lubridate-package-date-data",
    "title": "Data Visualization and Management",
    "section": "Working With The lubridate Package (Date Data)",
    "text": "Working With The lubridate Package (Date Data)\nPersonally, I don’t work with date data very often. Usually time simply isn’t a variable I’m interested in. However, for many of you who may be clinical or health focused, this is likely not your experience. Lets see how we can use the lubridate package to mess with date formatted data\n\nConverting to Date Format\n\nlibrary(tidyverse)\n\nlubridate_data &lt;- lubridate::lakers\n\nlubridate_data &lt;- lubridate_data %&gt;% \n1  mutate(across(.cols=date,\n                .fns=ymd)) %&gt;%\n2  mutate(date_myd = format(as.Date(date),\"%m-%d-%Y\"))\n\n\n1\n\nHere I am saying I wish to apply the function ymd() to the date column\n\n2\n\nFor this line, I am saying I wish to create a new variable called date_myd by formatting the date variable both as a date AND then formatted to a mm-dd-yyyy format. That corresponds to the \"%m-%d-%Y\" string we see on this line.\n\n\n\n\n\n\nModifying Date Format\nWe can see here that we’ve converted a numeric value in the format (YYYYMMDD) into a date in the “Year-Month-Date” format. This even looks a little more appealing to the eye especically as you’re scanning the date. However, what if you don’t like YYYY-MM-DD format and would rather have something like MM-DD-YYYY format instead as is common in the US? Below you can see how to take the format we just used and convert it to the more US common syntax shown on the left. On the right, we can see how to do it for the more EU common syntax of DD-MM-YY\n\n1lubridate_data &lt;- lubridate_data %&gt;%\n  mutate(date_dmy = format(as.Date(date),\"%d-%m-%Y\"))\n\n\n1\n\nHere I am doing the same as earlier but I am changing the format code to be dd-mm-yyyy using the string \"%d-%m-%Y\"\n\n\n\n\n\n\n\n\nMay-20-2023 Format\n\n\n[1] \"10-28-2008\"\n\n\n\n\n\n20-May-2023 Format\n\n\n[1] \"28-10-2008\""
  },
  {
    "objectID": "data_clean.html#working-with-the-ggplot2-package",
    "href": "data_clean.html#working-with-the-ggplot2-package",
    "title": "Data Visualization and Management",
    "section": "Working With The ggplot2 Package",
    "text": "Working With The ggplot2 Package\n\nStandard Histogram With Density Curve\n\nlibrary(tidyverse)\nlibrary(jtools)\n  \n1gender &lt;- rep(c(\"male\",\"female\"),50)\ntest &lt;- rnorm(100,mean = 75,sd=2)\n  \ndf &lt;- data.frame(gender,test)\n\n2density_plot &lt;- ggplot(df,aes(x = test)) +\n3  geom_histogram(aes(y=after_stat(density)),binwidth = 1) +\n4  stat_function(fun = dnorm,\n5                args = list(mean = mean(test),\n                            sd = sd(test)),\n6                col = \"blue\",\n                linewidth = 1) +\n7  jtools::theme_apa() +\n8  labs(title = \"Figure 1. Histogram of Test Scores\",\n       x = \"Test Scores\",\n       y = \"Score Density\")\n\n9ggsave(\"histogram.png\")\n10print(density_plot)\n\n\n1\n\nCreation of a basic data set consisting of 100 observations of 2 variables (gender and test)\n\n2\n\nInitial ggplot2 taking the arguments for df as the data and test as our variable to create a histogram of\n\n3\n\nThe geom_histogram() tells ggplot2 what type of geom to draw using the aes() data above. The aes(y=after_stat(density)) tells ggplot to convert the y axis as a function of density (vs count which is the default)\n\n4\n\nThis stats_function allows us to graph a statistic onto the graph. In this case we want it to graph a normal distribution (the dnorm function) of the variable we care about.\n\n5\n\nThe stats_function takes an args() function that we have to give it the mean and sd of the variable we care about. This is shown here\n\n6\n\nThese provide some general aesthetic choices so we’ve specified the curve to be colored blue with a relatively small line width of 1.\n\n7\n\nThe theme_apa() function simply modifies the ggplot2 graph to roughly align with APA formatting\n\n8\n\nThe labs() function allows us to add labels to our prospective histogram\n\n9\n\nThis will save the built graphic as a .png file\n\n10\n\nThis will print the ggplot2 column plot\n\n\n\n\n\n\n\n\n\nStandard Column Bar Graph\n\nlibrary(tidyverse)\nlibrary(jtools)\n\n1col_data &lt;- mtcars\n\nskimr::skim(col_data)\n\n\n1\n\nThe mtcars data set comes with the ggplot2 package. Finally I used the skim() function to take a quick look at the data\n\n\n\n\n\nData summary\n\n\nName\ncol_data\n\n\nNumber of rows\n32\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmpg\n0\n1\n20.09\n6.03\n10.40\n15.43\n19.20\n22.80\n33.90\n▃▇▅▁▂\n\n\ncyl\n0\n1\n6.19\n1.79\n4.00\n4.00\n6.00\n8.00\n8.00\n▆▁▃▁▇\n\n\ndisp\n0\n1\n230.72\n123.94\n71.10\n120.83\n196.30\n326.00\n472.00\n▇▃▃▃▂\n\n\nhp\n0\n1\n146.69\n68.56\n52.00\n96.50\n123.00\n180.00\n335.00\n▇▇▆▃▁\n\n\ndrat\n0\n1\n3.60\n0.53\n2.76\n3.08\n3.70\n3.92\n4.93\n▇▃▇▅▁\n\n\nwt\n0\n1\n3.22\n0.98\n1.51\n2.58\n3.33\n3.61\n5.42\n▃▃▇▁▂\n\n\nqsec\n0\n1\n17.85\n1.79\n14.50\n16.89\n17.71\n18.90\n22.90\n▃▇▇▂▁\n\n\nvs\n0\n1\n0.44\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nam\n0\n1\n0.41\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\ngear\n0\n1\n3.69\n0.74\n3.00\n3.00\n4.00\n4.00\n5.00\n▇▁▆▁▂\n\n\ncarb\n0\n1\n2.81\n1.62\n1.00\n2.00\n2.00\n4.00\n8.00\n▇▂▅▁▁\n\n\n\n\n\n\n2col_data &lt;- col_data %&gt;%\n  group_by(cyl) %&gt;%\n  summarize(n = n(),\n            mpg_average = mean(mpg))\n\n3col_plot &lt;- ggplot(col_data,aes(x = as.factor(cyl),\n                                y = mpg_average,\n                                fill = as.factor(cyl))) +\n4  geom_col(color = \"black\") +\n  labs(x = \"Number of Cylinders\", \n       y = \"Average Fuel Economy (mpg)\",\n       title = \"Figure 2. Average Fuel Economy by Cylinder Count\",\n       caption = \"Source: Data from the mtcars data set\") +\n  jtools::theme_apa() +\n5  theme(plot.caption = element_text(hjust = 0)) +\n6  scale_fill_manual(values = c(\"grey50\",\"grey80\",\"grey100\"))\n\nggsave(\"col_plot.png\")\nprint(col_plot)\n\n\n2\n\nI want to modify my data so that I have it grouped by cyl and n() and average_mpg are calculated\n\n3\n\nI’m starting to layer my column plot with this. aes() is where you put your important data (e.g., x and y variables)\n\n4\n\nThe geom_col() tells ggplot2 what type of geom to draw using the aes() data above\n\n5\n\nThe theme(plot.caption = element_text(hjust = 0)) just left justifies the caption\n\n6\n\nThe scale_fill_manual() tells ggplot2 what to assign for the fill variable in the aes() function\n\n\n\n\n\n\n\n\n\nStandard Boxplot Graph\n\nlibrary(tidyverse)\nlibrary(jtools)\nbplot_data &lt;- mtcars\n\nbox_plot &lt;- ggplot(bplot_data,aes(x = as.factor(cyl),\n                                  y = mpg)) + \n1  geom_boxplot(outlier.shape = NA) +\n  labs(x = \"Number of Cylinders\", \n       y = \"Average Fuel Economy (mpg)\", \n       title = \"Figure 3. Boxplot of Distribution of Average Fuel Economy by Cylinder Count\",\n       caption = \"Source: Data from the mtcars data set\") +\n  jtools::theme_apa() +\n  theme(plot.caption = element_text(hjust = 0)) +\n  scale_fill_manual(values = c(\"grey50\",\"grey80\",\"grey100\"))\n\nggsave(\"box_plot.png\")\nprint(box_plot)\n\n\n1\n\nThe beauty of ggplot2 is that there is a lot of overlap between different geom. The data to make a column chart vs a box plot in ggplot2 is just the geom_boxplot vs geom_col function calls shown here\n\n\n\n\n\n\n\n\n\nStandard Violin Plot\n\nlibrary(tidyverse)\nlibrary(jtools)\nviolin_data &lt;- mtcars\n\nviolin_plot &lt;- ggplot(violin_data,aes(x = as.factor(cyl),\n                                      y = mpg,\n                                      fill = as.factor(cyl))) + \n1  geom_violin(draw_quantiles = c(.25,.50,.75)) +\n  labs(x = \"Number of Cylinders\",\n       y = \"Average Fuel Economy (mpg)\",\n       title = \"Figure 4. Violin Plot of Distribution of Average Fuel Economy by Cylinder Count\",\n       caption = \"Source: Data from the mtcars data set\") +\n  jtools::theme_apa() +\n  theme(plot.caption = element_text(hjust = 0)) + \n  scale_fill_manual(values = c(\"grey50\",\"grey80\",\"grey100\"))\n\nggsave(\"violin.png\")\nviolin_plot\n\n\n1\n\nThe draw_quartiles function takes a numeric list to represent the quartiles you want. I’ve chosen the most common of 25%, 50% and 75% but you can input any set of 3 values you’d like\n\n\n\n\n\n\n\n\n\nStandard Line Graph\n\nlibrary(tidyverse)\nlibrary(jtools)\nlibrary(skimr)\n\n1line_data &lt;- txhousing\nskimr::skim(line_data)\n\n\n1\n\nWe’re now using a Texas housing data set found the ggplot2 package. We can take a look at it by using the skim() function in the skimr package\n\n\n\n\n\nData summary\n\n\nName\nline_data\n\n\nNumber of rows\n8602\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncity\n0\n1\n4\n21\n0\n46\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1.00\n2007.30\n4.50\n2000\n2003.00\n2007.00\n2011.00\n2015.0\n▇▆▆▆▅\n\n\nmonth\n0\n1.00\n6.41\n3.44\n1\n3.00\n6.00\n9.00\n12.0\n▇▅▅▅▇\n\n\nsales\n568\n0.93\n549.56\n1110.74\n6\n86.00\n169.00\n467.00\n8945.0\n▇▁▁▁▁\n\n\nvolume\n568\n0.93\n106858620.78\n244933668.97\n835000\n10840000.00\n22986824.00\n75121388.75\n2568156780.0\n▇▁▁▁▁\n\n\nmedian\n616\n0.93\n128131.44\n37359.58\n50000\n100000.00\n123800.00\n150000.00\n304200.0\n▅▇▃▁▁\n\n\nlistings\n1424\n0.83\n3216.90\n5968.33\n0\n682.00\n1283.00\n2953.75\n43107.0\n▇▁▁▁▁\n\n\ninventory\n1467\n0.83\n7.17\n4.61\n0\n4.90\n6.20\n8.15\n55.9\n▇▁▁▁▁\n\n\ndate\n0\n1.00\n2007.75\n4.50\n2000\n2003.83\n2007.75\n2011.67\n2015.5\n▇▇▇▇▇\n\n\n\n\n\n\n2line_data &lt;- line_data %&gt;%\n  group_by(year) %&gt;%\n  summarize(total_sales = sum(sales, na.rm = TRUE))\n\n3line_plot &lt;- ggplot(line_data,aes(x = year,\n                                  y = total_sales)) +\n4  geom_point() +\n5  geom_line() +\n6  labs(x = \"Year\",\n       y = \"Total Housing Sales\",\n       title = \"Figure 5. Total Texas Housing Sales By Year\",\n       caption = \"Source: Data from the ggplot2 data set\") +\n7  scale_x_continuous(breaks = seq(2000,2015,2)) +\n  jtools::theme_apa() +\n  theme(plot.caption = element_text(hjust = 0))\n\nggsave(\"line.png\")\nprint(line_plot)\n\n\n2\n\nIt might be useful to see how sales have changed over time within Texas. As such we might want to summarize the total number of home sales by year. How to do this is illustrated here with a group_by() and summarize() function.\n\n3\n\nWe need to feed the ggplot object our aes() variables. For this we’ve selected year and total_sales as our x and y variable respectively\n\n4\n\nWe might want to add points to our line graph for readability so we can add a geom_point() layer\n\n5\n\nNow we want to add our actual lines. We can do that by providing a geom_line() layer\n\n6\n\nAgain we are adding our typical labels here\n\n7\n\nThis scale_x_continous variable might seem weird. However if we look at our data we will see that our year variable is continuous rather than categorical. Further, the initial breaks skip by intervals of 5 between 2000 and 2015. As such, we may want to change this. We can do that with this function call. The seq function allows us to dictate the min and max of the x values and how we scale our graph. I’ve choosen to go by increments of 2.\n\n\n\n\n\n\n\n\n\nStandard Column Bar Graph W/ Std. Error\n\nlibrary(tidyverse)\nlibrary(jtools)\ncol_SE_data &lt;- mtcars\n\n1col_SE_data &lt;- col_SE_data %&gt;%\n  group_by(cyl) %&gt;%\n  summarize(n = n(),\n            mpg_average = mean(mpg, na.rm = TRUE),\n            sd = sd(mpg, na.rm = FALSE),\n            se = sd/sqrt(n))\n\n2col_SE_plot &lt;- ggplot(col_SE_data,aes(x = as.factor(cyl),\n                                      y = mpg_average,\n                                      fill = as.factor(cyl))) +\n3  geom_col(color = \"black\") +\n4  geom_errorbar(aes(ymax = mpg_average + se,\n                    ymin = mpg_average - se), width = .5) +\n5  labs(x = \"Number of Cylinders\",\n       y = \"Average Fuel Economy (mpg)\",\n       title = \"Figure 6. Average Fuel Economy by Cylinder Count\",\n       caption = \"Source: Data from the mtcars data set\") +\n6  scale_fill_manual(values = c(\"grey50\",\"grey80\",\"grey100\")) +\n  jtools::theme_apa() + \n  theme(plot.caption = element_text(hjust = 0))\n\nggsave(\"col_se.png\")\nprint(col_SE_plot)\n\n\n1\n\nFor the standard error chart, we have to borrow a bit from our previous line chart syntax as we need to manually compute some group level statistics in order to calculate SE. Here we’re grouping by cyl and we need to compute the n and SD to compute the SE. This syntax shows how to do this\n\n2\n\nWe need to provide our aes() factors. Here we want cyl,mpg_average and a fill aesthetic (for color)\n\n3\n\nWe need to add our standard geom_col layer\n\n4\n\nFor our error bars, we want to call geom_errorbar and designate our ymax (upper level) and ymin (lower level) bands. This will do that\n\n5\n\nAdding our usual labels\n\n6\n\nModify our colors for the column"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About This Website",
    "section": "",
    "text": "This is a website focused on providing R enthusiasts with an easy to navigate reference guide for how to perform a vast number of statistical analyses using R, RStudio and occasionally Quarto. I hope you find it useful for your needs"
  },
  {
    "objectID": "Workshop.html",
    "href": "Workshop.html",
    "title": "R Workshop",
    "section": "",
    "text": "If you’re someone who likes to use\n\n\n\n\n\n\n\n\n\n\n  to run data analysis, you’re using already “using”"
  },
  {
    "objectID": "Workshop.html#fun-fact",
    "href": "Workshop.html#fun-fact",
    "title": "R Workshop",
    "section": "Fun Fact",
    "text": "Fun Fact\n\nIf you’re someone who likes to use\n\n\n  to run data analysis, you’re using already “using”"
  },
  {
    "objectID": "Workshop.html#downloading-r",
    "href": "Workshop.html#downloading-r",
    "title": "R Workshop",
    "section": "Downloading R",
    "text": "Downloading R\n\n\n\n\nYou’ll want to download R first before downloading any additional software\nYou can download the newest version of R (4.2) here"
  },
  {
    "objectID": "Workshop.html#downloading-rstudio",
    "href": "Workshop.html#downloading-rstudio",
    "title": "R Workshop",
    "section": "Downloading RStudio",
    "text": "Downloading RStudio\n\n\n\n\nRStudio is the last software program you’ll need to get started\nYou can download the newest version of RStudio (Dec 2022) here"
  },
  {
    "objectID": "Workshop.html#r-quirks",
    "href": "Workshop.html#r-quirks",
    "title": "R Workshop",
    "section": "R “Quirks”",
    "text": "R “Quirks”\n\n\n\nR is case sensitive so what your spelling and the case you use\n\nCase =/= case\n\n\n\n\nR hates spaces for variable. It will not run with a space\n\nvariable_1 is a GOOD variable name\nvariable 1 is a BAD variable name"
  },
  {
    "objectID": "Workshop.html#downloading-materials-for-day-1",
    "href": "Workshop.html#downloading-materials-for-day-1",
    "title": "R Workshop",
    "section": "Downloading Materials For Day 1",
    "text": "Downloading Materials For Day 1\nYou can download all the materials for Day 1 of this workshop here * You want the correlation.qmd, data_clean.qmd, ttest.qmd, and regression.qmd files"
  },
  {
    "objectID": "Workshop.html#installing-and-loading-packages",
    "href": "Workshop.html#installing-and-loading-packages",
    "title": "R Workshop",
    "section": "Installing and Loading Packages",
    "text": "Installing and Loading Packages\n\nR CodeCommentary\n\n\n\n# To Install a Package\ninstall.packages(\"tidyverse\")\n\n# To Load a Package\nlibrary(tidyverse)\n\n\n\n\nYou only have to install a package once (one exception)\nYou must load a library every time you open an R file (.R, .qmd, .rmd, etc) or restart R/RStudio\n\n\n\n\n\n\n\nImportant\n\n\nA new install of R will remove all installed packages. You must either re-install the packages or save them prior to a new R installation. I’ll cover how to save them at a later date"
  },
  {
    "objectID": "Workshop.html#importing-data",
    "href": "Workshop.html#importing-data",
    "title": "R Workshop",
    "section": "Importing Data",
    "text": "Importing Data\n\nCommentaryR Code\n\n\n\nR works best with csv files (smaller in size) but it will take .sav files (SPSS) and other file formats as well (e.g., .tsv)\nOh and obviously it can read Microsoft Excel files\n\n\n\n\n# For CSV\n\ndata &lt;- read.csv(\"file_name.csv\")\n\n# For TSV\n\ndata &lt;- read_tsv(\"file_name.tsv\")\n\n# For SAV\n\nlibrary(haven)\ndata &lt;- read_sav(\"file_name.sav\")\n\n# For EXCEL\n\nlibrary(readxl)\ndata &lt;- read_xlsx(\"file_name.xlsx\")\ndata &lt;- read_xls(\"file_name.xls\")"
  },
  {
    "objectID": "Workshop.html#variable-types",
    "href": "Workshop.html#variable-types",
    "title": "R Workshop",
    "section": "Variable Types",
    "text": "Variable Types\n\nNumerical\n\nA positive or negative number between (- \\(\\infty\\), \\(\\infty\\))\n\nInteger\n\nA positive or negative whole number between (- \\(\\infty\\), \\(\\infty\\))\n\nFactor\n\nA grouping category\n\nCharacter\n\nA text string\n\nLogical\n\nA TRUE or FALSE value (e.g., Is X &gt; 1?)\n\nDate\n\nExactly what you think it is"
  },
  {
    "objectID": "Workshop.html#uses-for-the-dplyr-package",
    "href": "Workshop.html#uses-for-the-dplyr-package",
    "title": "R Workshop",
    "section": "Uses for the dplyr package",
    "text": "Uses for the dplyr package\n\nPrimary use is to transform and manipulate data in a data set\n\nCalculate means, log transform, compute basic summary statistics\n\nAnyone here who has maybe used database data will see it mimics SQL programming\n\n\n\n\n\n\n\nNote\n\n\nFor anyone who might work with database data, you can pull data from external databases with R and RStudio. We won’t cover that in this workshop but you can do it"
  },
  {
    "objectID": "Workshop.html#live-ish-coding",
    "href": "Workshop.html#live-ish-coding",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nOpen the data_clean.qmd file provided. We’re going to walk through some example code and then live code a little bit"
  },
  {
    "objectID": "Workshop.html#uses-for-the-stringr-package",
    "href": "Workshop.html#uses-for-the-stringr-package",
    "title": "R Workshop",
    "section": "Uses for the stringr package",
    "text": "Uses for the stringr package\n\nPrimarily used for dealing with character or string data\n\nUseful for free response questions\nEssentially it’s the dplyr package for string variable types"
  },
  {
    "objectID": "Workshop.html#live-ish-coding-1",
    "href": "Workshop.html#live-ish-coding-1",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nGo back to the data_clean.qmd file provided. We’re going to walk through some example code and then live code a little bit"
  },
  {
    "objectID": "Workshop.html#uses-for-the-lubridate-package",
    "href": "Workshop.html#uses-for-the-lubridate-package",
    "title": "R Workshop",
    "section": "Uses for the lubridate package",
    "text": "Uses for the lubridate package\n\nPrimarily used for dealing with dates\nProvides handy function for converting date formats into other date formats\n\nE.g., (MM-DD-YY to DD-MM-YY or Month Date, Year)\n\n\n\n\n\n\n\n\nTip\n\n\nIt won’t auto convert your dates to weird incorrect formats like certain spreadsheet programs might."
  },
  {
    "objectID": "Workshop.html#live-ish-coding-2",
    "href": "Workshop.html#live-ish-coding-2",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nGo back to the data_clean.qmd file provided. We’re going to walk through some example code and then live code a little bit"
  },
  {
    "objectID": "Workshop.html#uses-for-the-ggplot2-package",
    "href": "Workshop.html#uses-for-the-ggplot2-package",
    "title": "R Workshop",
    "section": "Uses for the ggplot2 package",
    "text": "Uses for the ggplot2 package\n\nThis may be the most popular package download in R and it’s probably not close\nThis is THE visualization package in R. If you can THINK of a graphic, this package can create it\n\nE.g. box plots, box and whisker plots, violin plots, bar graphs, etc\n\nIf you’re REALLY good, you can do this (credit @ralitza_s)"
  },
  {
    "objectID": "Workshop.html#liveish-coding",
    "href": "Workshop.html#liveish-coding",
    "title": "R Workshop",
    "section": "“Live”ish Coding",
    "text": "“Live”ish Coding\n\nGo back to the data_clean.qmd file provided. We’re going to walk through some example code and then live code a little bit"
  },
  {
    "objectID": "Workshop.html#section-1",
    "href": "Workshop.html#section-1",
    "title": "R Workshop",
    "section": "",
    "text": "CommentaryExcelSPSS or SASCSV\n\n\n\nSometimes you want or need to export data you’ve cleaned to another program. Maybe you want to use a program like JASP\nOr you’re not comfortable using R for analyses yet so you want to use SPSS\nOr maybe others on your team use a different program\nR can export to Excel, SPSS, SAS, and CSV files\n\n\n\n\nlibrary(openxlsx)\n\nwrite.xlsx(df,file = \"filename.xlsx\")\n\n\n\n\nlibrary(haven)\n\n# For SPSS\nwrite_sav(df, path = \"filename.sav\")\n\n# For SAS\nwrite_sas(df, path = \"filename.sas\")\n\n\n\n\nwrite.csv(df, file = \"filename.csv\")"
  },
  {
    "objectID": "Workshop.html#assumptions-fields-et-al-2012",
    "href": "Workshop.html#assumptions-fields-et-al-2012",
    "title": "R Workshop",
    "section": "Assumptions (Fields et al, 2012)",
    "text": "Assumptions (Fields et al, 2012)\n\nOn at least an interval scale\nNormality of Residuals"
  },
  {
    "objectID": "Workshop.html#live-ish-coding-3",
    "href": "Workshop.html#live-ish-coding-3",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nPlease see correlation.qmd file provided"
  },
  {
    "objectID": "Workshop.html#assumptions-fields-et-al-2012-1",
    "href": "Workshop.html#assumptions-fields-et-al-2012-1",
    "title": "R Workshop",
    "section": "Assumptions (Fields et al, 2012)",
    "text": "Assumptions (Fields et al, 2012)\n\nNormality of Residuals\nIndependent Observations\nHomogeneity of Variance"
  },
  {
    "objectID": "Workshop.html#live-ish-coding-4",
    "href": "Workshop.html#live-ish-coding-4",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nPlease see the ttest.qmd file provided"
  },
  {
    "objectID": "Workshop.html#assumptions-fields-et-al-2012-2",
    "href": "Workshop.html#assumptions-fields-et-al-2012-2",
    "title": "R Workshop",
    "section": "Assumptions (Fields et al, 2012)",
    "text": "Assumptions (Fields et al, 2012)\n\nOutliers and Influential Cases\nNormality of Residuals\nIndependent Observations\nHomogeneity of Variance\n\n\n\n\n\n\n\nImportant\n\n\nWhile important, outliers and influential cases rarely influence results with a sufficient sample size. Also difficult to say what “is” and “isn’t” an outlier. Outlier shouldn’t always mean removal"
  },
  {
    "objectID": "Workshop.html#live-ish-coding-5",
    "href": "Workshop.html#live-ish-coding-5",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nPlease open the regression.qmd file provided"
  },
  {
    "objectID": "Workshop.html#downloading-material-for-day-2",
    "href": "Workshop.html#downloading-material-for-day-2",
    "title": "R Workshop",
    "section": "Downloading Material For Day 2",
    "text": "Downloading Material For Day 2\nYou can download all the materials for Day 2 of this workshop here * You want the anova.qmd, nonparametric.qmd, intro_qarto.qmd, mlm.qmd, sem.qmd and factor_analysis.qmd"
  },
  {
    "objectID": "Workshop.html#assumptions-fields-et-al-2012-3",
    "href": "Workshop.html#assumptions-fields-et-al-2012-3",
    "title": "R Workshop",
    "section": "Assumptions (Fields et al, 2012)",
    "text": "Assumptions (Fields et al, 2012)\n\nNormality Within Groups\nHomogeneity of Variance\nIndependent Observations"
  },
  {
    "objectID": "Workshop.html#live-ish-coding-6",
    "href": "Workshop.html#live-ish-coding-6",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nPlease open the anova.qmd file provided"
  },
  {
    "objectID": "Workshop.html#live-ish-coding-7",
    "href": "Workshop.html#live-ish-coding-7",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nPlease open the nonparametric.qmd file"
  },
  {
    "objectID": "Workshop.html#efa-assumptions-fields-et-al-2012",
    "href": "Workshop.html#efa-assumptions-fields-et-al-2012",
    "title": "R Workshop",
    "section": "EFA Assumptions (Fields et al, 2012)",
    "text": "EFA Assumptions (Fields et al, 2012)\n\nSufficient Sample Size\nNormality of Items\nCorrelation Between Items1\nAppropriate Determinant (Det \\(&gt;\\) 1 x 10-5)\n\n\n\n\n\n\n\nImportant\n\n\n\nWe want variables to correlate however we do not want them to correlate either  too low (r \\(&lt;\\) .30) or too high (r \\(&gt;\\) .80) across multiple items"
  },
  {
    "objectID": "Workshop.html#cfa-assumptions",
    "href": "Workshop.html#cfa-assumptions",
    "title": "R Workshop",
    "section": "CFA Assumptions",
    "text": "CFA Assumptions\n\nMultivariate Normality"
  },
  {
    "objectID": "Workshop.html#live-ish-coding-8",
    "href": "Workshop.html#live-ish-coding-8",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nPlease open the factor_analysis.qmd file"
  },
  {
    "objectID": "Workshop.html#assumptions-to-test-kaplan-2001-p.-15218",
    "href": "Workshop.html#assumptions-to-test-kaplan-2001-p.-15218",
    "title": "R Workshop",
    "section": "Assumptions To Test (Kaplan, 2001, p. 15218)",
    "text": "Assumptions To Test (Kaplan, 2001, p. 15218)\n\nMultivariate Normality\nNo Systematic Missing Data\nSufficiently Large Sample Size\nCorrect Model Specification"
  },
  {
    "objectID": "Workshop.html#live-ish-coding-9",
    "href": "Workshop.html#live-ish-coding-9",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nPlease open the sem.qmd file"
  },
  {
    "objectID": "Workshop.html#assumptions-to-test",
    "href": "Workshop.html#assumptions-to-test",
    "title": "R Workshop",
    "section": "Assumptions To Test",
    "text": "Assumptions To Test\n\nOutliers and Influential Cases\nNormality of Residuals\nIndependent Observations1\nHomogeneity of Variance\n\n\n\n\n\n\n\nA Note On Independence\n\n\n\nThis assumption is not necessarily a concern given that MLM assumes observations are nested (Fields et al, 2012)"
  },
  {
    "objectID": "Workshop.html#live-ish-coding-10",
    "href": "Workshop.html#live-ish-coding-10",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nPlease open the mlm.qmd file"
  },
  {
    "objectID": "Workshop.html#the-holy-grail-of-reproducibility",
    "href": "Workshop.html#the-holy-grail-of-reproducibility",
    "title": "R Workshop",
    "section": "The Holy Grail of Reproducibility",
    "text": "The Holy Grail of Reproducibility\n\nWhat if I told you that it was possible to generate 95% of what you need for a manuscript within RStudio AND you could integrate your analyses as well?\nWhat if I also said you could export this to Microsoft Word?"
  },
  {
    "objectID": "Workshop.html#lets-talk-about-quarto",
    "href": "Workshop.html#lets-talk-about-quarto",
    "title": "R Workshop",
    "section": "Let’s Talk About Quarto",
    "text": "Let’s Talk About Quarto\n\nPlease open the intro_quarto.qmd file"
  },
  {
    "objectID": "Workshop.html#section-2",
    "href": "Workshop.html#section-2",
    "title": "R Workshop",
    "section": "",
    "text": "Firstly, thank you for your time this weekend and I hope you’ve learned something\nSecond, this is A LOT. I crammed stuffed about 2 years of statistical analyses time and practice into like 2 days. It’s okay and normal if you’re swimming. I’m here if anyone has any questions after or even if they’re using R and trying to do an actual analysis in R. People ask me for help all the time. I’m happy to help\nFinally, one last “minor” detail. Some of you already know this but lets just check out the following link"
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "Correlation",
    "section": "",
    "text": "Correlations might be the most common statistical test run (especially early on in a project). For this part of the workshop we will be using the mtcars data set due to its many numerical values that we can assess for correlation. Below we will see the first 10 rows of the data set displayed\n\nlibrary(tidyverse)\nlibrary(car)\nlibrary(psych)\n\ndata &lt;- mtcars\n\n1print(head(data,10))\n\n\n1\n\nDisplay the first 10 rows of the mtcars data. You can also return the last 10 rows with the following function tail(data,10)\n\n\n\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280          19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4"
  },
  {
    "objectID": "correlation.html#introduction",
    "href": "correlation.html#introduction",
    "title": "Correlation",
    "section": "",
    "text": "Correlations might be the most common statistical test run (especially early on in a project). For this part of the workshop we will be using the mtcars data set due to its many numerical values that we can assess for correlation. Below we will see the first 10 rows of the data set displayed\n\nlibrary(tidyverse)\nlibrary(car)\nlibrary(psych)\n\ndata &lt;- mtcars\n\n1print(head(data,10))\n\n\n1\n\nDisplay the first 10 rows of the mtcars data. You can also return the last 10 rows with the following function tail(data,10)\n\n\n\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280          19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4"
  },
  {
    "objectID": "correlation.html#statistical-assumptions",
    "href": "correlation.html#statistical-assumptions",
    "title": "Correlation",
    "section": "Statistical Assumptions",
    "text": "Statistical Assumptions\nThe primary assumption of a Pearson’s correlation coefficient is that the data is on some kind of interval scale. However, if we wish to generalize, we must have a random large sample (unlikely) and the individual variables should be roughly normally distributed. This is the assumption we will focus on for this part of the workshop.\n\nNormality of Variables\nFor this part of the workshop, we are going to focus on the mpg and wt variables. We will be looking at normality both statistically1 as well as graphically.\n\n\n\n\n\n\nA Note About Statistical Assumption Testing\n\n\n\n\nOdds are your statistical test is going to fail pretty much every single time. Particularly if you use something like a Shapiro Wilk test, but we’ll look at it anyway\n\n\n\n\n\nGraphical Depiction of Normality Assumption (mpg)\n\n1mpg_plot &lt;- ggplot(data,aes(x = mpg)) +\n  geom_histogram(aes(y=after_stat(density))) +\n  stat_function(fun = dnorm,\n                args = list(mean = mean(data$mpg),\n                            sd = sd(data$mpg)),\n                col = \"blue\")\n\nprint(mpg_plot)\n\n\n\n\n1\n\nThis should look very familiar to the histogram part of the workshop\n\n\n\n\n\n\n\n\n\n\n\nStatistical Depiction of Normality Assumption (mpg)\n\n1print(psych::describe(data$mpg))\n\n2print(shapiro.test(data$mpg))\n\n\n\n\n1\n\nThe psych package has a bunch of nifty functions for social science research. One is the describe() function which gives you a bunch of variable level summary statistics (e.g., mean, median, se, etc.)\n\n2\n\nThe shapiro.test() performs a Shapiro Wilk test of normality. Keep in mind this particular test is very sensitive to sample sizes\n\n\n\n\n\n\n   vars  n  mean   sd median trimmed  mad  min  max range skew kurtosis   se\nX1    1 32 20.09 6.03   19.2    19.7 5.41 10.4 33.9  23.5 0.61    -0.37 1.07\n\n    Shapiro-Wilk normality test\n\ndata:  data$mpg\nW = 0.94756, p-value = 0.1229\n\n\n\n\n\n\nGraphical Depiction of Normality Assumption (wt)\n\nwt_plot &lt;- ggplot(data,aes(x = wt)) +\n  geom_histogram(aes(y=after_stat(density))) + \n  stat_function(fun = dnorm,\n                args = list(mean = mean(data$wt),\n                            sd = sd(data$wt)),\n                col = \"blue\")\n\nprint(wt_plot)\n\n\n\n\n\n\n\n\n\n\nStatistical Depiction of Normality Assumption (wt)\n\nprint(psych::describe(data$wt))\nprint(shapiro.test(data$wt))\n\n\n\n   vars  n mean   sd median trimmed  mad  min  max range skew kurtosis   se\nX1    1 32 3.22 0.98   3.33    3.15 0.77 1.51 5.42  3.91 0.42    -0.02 0.17\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  data$wt\nW = 0.94326, p-value = 0.09265"
  },
  {
    "objectID": "correlation.html#running-an-actual-correlation",
    "href": "correlation.html#running-an-actual-correlation",
    "title": "Correlation",
    "section": "Running An Actual Correlation",
    "text": "Running An Actual Correlation\nThere are multiple packages and methods for calculating a correlation in R depending on what you want to assess. The best to use for psychology is probably the corr.test() function in the psych package because it allows you to change the type of correlation you wish to compute (e.g., spearman vs pearson) as well as generate confidence intervals and do p value adjustments\n\n1corr_results &lt;- corr.test(x = data$mpg,\n2          y = data$wt,\n3          use = \"pairwise\",\n4          method = \"pearson\",\n5          adjust = \"holm\")\n\n?corr.test()\n\n\n1\n\nChoose one of your variables to be your x variable\n\n2\n\nChoose the other to be your y variable\n\n3\n\nYou can choose “pairwise” or “complete”. For information on what each does, use the following function to access the documentation: ?psych::corr.test()\n\n4\n\nYou can adjust method to be other ones like “spearman”\n\n5\n\nYou can also use “bonferroni” among a few others\n\n\n\n\nBelow we will see the output of the correlation results as you might be used to seeing in a program like SPSS.\n\nprint(corr_results)\n\nCall:corr.test(x = data$mpg, y = data$wt, use = \"pairwise\", method = \"pearson\", \n    adjust = \"holm\")\nCorrelation matrix \n[1] -0.87\nSample Size \n[1] 32\nThese are the unadjusted probability values.\n  The probability values  adjusted for multiple tests are in the p.adj object. \n[1] 0\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\nWhile the above is great, notice we didn’t get a confidence interval output despite asking for it with ci = TRUE. Sometimes R will store complex computations within the output object (e.g., corr_results). To get this output we can put a $ after the output. If there is extra information stored, but not shown, we’ll get a drop down box. We want the ci option. Below we will see the output that results from this. We should see the following:\n\\(r\\) = -.87 , \\(p\\) &lt; .001 with a CI = [-.93,-.74]\n\nprint(corr_results$ci)\n\n           lower          r      upper            p\nNA-NA -0.9338264 -0.8676594 -0.7440872 1.293959e-10"
  }
]