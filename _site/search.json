[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Statistical Analysis",
    "section": "",
    "text": "Welcome to this R workshop reference website! Here you will find all information covered in the R workshop in both a website form and a downloadable PDF form for easy reference. I hope you found the R workshop helpful and I encourage everyone to provide feedback so that I can improve should I run other R workshops in the future."
  },
  {
    "objectID": "data_clean.html",
    "href": "data_clean.html",
    "title": "R Workshop: Data Visualization and Management",
    "section": "",
    "text": "install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\nThe above code is how you want to start any R script. You always want to install and load in any packages that you may need in order to run analyses. For this part of the R workshop we will be working with what is called the tidyverse package. It’s essentially the go to array of packages in R for data science needs (and therefore a good portion of our needs as well)\n\n\n\n\n\n\nA Note On Packages & library() Function\n\n\n\nYou only need to install a package once (unless you update your version of R). The package gets stored on your local computer. A library() function call imports the installed package from your local storage. Further, you only need to call the library() function once per R script"
  },
  {
    "objectID": "data_clean.html#installing-the-tidyverse-package",
    "href": "data_clean.html#installing-the-tidyverse-package",
    "title": "R Workshop: Data Visualization and Management",
    "section": "",
    "text": "install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\nThe above code is how you want to start any R script. You always want to install and load in any packages that you may need in order to run analyses. For this part of the R workshop we will be working with what is called the tidyverse package. It’s essentially the go to array of packages in R for data science needs (and therefore a good portion of our needs as well)\n\n\n\n\n\n\nA Note On Packages & library() Function\n\n\n\nYou only need to install a package once (unless you update your version of R). The package gets stored on your local computer. A library() function call imports the installed package from your local storage. Further, you only need to call the library() function once per R script"
  },
  {
    "objectID": "data_clean.html#working-with-the-dplyr-package-data-manipulation",
    "href": "data_clean.html#working-with-the-dplyr-package-data-manipulation",
    "title": "R Workshop: Data Visualization and Management",
    "section": "Working With The dplyr Package (Data Manipulation)",
    "text": "Working With The dplyr Package (Data Manipulation)\n\n1library(tidyverse)\nlibrary(skimr)\n\n2dplyr_data &lt;- dplyr::starwars\n\n\n1\n\nCall the tidyverse packages\n\n2\n\nWe will be using the starwars data set for the dplyr tutorial. I’ve assigned it to the variable dplyr_data here.\n\n\n\n\n\n1skim(dplyr_data)\n\n\n1\n\nWe can view some of the key variable data using the skim()\n\n\n\n\n\nData summary\n\n\nName\ndplyr_data\n\n\nNumber of rows\n87\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nlist\n3\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nname\n0\n1.00\n3\n21\n0\n87\n0\n\n\nhair_color\n5\n0.94\n4\n13\n0\n12\n0\n\n\nskin_color\n0\n1.00\n3\n19\n0\n31\n0\n\n\neye_color\n0\n1.00\n3\n13\n0\n15\n0\n\n\nsex\n4\n0.95\n4\n14\n0\n4\n0\n\n\ngender\n4\n0.95\n8\n9\n0\n2\n0\n\n\nhomeworld\n10\n0.89\n4\n14\n0\n48\n0\n\n\nspecies\n4\n0.95\n3\n14\n0\n37\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\nfilms\n0\n1\n24\n1\n7\n\n\nvehicles\n0\n1\n11\n0\n2\n\n\nstarships\n0\n1\n17\n0\n5\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nheight\n6\n0.93\n174.36\n34.77\n66\n167.0\n180\n191.0\n264\n▁▁▇▅▁\n\n\nmass\n28\n0.68\n97.31\n169.46\n15\n55.6\n79\n84.5\n1358\n▇▁▁▁▁\n\n\nbirth_year\n44\n0.49\n87.57\n154.69\n8\n35.0\n52\n72.0\n896\n▇▁▁▁▁\n\n\n\n\n\n\n1head(dplyr_data)\n\n\n1\n\nWe can also use the head() function which simply gives you a print out of the first 5 rows of a data set.\n\n\n\n\n\nRecoding Variables\nOne variable when looking at the starwars data set might be sex. Here we can see it is coded as both a character and as either male, female or NA. For a simple recode we might wish to\n\nTransform the variables into a factor\nChange the naming convention to maybe 1, 0 and Unknown\n\nWe can achieve this with the code below\n\ndplyr_data &lt;- dplyr_data %&gt;% \n1  mutate(sex = as.factor(sex),\n2         sex = recode(sex,\n                      'male' = '0',\n                      'female' = '1'))\n\n\n1\n\nTo recode the variable sex we need to use the mutate() function and as.factor() functions as shown above\n\n2\n\nTo recode the values for male and female to 0 and 1 respectively, we need to use the recode() function as shown here. ::: {.callout-tip} ### A Note On %&gt;% Operator You may have noticed this %&gt;% operator. This is a handy operator that essentially takes the data on the left hand side and “pipe”s it into whatever is on the right as the first argument. This is most effective when the right hand function is expecting some form of a data set :::\n\n\n\n\n\n\n\n\n\n\nA Note On dplyr::recode() Function\n\n\n\nThe recode() function in the dplyr package uses what is called OLD to NEW syntax. This just means that when renaming variables as shown here, you want to list the original variable name followed by you new desired variable name\n\n\n\n\nCreating Variables\nCreating variables in R can be done a couple of ways. One is a little clunky (from a code perspective) and the other is more elegant. I’ll cover the more clunky way first followed by the more elegant way second. I’ll illustrate this by creating a variable that takes the mass variable from the starwars data set and reduces it by 10 units\n\n1dplyr_data$mass_10a &lt;- dplyr_data$mass - 10\n\n\n2dplyr_data &lt;- dplyr_data %&gt;%\n  mutate(mass_10b = mass - 10)\n\n\n1\n\nThis ways is relatively simply because you can think of it as a simple formula notation. However, it’s a little clunky because typically adding a $ operator is considered poor coding practice\n\n2\n\nThe more elegant way to create a variable is to simply again use the mutate() function\n\n\n\n\n\n\n\n\n\n\nA Note On the $ Operator\n\n\n\nThe $ operator simply says from the data set on the left of the operator, please find (or create) the variable on the right. In this case, from the dplyr_data data, create the variable mass_10a\n\n\n\n\nFiltering Variables\nKeeping with the starwars data set, we might wish to revisit our earlier mutate of the male and female sex variable categories. Suppose for an analysis we wish to only include the male and female starwars characters? For this we might wish to filter so that our data only contains males and females. The code below will illustrate exactly how to do this\n\n1dplyr_data_mf &lt;- dplyr_data %&gt;%\n  filter(sex == 1 | sex == 0)\n\n\n1\n\nHere we have a filter() function that takes an argument for which conditions to include [==]. In this case we have when sex = 1 OR [|] when sex = 0.\n\n\n\n\n\n\n\n\n\n\nA Note on Syntax\n\n\n\nThe filter() function uses the notation == to serve as “equals”. You may also tell filter() what NOT to include with the notation !=\n\n\n\n\nReverse Coding\nIt is not uncommon for many of you to work with scales that might require some form of reverse coding. This can be accomplished using the following syntax. What is left will be the original dataframe with added columns for the items that we’ve reverse coded. They will have a “_R” variable name for ease of use\n\nlibrary(tidyverse)\n1library(psych)\n\n2df &lt;- data.frame(Q1 = c(1,3,4,5,6,7),\n                 Q2 = c(3,4,5,5,7,7),\n                 Q3 = c(1,2,2,4,1,1))\n\n3reverse_key &lt;- c(1,-1,1)\n\n4df_R &lt;- data.frame(reverse.code(keys = reverse_key,\n5                   items = df[,c(\"Q1\",\"Q2\",\"Q3\")],\n6                   mini = 1,\n7                   maxi = 7)) %&gt;%\n8  rename(\"Q2_R\" = \"Q2.\")\n\n9df &lt;- right_join(df,df_R,\n10                keep = FALSE)\n\nprint(df)\n\n\n1\n\nThe psych package contains a reverse.code() function for scale items\n\n2\n\nThere are no convenient pre-built data sets for this so I’ve created a quick toy one called df with the variables Q1, Q2, and Q3\n\n3\n\nThe reverse.code() function requires a keys argument which is essentially a numerical vector of length of the reverse coded items that correspond sequentially to which items are (-1) and aren’t (1) reverse coded\n\n4\n\nThis is the start of the reverse.code() function within a new dataframe\n\n5\n\nI’ve subset (only included) the scale items here using this notation\n\n6\n\nMini refers to the lowest possible value for the scale (i.e., 1)\n\n7\n\nMaxi refers to the highest possible value for the scale (i.e., 7)\n\n8\n\nI’ve added a rename() function to rename the reverse coded items from “ItemX.” to “ItemX_R” so we can track which items are the reverse coded one’s later\n\n9\n\nJoining the two data frames into the original one so we only have to worry about the original data set\n\n10\n\nRefers to keeping the keys used to join the two data frames (i.e., unique identifiers). We don’t want to keep them here\n\n\n\n\n  Q1 Q2 Q3 Q2_R\n1  1  3  1    5\n2  3  4  2    4\n3  4  5  2    3\n4  5  5  4    3\n5  6  7  1    1\n6  7  7  1    1"
  },
  {
    "objectID": "data_clean.html#working-with-the-stringr-package-working-w-strings",
    "href": "data_clean.html#working-with-the-stringr-package-working-w-strings",
    "title": "R Workshop: Data Visualization and Management",
    "section": "Working With The stringr Package (Working w/ Strings)",
    "text": "Working With The stringr Package (Working w/ Strings)\nThe stringr package is primarily used when working with what are known as strings of data. Essentially text box types of free response options. For example maybe in a Qualtrics form you allow someone to list “Other” as their religious belief system but ask them upon that selection choice to type out a better word. Same might be true for gender for example. Below we’ll use the words data set to some basic text manipulation with the first 10 rows of data. On the right, we will see our original data set. However, on the right we will see that data set ultimately filtered by whether or not there is an ab in the words variable for a given observation.\n\nText Detection With stringr Package\n\nlibrary(tidyverse)\n\n1stringr_data &lt;- data.frame(stringr::words %&gt;%\n                             head(10)) %&gt;%\n2  rename(\"words\" = \"stringr..words.....head.10.\")\n\n3stringr_data_original &lt;- stringr_data %&gt;%\n4  mutate(match = str_detect(words,pattern = \"ab\"))\n\n\n5stringr_data_new &lt;- stringr_data_original %&gt;%\n  filter(match == TRUE)\n\n\n1\n\nHere I am converting the stringr data into a data frame and selecting the first 10 observations for simplicity.\n\n2\n\nI’m also using the rename() function to change the preset variable name to “words”.\n\n3\n\nI’m “piping” the stringr_data into the mutate() function\n\n4\n\nThis line shows that I am creating a variable called match that will output a TRUE or FALSE if in the column words there is a pattern of \"ab\".\n\n5\n\nI am filtering the column match by whether or not it is TRUE (i.e., whether an observation consists of the pattern “ab”)\n\n\n\n\n\n\n\n\nOriginal Output\n\n\n      words match\n1         a FALSE\n2      able  TRUE\n3     about  TRUE\n4  absolute  TRUE\n5    accept FALSE\n6   account FALSE\n7   achieve FALSE\n8    across FALSE\n9       act FALSE\n10   active FALSE\n\n\n\n\n\nNew Output\n\n\n     words match\n1     able  TRUE\n2    about  TRUE\n3 absolute  TRUE\n\n\n\n\n\n\nText Replacement With The stringr Package\nWhile we’ve seen how to pull out matching observations using text responses, maybe we want to actually modify the responses. We can do that as well. We will demonstrate using the new data frame consisting of 3 words. Let’s as an example replace the pattern “ab” with nothing. We see how to do that below\n\nstringr_data_new &lt;- stringr_data_new %&gt;% \n1  mutate(across(.cols=\"words\",\n2                .fns=str_replace,\n3                pattern = \"ab\",\n4                replacement = \"\"))\n\nprint(stringr_data_new)\n\n\n1\n\nHere I am specifing that I wish to apply a function to the words column\n\n2\n\nThe function I wish to apply is the str_replace function which takes two arguments (pattern and replacement which I’m about to specify)\n\n3\n\nI specify the pattern I’m looking for as \"ab\"\n\n4\n\nI specify what I would like to replace that pattern with. In this case I don’t want anything so I just put \"\"\n\n\n\n\n   words match\n1     le  TRUE\n2    out  TRUE\n3 solute  TRUE"
  },
  {
    "objectID": "data_clean.html#working-with-the-lubridate-package-date-data",
    "href": "data_clean.html#working-with-the-lubridate-package-date-data",
    "title": "R Workshop: Data Visualization and Management",
    "section": "Working With The lubridate Package (Date Data)",
    "text": "Working With The lubridate Package (Date Data)\nPersonally, I don’t work with date data very often. Usually time simply isn’t a variable I’m interested in. However, for many of you who may be clinical or health focused, this is likely not your experience. Lets see how we can use the lubridate package to mess with date formatted data\n\nConverting to Date Format\n\nlibrary(tidyverse)\n\nlubridate_data &lt;- lubridate::lakers\n\nlubridate_data &lt;- lubridate_data %&gt;% \n1  mutate(across(.cols=date,\n                .fns=ymd)) %&gt;%\n2  mutate(date_myd = format(as.Date(date),\"%m-%d-%Y\"))\n\n\n1\n\nHere I am saying I wish to apply the function ymd() to the date column\n\n2\n\nFor this line, I am saying I wish to create a new variable called date_myd by formatting the date variable both as a date AND then formatted to a mm-dd-yyyy format. That corresponds to the \"%m-%d-%Y\" string we see on this line.\n\n\n\n\n\n\nModifying Date Format\nWe can see here that we’ve converted a numeric value in the format (YYYYMMDD) into a date in the “Year-Month-Date” format. This even looks a little more appealing to the eye especially as you’re scanning the date. However, what if you don’t like YYYY-MM-DD format and would rather have something like MM-DD-YYYY format instead as is common in the US? Below you can see how to take the format we just used and convert it to the more US common syntax shown on the left. On the right, we can see how to do it for the more EU common syntax of DD-MM-YY\n\n1lubridate_data &lt;- lubridate_data %&gt;%\n  mutate(date_dmy = format(as.Date(date),\"%d-%m-%Y\"))\n\n\n1\n\nHere I am doing the same as earlier but I am changing the format code to be dd-mm-yyyy using the string \"%d-%m-%Y\"\n\n\n\n\n\n\n\n\nMay-20-2023 Format\n\n\n[1] \"10-28-2008\"\n\n\n\n\n\n20-May-2023 Format\n\n\n[1] \"28-10-2008\""
  },
  {
    "objectID": "data_clean.html#working-with-the-ggplot2-package",
    "href": "data_clean.html#working-with-the-ggplot2-package",
    "title": "R Workshop: Data Visualization and Management",
    "section": "Working With The ggplot2 Package",
    "text": "Working With The ggplot2 Package\n\nStandard Histogram With Density Curve\n\nlibrary(tidyverse)\nlibrary(jtools)\n  \n1gender &lt;- rep(c(\"male\",\"female\"),50)\ntest &lt;- rnorm(100,mean = 75,sd=2)\n  \ndf &lt;- data.frame(gender,test)\n\n2density_plot &lt;- ggplot(df,aes(x = test)) +\n3  geom_histogram(aes(y=after_stat(density)),binwidth = 1) +\n4  stat_function(fun = dnorm,\n5                args = list(mean = mean(test),\n                            sd = sd(test)),\n6                col = \"blue\",\n                linewidth = 1) +\n7  jtools::theme_apa() +\n8  labs(title = \"Figure 1. Histogram of Test Scores\",\n       x = \"Test Scores\",\n       y = \"Score Density\")\n\n9ggsave(\"histogram.png\")\n10print(density_plot)\n\n\n1\n\nCreation of a basic data set consisting of 100 observations of 2 variables (gender and test)\n\n2\n\nInitial ggplot2 taking the arguments for df as the data and test as our variable to create a histogram of\n\n3\n\nThe geom_histogram() tells ggplot2 what type of geom to draw using the aes() data above. The aes(y=after_stat(density)) tells ggplot to convert the y axis as a function of density (vs count which is the default)\n\n4\n\nThis stats_function allows us to graph a statistic onto the graph. In this case we want it to graph a normal distribution (the dnorm function) of the variable we care about.\n\n5\n\nThe stats_function takes an args() function that we have to give it the mean and sd of the variable we care about. This is shown here\n\n6\n\nThese provide some general aesthetic choices so we’ve specified the curve to be colored blue with a relatively small line width of 1.\n\n7\n\nThe theme_apa() function simply modifies the ggplot2 graph to roughly align with APA formatting\n\n8\n\nThe labs() function allows us to add labels to our prospective histogram\n\n9\n\nThis will save the built graphic as a .png file\n\n10\n\nThis will print the ggplot2 column plot\n\n\n\n\n\n\n\n\n\nStandard Column Bar Graph\n\nlibrary(tidyverse)\nlibrary(jtools)\n\n1col_data &lt;- mtcars\n\nskimr::skim(col_data)\n\n\n1\n\nThe mtcars data set comes with the ggplot2 package. Finally I used the skim() function to take a quick look at the data\n\n\n\n\n\nData summary\n\n\nName\ncol_data\n\n\nNumber of rows\n32\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmpg\n0\n1\n20.09\n6.03\n10.40\n15.43\n19.20\n22.80\n33.90\n▃▇▅▁▂\n\n\ncyl\n0\n1\n6.19\n1.79\n4.00\n4.00\n6.00\n8.00\n8.00\n▆▁▃▁▇\n\n\ndisp\n0\n1\n230.72\n123.94\n71.10\n120.83\n196.30\n326.00\n472.00\n▇▃▃▃▂\n\n\nhp\n0\n1\n146.69\n68.56\n52.00\n96.50\n123.00\n180.00\n335.00\n▇▇▆▃▁\n\n\ndrat\n0\n1\n3.60\n0.53\n2.76\n3.08\n3.70\n3.92\n4.93\n▇▃▇▅▁\n\n\nwt\n0\n1\n3.22\n0.98\n1.51\n2.58\n3.33\n3.61\n5.42\n▃▃▇▁▂\n\n\nqsec\n0\n1\n17.85\n1.79\n14.50\n16.89\n17.71\n18.90\n22.90\n▃▇▇▂▁\n\n\nvs\n0\n1\n0.44\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nam\n0\n1\n0.41\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\ngear\n0\n1\n3.69\n0.74\n3.00\n3.00\n4.00\n4.00\n5.00\n▇▁▆▁▂\n\n\ncarb\n0\n1\n2.81\n1.62\n1.00\n2.00\n2.00\n4.00\n8.00\n▇▂▅▁▁\n\n\n\n\n\n\n2col_data &lt;- col_data %&gt;%\n  group_by(cyl) %&gt;%\n  summarize(n = n(),\n            mpg_average = mean(mpg))\n\n3col_plot &lt;- ggplot(col_data,aes(x = as.factor(cyl),\n                                y = mpg_average,\n                                fill = as.factor(cyl))) +\n4  geom_col(color = \"black\") +\n  labs(x = \"Number of Cylinders\", \n       y = \"Average Fuel Economy (mpg)\",\n       title = \"Figure 2. Average Fuel Economy by Cylinder Count\",\n       caption = \"Source: Data from the mtcars data set\") +\n  jtools::theme_apa() +\n5  theme(plot.caption = element_text(hjust = 0)) +\n6  scale_fill_manual(values = c(\"grey50\",\"grey80\",\"grey100\"))\n\nggsave(\"col_plot.png\")\nprint(col_plot)\n\n\n2\n\nI want to modify my data so that I have it grouped by cyl and n() and average_mpg are calculated\n\n3\n\nI’m starting to layer my column plot with this. aes() is where you put your important data (e.g., x and y variables)\n\n4\n\nThe geom_col() tells ggplot2 what type of geom to draw using the aes() data above\n\n5\n\nThe theme(plot.caption = element_text(hjust = 0)) just left justifies the caption\n\n6\n\nThe scale_fill_manual() tells ggplot2 what to assign for the fill variable in the aes() function\n\n\n\n\n\n\n\n\n\nStandard Boxplot Graph\n\nlibrary(tidyverse)\nlibrary(jtools)\nbplot_data &lt;- mtcars\n\nbox_plot &lt;- ggplot(bplot_data,aes(x = as.factor(cyl),\n                                  y = mpg)) + \n1  geom_boxplot(outlier.shape = NA) +\n  labs(x = \"Number of Cylinders\", \n       y = \"Average Fuel Economy (mpg)\", \n       title = \"Figure 3. Boxplot of Distribution of Average Fuel Economy by Cylinder Count\",\n       caption = \"Source: Data from the mtcars data set\") +\n  jtools::theme_apa() +\n  theme(plot.caption = element_text(hjust = 0)) +\n  scale_fill_manual(values = c(\"grey50\",\"grey80\",\"grey100\"))\n\nggsave(\"box_plot.png\")\nprint(box_plot)\n\n\n1\n\nThe beauty of ggplot2 is that there is a lot of overlap between different geom. The data to make a column chart vs a box plot in ggplot2 is just the geom_boxplot vs geom_col function calls shown here\n\n\n\n\n\n\n\n\n\nStandard Violin Plot\n\nlibrary(tidyverse)\nlibrary(jtools)\nviolin_data &lt;- mtcars\n\nviolin_plot &lt;- ggplot(violin_data,aes(x = as.factor(cyl),\n                                      y = mpg,\n                                      fill = as.factor(cyl))) + \n1  geom_violin(draw_quantiles = c(.25,.50,.75)) +\n  labs(x = \"Number of Cylinders\",\n       y = \"Average Fuel Economy (mpg)\",\n       title = \"Figure 4. Violin Plot of Distribution of Average Fuel Economy by Cylinder Count\",\n       caption = \"Source: Data from the mtcars data set\") +\n  jtools::theme_apa() +\n  theme(plot.caption = element_text(hjust = 0)) + \n  scale_fill_manual(values = c(\"grey50\",\"grey80\",\"grey100\"))\n\nggsave(\"violin.png\")\nviolin_plot\n\n\n1\n\nThe draw_quartiles function takes a numeric list to represent the quartiles you want. I’ve chosen the most common of 25%, 50% and 75% but you can input any set of 3 values you’d like\n\n\n\n\n\n\n\n\n\nStandard Line Graph\n\nlibrary(tidyverse)\nlibrary(jtools)\nlibrary(skimr)\n\n1line_data &lt;- txhousing\nskimr::skim(line_data)\n\n\n1\n\nWe’re now using a Texas housing data set found the ggplot2 package. We can take a look at it by using the skim() function in the skimr package\n\n\n\n\n\nData summary\n\n\nName\nline_data\n\n\nNumber of rows\n8602\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncity\n0\n1\n4\n21\n0\n46\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1.00\n2007.30\n4.50\n2000\n2003.00\n2007.00\n2011.00\n2015.0\n▇▆▆▆▅\n\n\nmonth\n0\n1.00\n6.41\n3.44\n1\n3.00\n6.00\n9.00\n12.0\n▇▅▅▅▇\n\n\nsales\n568\n0.93\n549.56\n1110.74\n6\n86.00\n169.00\n467.00\n8945.0\n▇▁▁▁▁\n\n\nvolume\n568\n0.93\n106858620.78\n244933668.97\n835000\n10840000.00\n22986824.00\n75121388.75\n2568156780.0\n▇▁▁▁▁\n\n\nmedian\n616\n0.93\n128131.44\n37359.58\n50000\n100000.00\n123800.00\n150000.00\n304200.0\n▅▇▃▁▁\n\n\nlistings\n1424\n0.83\n3216.90\n5968.33\n0\n682.00\n1283.00\n2953.75\n43107.0\n▇▁▁▁▁\n\n\ninventory\n1467\n0.83\n7.17\n4.61\n0\n4.90\n6.20\n8.15\n55.9\n▇▁▁▁▁\n\n\ndate\n0\n1.00\n2007.75\n4.50\n2000\n2003.83\n2007.75\n2011.67\n2015.5\n▇▇▇▇▇\n\n\n\n\n\n\n2line_data &lt;- line_data %&gt;%\n  group_by(year) %&gt;%\n  summarize(total_sales = sum(sales, na.rm = TRUE))\n\n3line_plot &lt;- ggplot(line_data,aes(x = year,\n                                  y = total_sales)) +\n4  geom_point() +\n5  geom_line() +\n6  labs(x = \"Year\",\n       y = \"Total Housing Sales\",\n       title = \"Figure 5. Total Texas Housing Sales By Year\",\n       caption = \"Source: Data from the ggplot2 data set\") +\n7  scale_x_continuous(breaks = seq(2000,2015,2)) +\n  jtools::theme_apa() +\n  theme(plot.caption = element_text(hjust = 0))\n\nggsave(\"line.png\")\nprint(line_plot)\n\n\n2\n\nIt might be useful to see how sales have changed over time within Texas. As such we might want to summarize the total number of home sales by year. How to do this is illustrated here with a group_by() and summarize() function.\n\n3\n\nWe need to feed the ggplot object our aes() variables. For this we’ve selected year and total_sales as our x and y variable respectively\n\n4\n\nWe might want to add points to our line graph for readability so we can add a geom_point() layer\n\n5\n\nNow we want to add our actual lines. We can do that by providing a geom_line() layer\n\n6\n\nAgain we are adding our typical labels here\n\n7\n\nThis scale_x_continous variable might seem weird. However if we look at our data we will see that our year variable is continuous rather than categorical. Further, the initial breaks skip by intervals of 5 between 2000 and 2015. As such, we may want to change this. We can do that with this function call. The seq function allows us to dictate the min and max of the x values and how we scale our graph. I’ve choosen to go by increments of 2.\n\n\n\n\n\n\n\n\n\nStandard Column Bar Graph W/ Std. Error\n\nlibrary(tidyverse)\nlibrary(jtools)\ncol_SE_data &lt;- mtcars\n\n1col_SE_data &lt;- col_SE_data %&gt;%\n  group_by(cyl) %&gt;%\n  summarize(n = n(),\n            mpg_average = mean(mpg, na.rm = TRUE),\n            sd = sd(mpg, na.rm = FALSE),\n            se = sd/sqrt(n))\n\n2col_SE_plot &lt;- ggplot(col_SE_data,aes(x = as.factor(cyl),\n                                      y = mpg_average,\n                                      fill = as.factor(cyl))) +\n3  geom_col(color = \"black\") +\n4  geom_errorbar(aes(ymax = mpg_average + se,\n                    ymin = mpg_average - se), width = .5) +\n5  labs(x = \"Number of Cylinders\",\n       y = \"Average Fuel Economy (mpg)\",\n       title = \"Figure 6. Average Fuel Economy by Cylinder Count\",\n       caption = \"Source: Data from the mtcars data set\") +\n6  scale_fill_manual(values = c(\"grey50\",\"grey80\",\"grey100\")) +\n  jtools::theme_apa() + \n  theme(plot.caption = element_text(hjust = 0))\n\nggsave(\"col_se.png\")\nprint(col_SE_plot)\n\n\n1\n\nFor the standard error chart, we have to borrow a bit from our previous line chart syntax as we need to manually compute some group level statistics in order to calculate SE. Here we’re grouping by cyl and we need to compute the n and SD to compute the SE. This syntax shows how to do this\n\n2\n\nWe need to provide our aes() factors. Here we want cyl,mpg_average and a fill aesthetic (for color)\n\n3\n\nWe need to add our standard geom_col layer\n\n4\n\nFor our error bars, we want to call geom_errorbar and designate our ymax (upper level) and ymin (lower level) bands. This will do that\n\n5\n\nAdding our usual labels\n\n6\n\nModify our colors for the column"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About This Website",
    "section": "",
    "text": "This is a website focused on providing R enthusiasts with an easy to navigate reference guide for how to perform a vast number of statistical analyses using R, RStudio and occasionally Quarto. I hope you find it useful for your needs"
  },
  {
    "objectID": "Workshop.html",
    "href": "Workshop.html",
    "title": "R Workshop",
    "section": "",
    "text": "If you’re someone who likes to use\n\n\n\n\n\n\n\n\n\n\n  to run data analysis, you’re using already “using”"
  },
  {
    "objectID": "Workshop.html#fun-fact",
    "href": "Workshop.html#fun-fact",
    "title": "R Workshop",
    "section": "Fun Fact",
    "text": "Fun Fact\n\nIf you’re someone who likes to use\n\n\n  to run data analysis, you’re using already “using”"
  },
  {
    "objectID": "Workshop.html#downloading-r",
    "href": "Workshop.html#downloading-r",
    "title": "R Workshop",
    "section": "Downloading R",
    "text": "Downloading R\n\n\n\n\nYou’ll want to download R first before downloading any additional software\nYou can download the newest version of R (4.2) here"
  },
  {
    "objectID": "Workshop.html#downloading-rstudio",
    "href": "Workshop.html#downloading-rstudio",
    "title": "R Workshop",
    "section": "Downloading RStudio",
    "text": "Downloading RStudio\n\n\n\n\nRStudio is the last software program you’ll need to get started\nYou can download the newest version of RStudio (Dec 2022) here"
  },
  {
    "objectID": "Workshop.html#r-quirks",
    "href": "Workshop.html#r-quirks",
    "title": "R Workshop",
    "section": "R “Quirks”",
    "text": "R “Quirks”\n\n\n\nR is case sensitive so what your spelling and the case you use\n\nCase =/= case\n\n\n\n\nR hates spaces for variable. It will not run with a space\n\nvariable_1 is a GOOD variable name\nvariable 1 is a BAD variable name"
  },
  {
    "objectID": "Workshop.html#downloading-materials-for-day-1",
    "href": "Workshop.html#downloading-materials-for-day-1",
    "title": "R Workshop",
    "section": "Downloading Materials For Day 1",
    "text": "Downloading Materials For Day 1\nYou can download all the materials for Day 1 of this workshop here * You want the correlation.qmd, data_clean.qmd, ttest.qmd, and regression.qmd files"
  },
  {
    "objectID": "Workshop.html#installing-and-loading-packages",
    "href": "Workshop.html#installing-and-loading-packages",
    "title": "R Workshop",
    "section": "Installing and Loading Packages",
    "text": "Installing and Loading Packages\n\nR CodeCommentary\n\n\n\n# To Install a Package\ninstall.packages(\"tidyverse\")\n\n# To Load a Package\nlibrary(tidyverse)\n\n\n\n\nYou only have to install a package once (one exception)\nYou must load a library every time you open an R file (.R, .qmd, .rmd, etc) or restart R/RStudio\n\n\n\n\n\n\n\nImportant\n\n\nA new install of R will remove all installed packages. You must either re-install the packages or save them prior to a new R installation. I’ll cover how to save them at a later date"
  },
  {
    "objectID": "Workshop.html#importing-data",
    "href": "Workshop.html#importing-data",
    "title": "R Workshop",
    "section": "Importing Data",
    "text": "Importing Data\n\nCommentaryR Code\n\n\n\nR works best with csv files (smaller in size) but it will take .sav files (SPSS) and other file formats as well (e.g., .tsv)\nOh and obviously it can read Microsoft Excel files\n\n\n\n\n# For CSV\n\ndata &lt;- read.csv(\"file_name.csv\")\n\n# For TSV\n\ndata &lt;- read_tsv(\"file_name.tsv\")\n\n# For SAV\n\nlibrary(haven)\ndata &lt;- read_sav(\"file_name.sav\")\n\n# For EXCEL\n\nlibrary(readxl)\ndata &lt;- read_xlsx(\"file_name.xlsx\")\ndata &lt;- read_xls(\"file_name.xls\")"
  },
  {
    "objectID": "Workshop.html#variable-types",
    "href": "Workshop.html#variable-types",
    "title": "R Workshop",
    "section": "Variable Types",
    "text": "Variable Types\n\nNumerical\n\nA positive or negative number between (- \\(\\infty\\), \\(\\infty\\))\n\nInteger\n\nA positive or negative whole number between (- \\(\\infty\\), \\(\\infty\\))\n\nFactor\n\nA grouping category\n\nCharacter\n\nA text string\n\nLogical\n\nA TRUE or FALSE value (e.g., Is X &gt; 1?)\n\nDate\n\nExactly what you think it is"
  },
  {
    "objectID": "Workshop.html#uses-for-the-dplyr-package",
    "href": "Workshop.html#uses-for-the-dplyr-package",
    "title": "R Workshop",
    "section": "Uses for the dplyr package",
    "text": "Uses for the dplyr package\n\nPrimary use is to transform and manipulate data in a data set\n\nCalculate means, log transform, compute basic summary statistics\n\nAnyone here who has maybe used database data will see it mimics SQL programming\n\n\n\n\n\n\n\nNote\n\n\nFor anyone who might work with database data, you can pull data from external databases with R and RStudio. We won’t cover that in this workshop but you can do it"
  },
  {
    "objectID": "Workshop.html#live-ish-coding",
    "href": "Workshop.html#live-ish-coding",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nOpen the data_clean.qmd file provided. We’re going to walk through some example code and then live code a little bit"
  },
  {
    "objectID": "Workshop.html#uses-for-the-stringr-package",
    "href": "Workshop.html#uses-for-the-stringr-package",
    "title": "R Workshop",
    "section": "Uses for the stringr package",
    "text": "Uses for the stringr package\n\nPrimarily used for dealing with character or string data\n\nUseful for free response questions\nEssentially it’s the dplyr package for string variable types"
  },
  {
    "objectID": "Workshop.html#live-ish-coding-1",
    "href": "Workshop.html#live-ish-coding-1",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nGo back to the data_clean.qmd file provided. We’re going to walk through some example code and then live code a little bit"
  },
  {
    "objectID": "Workshop.html#uses-for-the-lubridate-package",
    "href": "Workshop.html#uses-for-the-lubridate-package",
    "title": "R Workshop",
    "section": "Uses for the lubridate package",
    "text": "Uses for the lubridate package\n\nPrimarily used for dealing with dates\nProvides handy function for converting date formats into other date formats\n\nE.g., (MM-DD-YY to DD-MM-YY or Month Date, Year)\n\n\n\n\n\n\n\n\nTip\n\n\nIt won’t auto convert your dates to weird incorrect formats like certain spreadsheet programs might."
  },
  {
    "objectID": "Workshop.html#live-ish-coding-2",
    "href": "Workshop.html#live-ish-coding-2",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nGo back to the data_clean.qmd file provided. We’re going to walk through some example code and then live code a little bit"
  },
  {
    "objectID": "Workshop.html#uses-for-the-ggplot2-package",
    "href": "Workshop.html#uses-for-the-ggplot2-package",
    "title": "R Workshop",
    "section": "Uses for the ggplot2 package",
    "text": "Uses for the ggplot2 package\n\nThis may be the most popular package download in R and it’s probably not close\nThis is THE visualization package in R. If you can THINK of a graphic, this package can create it\n\nE.g. box plots, box and whisker plots, violin plots, bar graphs, etc\n\nIf you’re REALLY good, you can do this (credit @ralitza_s)"
  },
  {
    "objectID": "Workshop.html#liveish-coding",
    "href": "Workshop.html#liveish-coding",
    "title": "R Workshop",
    "section": "“Live”ish Coding",
    "text": "“Live”ish Coding\n\nGo back to the data_clean.qmd file provided. We’re going to walk through some example code and then live code a little bit"
  },
  {
    "objectID": "Workshop.html#section-1",
    "href": "Workshop.html#section-1",
    "title": "R Workshop",
    "section": "",
    "text": "CommentaryExcelSPSS or SASCSV\n\n\n\nSometimes you want or need to export data you’ve cleaned to another program. Maybe you want to use a program like JASP\nOr you’re not comfortable using R for analyses yet so you want to use SPSS\nOr maybe others on your team use a different program\nR can export to Excel, SPSS, SAS, and CSV files\n\n\n\n\nlibrary(openxlsx)\n\nwrite.xlsx(df,file = \"filename.xlsx\")\n\n\n\n\nlibrary(haven)\n\n# For SPSS\nwrite_sav(df, path = \"filename.sav\")\n\n# For SAS\nwrite_sas(df, path = \"filename.sas\")\n\n\n\n\nwrite.csv(df, file = \"filename.csv\")"
  },
  {
    "objectID": "Workshop.html#assumptions-fields-et-al-2012",
    "href": "Workshop.html#assumptions-fields-et-al-2012",
    "title": "R Workshop",
    "section": "Assumptions (Fields et al, 2012)",
    "text": "Assumptions (Fields et al, 2012)\n\nOn at least an interval scale\nNormality of Residuals"
  },
  {
    "objectID": "Workshop.html#live-ish-coding-3",
    "href": "Workshop.html#live-ish-coding-3",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nPlease see correlation.qmd file provided"
  },
  {
    "objectID": "Workshop.html#assumptions-fields-et-al-2012-1",
    "href": "Workshop.html#assumptions-fields-et-al-2012-1",
    "title": "R Workshop",
    "section": "Assumptions (Fields et al, 2012)",
    "text": "Assumptions (Fields et al, 2012)\n\nNormality of Residuals\nIndependent Observations\nHomogeneity of Variance"
  },
  {
    "objectID": "Workshop.html#live-ish-coding-4",
    "href": "Workshop.html#live-ish-coding-4",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nPlease see the ttest.qmd file provided"
  },
  {
    "objectID": "Workshop.html#assumptions-fields-et-al-2012-2",
    "href": "Workshop.html#assumptions-fields-et-al-2012-2",
    "title": "R Workshop",
    "section": "Assumptions (Fields et al, 2012)",
    "text": "Assumptions (Fields et al, 2012)\n\nOutliers and Influential Cases\nNormality of Residuals\nIndependent Observations\nHomogeneity of Variance\n\n\n\n\n\n\n\nImportant\n\n\nWhile important, outliers and influential cases rarely influence results with a sufficient sample size. Also difficult to say what “is” and “isn’t” an outlier. Outlier shouldn’t always mean removal"
  },
  {
    "objectID": "Workshop.html#live-ish-coding-5",
    "href": "Workshop.html#live-ish-coding-5",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nPlease open the regression.qmd file provided"
  },
  {
    "objectID": "Workshop.html#downloading-material-for-day-2",
    "href": "Workshop.html#downloading-material-for-day-2",
    "title": "R Workshop",
    "section": "Downloading Material For Day 2",
    "text": "Downloading Material For Day 2\nYou can download all the materials for Day 2 of this workshop here * You want the anova.qmd, nonparametric.qmd, intro_qarto.qmd, mlm.qmd, sem.qmd and factor_analysis.qmd"
  },
  {
    "objectID": "Workshop.html#assumptions-fields-et-al-2012-3",
    "href": "Workshop.html#assumptions-fields-et-al-2012-3",
    "title": "R Workshop",
    "section": "Assumptions (Fields et al, 2012)",
    "text": "Assumptions (Fields et al, 2012)\n\nNormality Within Groups\nHomogeneity of Variance\nIndependent Observations"
  },
  {
    "objectID": "Workshop.html#live-ish-coding-6",
    "href": "Workshop.html#live-ish-coding-6",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nPlease open the anova.qmd file provided"
  },
  {
    "objectID": "Workshop.html#live-ish-coding-7",
    "href": "Workshop.html#live-ish-coding-7",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nPlease open the nonparametric.qmd file"
  },
  {
    "objectID": "Workshop.html#efa-assumptions-fields-et-al-2012",
    "href": "Workshop.html#efa-assumptions-fields-et-al-2012",
    "title": "R Workshop",
    "section": "EFA Assumptions (Fields et al, 2012)",
    "text": "EFA Assumptions (Fields et al, 2012)\n\nSufficient Sample Size\nNormality of Items\nCorrelation Between Items1\nAppropriate Determinant (Det \\(&gt;\\) 1 x 10-5)\n\n\n\n\n\n\n\nImportant\n\n\n\nWe want variables to correlate however we do not want them to correlate either  too low (r \\(&lt;\\) .30) or too high (r \\(&gt;\\) .80) across multiple items"
  },
  {
    "objectID": "Workshop.html#cfa-assumptions",
    "href": "Workshop.html#cfa-assumptions",
    "title": "R Workshop",
    "section": "CFA Assumptions",
    "text": "CFA Assumptions\n\nMultivariate Normality"
  },
  {
    "objectID": "Workshop.html#live-ish-coding-8",
    "href": "Workshop.html#live-ish-coding-8",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nPlease open the factor_analysis.qmd file"
  },
  {
    "objectID": "Workshop.html#assumptions-to-test-kaplan-2001-p.-15218",
    "href": "Workshop.html#assumptions-to-test-kaplan-2001-p.-15218",
    "title": "R Workshop",
    "section": "Assumptions To Test (Kaplan, 2001, p. 15218)",
    "text": "Assumptions To Test (Kaplan, 2001, p. 15218)\n\nMultivariate Normality\nNo Systematic Missing Data\nSufficiently Large Sample Size\nCorrect Model Specification"
  },
  {
    "objectID": "Workshop.html#live-ish-coding-9",
    "href": "Workshop.html#live-ish-coding-9",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nPlease open the sem.qmd file"
  },
  {
    "objectID": "Workshop.html#assumptions-to-test",
    "href": "Workshop.html#assumptions-to-test",
    "title": "R Workshop",
    "section": "Assumptions To Test",
    "text": "Assumptions To Test\n\nOutliers and Influential Cases\nNormality of Residuals\nIndependent Observations1\nHomogeneity of Variance\n\n\n\n\n\n\n\nA Note On Independence\n\n\n\nThis assumption is not necessarily a concern given that MLM assumes observations are nested (Fields et al, 2012)"
  },
  {
    "objectID": "Workshop.html#live-ish-coding-10",
    "href": "Workshop.html#live-ish-coding-10",
    "title": "R Workshop",
    "section": "Live-ish Coding",
    "text": "Live-ish Coding\n\nPlease open the mlm.qmd file"
  },
  {
    "objectID": "Workshop.html#the-holy-grail-of-reproducibility",
    "href": "Workshop.html#the-holy-grail-of-reproducibility",
    "title": "R Workshop",
    "section": "The Holy Grail of Reproducibility",
    "text": "The Holy Grail of Reproducibility\n\nWhat if I told you that it was possible to generate 95% of what you need for a manuscript within RStudio AND you could integrate your analyses as well?\nWhat if I also said you could export this to Microsoft Word?"
  },
  {
    "objectID": "Workshop.html#lets-talk-about-quarto",
    "href": "Workshop.html#lets-talk-about-quarto",
    "title": "R Workshop",
    "section": "Let’s Talk About Quarto",
    "text": "Let’s Talk About Quarto\n\nPlease open the intro_quarto.qmd file"
  },
  {
    "objectID": "Workshop.html#section-2",
    "href": "Workshop.html#section-2",
    "title": "R Workshop",
    "section": "",
    "text": "Firstly, thank you for your time this weekend and I hope you’ve learned something\nSecond, this is A LOT. I crammed stuffed about 2 years of statistical analyses time and practice into like 2 days. It’s okay and normal if you’re swimming. I’m here if anyone has any questions after or even if they’re using R and trying to do an actual analysis in R. People ask me for help all the time. I’m happy to help"
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "R Workshop: Correlation",
    "section": "",
    "text": "Correlations might be the most common statistical test run (especially early on in a project). For this part of the workshop we will be using the mtcars data set due to its many numerical values that we can assess for correlation. Below we will see the first 10 rows of the data set displayed\n\nlibrary(tidyverse)\nlibrary(car)\nlibrary(psych)\n\ndata &lt;- mtcars\n\n1print(head(data,10))\n\n\n1\n\nDisplay the first 10 rows of the mtcars data. You can also return the last 10 rows with the following function tail(data,10)\n\n\n\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280          19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4"
  },
  {
    "objectID": "correlation.html#introduction",
    "href": "correlation.html#introduction",
    "title": "R Workshop: Correlation",
    "section": "",
    "text": "Correlations might be the most common statistical test run (especially early on in a project). For this part of the workshop we will be using the mtcars data set due to its many numerical values that we can assess for correlation. Below we will see the first 10 rows of the data set displayed\n\nlibrary(tidyverse)\nlibrary(car)\nlibrary(psych)\n\ndata &lt;- mtcars\n\n1print(head(data,10))\n\n\n1\n\nDisplay the first 10 rows of the mtcars data. You can also return the last 10 rows with the following function tail(data,10)\n\n\n\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280          19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4"
  },
  {
    "objectID": "correlation.html#statistical-assumptions",
    "href": "correlation.html#statistical-assumptions",
    "title": "R Workshop: Correlation",
    "section": "Statistical Assumptions",
    "text": "Statistical Assumptions\nThe primary assumption of a Pearson’s correlation coefficient is that the data is on some kind of interval scale. However, if we wish to generalize, we must have a random large sample (unlikely) and the individual variables should be roughly normally distributed. This is the assumption we will focus on for this part of the workshop.\n\nNormality of Variables\nFor this part of the workshop, we are going to focus on the mpg and wt variables. We will be looking at normality both statistically1 as well as graphically.\n\n\n\n\n\n\nA Note About Statistical Assumption Testing\n\n\n\n\nOdds are your statistical test is going to fail pretty much every single time. Particularly if you use something like a Shapiro Wilk test, but we’ll look at it anyway\n\n\n\n\nGraphical Depiction of Normality Assumption (mpg)\n\n1mpg_plot &lt;- ggplot(data,aes(x = mpg)) +\n  geom_histogram(aes(y=after_stat(density))) +\n  stat_function(fun = dnorm,\n                args = list(mean = mean(data$mpg),\n                            sd = sd(data$mpg)),\n                col = \"blue\") +\n  theme_classic()\n\nprint(mpg_plot)\n\n\n\n\n1\n\nThis should look very familiar to the histogram part of the workshop\n\n\n\n\n\n\n\n\n\n\n\nStatistical Depiction of Normality Assumption (mpg)\n\n1print(psych::describe(data$mpg))\n\n2print(shapiro.test(data$mpg))\n\n\n\n\n1\n\nThe psych package has a bunch of nifty functions for social science research. One is the describe() function which gives you a bunch of variable level summary statistics (e.g., mean, median, se, etc.)\n\n2\n\nThe shapiro.test() performs a Shapiro Wilk test of normality. Keep in mind this particular test is very sensitive to sample sizes\n\n\n\n\n\n\n   vars  n  mean   sd median trimmed  mad  min  max range skew kurtosis   se\nX1    1 32 20.09 6.03   19.2    19.7 5.41 10.4 33.9  23.5 0.61    -0.37 1.07\n\n    Shapiro-Wilk normality test\n\ndata:  data$mpg\nW = 0.94756, p-value = 0.1229\n\n\n\n\n\n\nGraphical Depiction of Normality Assumption (wt)\n\nwt_plot &lt;- ggplot(data,aes(x = wt)) +\n  geom_histogram(aes(y=after_stat(density))) + \n  stat_function(fun = dnorm,\n                args = list(mean = mean(data$wt),\n                            sd = sd(data$wt)),\n                col = \"blue\") + \n  theme_classic()\n\nprint(wt_plot)\n\n\n\n\n\n\n\n\n\n\nStatistical Depiction of Normality Assumption (wt)\n\nprint(psych::describe(data$wt))\nprint(shapiro.test(data$wt))\n\n\n\n   vars  n mean   sd median trimmed  mad  min  max range skew kurtosis   se\nX1    1 32 3.22 0.98   3.33    3.15 0.77 1.51 5.42  3.91 0.42    -0.02 0.17\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  data$wt\nW = 0.94326, p-value = 0.09265"
  },
  {
    "objectID": "correlation.html#running-an-actual-correlation",
    "href": "correlation.html#running-an-actual-correlation",
    "title": "R Workshop: Correlation",
    "section": "Running An Actual Correlation",
    "text": "Running An Actual Correlation\nThere are multiple packages and methods for calculating a correlation in R depending on what you want to assess. The best to use for psychology is probably the corr.test() function in the psych package because it allows you to change the type of correlation you wish to compute (e.g., spearman vs pearson) as well as generate confidence intervals and do p value adjustments\n\n1corr_results &lt;- corr.test(x = data$mpg,\n2          y = data$wt,\n3          use = \"pairwise\",\n4          method = \"pearson\",\n5          adjust = \"holm\")\n\n?corr.test()\n\n\n1\n\nChoose one of your variables to be your x variable\n\n2\n\nChoose the other to be your y variable\n\n3\n\nYou can choose “pairwise” or “complete”. For information on what each does, use the following function to access the documentation: ?psych::corr.test()\n\n4\n\nYou can adjust method to be other ones like “spearman”\n\n5\n\nYou can also use “bonferroni” among a few others\n\n\n\n\nBelow we will see the output of the correlation results as you might be used to seeing in a program like SPSS.\n\nprint(corr_results)\n\nCall:corr.test(x = data$mpg, y = data$wt, use = \"pairwise\", method = \"pearson\", \n    adjust = \"holm\")\nCorrelation matrix \n[1] -0.87\nSample Size \n[1] 32\nThese are the unadjusted probability values.\n  The probability values  adjusted for multiple tests are in the p.adj object. \n[1] 0\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\nWhile the above is great, notice we didn’t get a confidence interval output despite asking for it with ci = TRUE. Sometimes R will store complex computations within the output object (e.g., corr_results). To get this output we can put a $ after the output. If there is extra information stored, but not shown, we’ll get a drop down box. We want the ci option. Below we will see the output that results from this. We should see the following:\n\\(r\\) = -.87 , \\(p\\) &lt; .001 with a CI = [-.93,-.74]\n\nprint(corr_results$ci)\n\n           lower          r      upper            p\nNA-NA -0.9338264 -0.8676594 -0.7440872 1.293959e-10"
  },
  {
    "objectID": "ttest.html#assumptions-of-t-tests",
    "href": "ttest.html#assumptions-of-t-tests",
    "title": "R Workshop: T-test",
    "section": "Assumptions of T-tests",
    "text": "Assumptions of T-tests\n\nNormality of Residuals\n\nlibrary(tidyverse)\n\ndata &lt;- starwars %&gt;% \n1  filter(sex == \"male\" | sex == \"female\")\n\n2model &lt;- lm(height ~ sex, data = data)\n\n3residuals &lt;- data.frame(res = residuals(model))\n\n4problem &lt;- residuals %&gt;% filter(res &gt; 2.5 | res &lt; -2.5)\n\nnrow(problem)/nrow(data) \n\n\n1\n\nFiltering for male the female using the filter() function\n\n2\n\nRunning a linear regression (that is a t-test here) to get residuals\n\n3\n\nCalculate residuals for the observations\n\n4\n\nFind potentially problematic observations\n\n\n\n\n[1] 0.8157895\n\n\n\nGraphical Depiction of Normality of Residuals\n\n1residual_graph &lt;- ggplot(residuals,aes(x = res)) +\n2  geom_histogram(aes(y=after_stat(density))) +\n3  stat_function(fun = dnorm,\n                args = list(mean = mean(residuals$res),\n                            sd = sd(residuals$res)),\n                col = \"blue\",\n                linewidth = 1) +\n  theme_classic()\n\n4print(residual_graph)\n\n\n1\n\nWe are plotting the residuals here. We give ggplot a geom (i.e., histogram)\n\n2\n\nWe also give some other arguments like a density distribution.\n\n3\n\nHere we are basically providing what is needed to draw a normal distribution given the data using the stat_function() function. The col and linewidth arguments simply change the colr and size of the normal curve. The theme_classic() just changes some aesthetic things. I personally prefer this theme for all ggplot2 graphs\n\n4\n\nprint will show us the graph output\n\n\n\n\n\n\n\n\n\nStatistical Depiction of Normality of Residuals\nWe can also test the assumption statistically using the shapiro.test() function here\n\nshapiro.test(residuals$res)\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals$res\nW = 0.84515, p-value = 3.515e-07\n\n\n\n\n\nHomogeneity of Variance\nHomogeneity of variance is important even for a basic t-test. Below is how we might go about testing this assumption.\n\nGraphical Depiction of Homogeneity of Variance\n\n1variance_boxplot &lt;- ggplot(data,aes(x = sex,\n                               y = height)) +\n2  geom_boxplot() +\n  theme_classic()\n\nprint(variance_boxplot)\n\n\n1\n\nGraphically we can represent this as a boxplot with the group variable as the x and the outcome as the y. We see this here\n\n2\n\nWe again provide a geom for ggplot2 to use and provide a theme() choice here\n\n\n\n\n\n\n\n\n\nStatistical Depiction of Homogeneity of Variance\nWe can also test the assumption using the Bartlett test. This can be shown below\n\nbartlett.test(height ~ sex,data)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  height by sex\nBartlett's K-squared = 11.126, df = 1, p-value = 0.0008511"
  },
  {
    "objectID": "ttest.html#running-a-t-test",
    "href": "ttest.html#running-a-t-test",
    "title": "R Workshop: T-test",
    "section": "Running a T-test",
    "text": "Running a T-test\n\n1t.test(height ~ sex, data = data)\n\n\n1\n\nThe t.test() function will take a DV and IV argument as well as the dataframe used. We can see this here\n\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  height by sex\nt = -1.5876, df = 55.148, p-value = 0.1181\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -22.256861   2.579668\nsample estimates:\nmean in group female   mean in group male \n            169.2667             179.1053"
  },
  {
    "objectID": "regression.html#linear-regression",
    "href": "regression.html#linear-regression",
    "title": "R Workshop: Linear & Logistic Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nlibrary(tidyverse)\nlibrary(car)\n\n1data &lt;- starwars %&gt;% select(height,mass,sex) %&gt;% na.omit()\n\n\n1\n\nThe above code takes the starwars data set and uses the select() function to pull out the columns labelled height, mass and sex. Then using the na.omit() function, I’ve removed (using listwise deletion) any rows which contain missing values\n\n\n\n\n\nRunning a Linear Regression\nSimilar to correlation, regression (particularly multiple regression) is very common in the social sciences. As such, lets dive into an example again using the starwars data set.\n\ndata &lt;- data %&gt;% \n1  filter(sex == \"male\" | sex == \"female\") %&gt;%\n2  mutate(sex = dplyr::recode(sex,\n                             \"male\" = 1,\n                             \"female\" = 0))\n\n3linear_regression &lt;- lm(height ~ sex + mass, data = data)\n\n4summary(linear_regression)\n\n\n1\n\nHere I want to filter out the data set for observations (rows) that meet the following conditions (in this case those with a sex “value” of either “male” or “female”).\n\n2\n\nI then want to recode the sex variables to a numeric value for the purposes of doing the linear regression using the recode() function. It takes the column as an input and takes the syntax “old value” to “new value”.\n\n3\n\nThis is a basic linear regression formula using the lm() function. It takes the form DV ~ IV + IV, df).\n\n4\n\nTo see the results of the regression, we simply run the summary() function and specify the name we assigned the regression (i.e., linear_regression).\n\n\n\n\n\nCall:\nlm(formula = height ~ sex + mass, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.431  -3.892   2.007   8.108  48.526 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 113.8554     9.6656  11.779 4.91e-16 ***\nsex         -18.0741     8.5390  -2.117   0.0393 *  \nmass          1.0144     0.1167   8.694 1.43e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.78 on 50 degrees of freedom\nMultiple R-squared:  0.6056,    Adjusted R-squared:  0.5898 \nF-statistic: 38.38 on 2 and 50 DF,  p-value: 7.938e-11"
  },
  {
    "objectID": "regression.html#assumptions-of-linear-regression",
    "href": "regression.html#assumptions-of-linear-regression",
    "title": "R Workshop: Linear & Logistic Regression",
    "section": "Assumptions of Linear Regression",
    "text": "Assumptions of Linear Regression\nAs some will attest to, we as social scientists don’t always explicitly test our statistical assumptions (this is a problem). However, as a parametric test, regression consists of the following key assumptions: homogeneity of variance, residual normality, lack of multicollinearity, and the independence of errors. Further, we likely want to at least investigate the potentiality that there are outliers and influential cases. I’m going to show you how to test each of these assumptions.\n\nHomogeneity of Variance\n\nGraphical\n\n# Should Be a Straight Line\n1plot(linear_regression,1)\n\n\n1\n\nThe plot(object,1) function when applied to a regression object will give you a plot assessing visually the homogeneity of variance assumption. The line should be roughly straight.\n\n\n\n\n\n\n\n\n\nStatistical\n\nlibrary(lmtest)\n# Goldfield Quandt Test (Less than .05 BAD)\n1gqtest(linear_regression)\n\n\n1\n\nStatistically, we can also investigate homogeneity of variance using a Goldfield Quandt Test. This is one test we hope is not statistically significant. To do this, we use the gqtest() function in the lmtest package.\n\n\n\n\n\n    Goldfeld-Quandt test\n\ndata:  linear_regression\nGQ = 1.1014, df1 = 24, df2 = 23, p-value = 0.4096\nalternative hypothesis: variance increases from segment 1 to 2\n\n\n\n\n\nResidual Normality\nContrary to popular belief, normality doesn’t typically refer to the distribution of each individual variable in a regression. What matters is that the model residuals will be roughly normally distributed. Below we will go through this. First, we’ll look at the assumption graphically using what is known as a qq plot\n\nGraphical\n\n# Should Follow Straight Diagonal Line\n1plot(linear_regression,2)\n\n\n1\n\nUse the plot(object,2) with a regression will give you a standard qqplot with a reference line. This line should look linear.\n\n\n\n\n\n\n\n\n\nStatistical\nWe can also assess the assumption statistically using a Shapiro Wilks test. Keep in mind this test can be heavily influenced by sample size but nonetheless it is a start.\n\n1shapiro.test(residuals(linear_regression))\n\n\n1\n\nThe shapiro.test() function will get us what we want. However, we need to make sure we get the residuals of the model so we need to use the residuals() function too.\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(linear_regression)\nW = 0.93558, p-value = 0.006747\n\n\n\n\n\nMulticollinearity\nMulticollinearity (or too high of correlation between variables) can be an issue. One could look at the correlation matrix but that’s highly subjective. A better idea might be to use what is known as variance inflation factors. Essentially higher values (i.e., north of 10) indicate an issue with said factor (i.e., variable). Below is the code for this using the car package.\n\nStatistical (VIF)\n\n# Individual Predictors (Less than 10 is OK)\n1car::vif(linear_regression)\n\n# Mean Across Predictors (Ideally Around 1)\n2mean(vif(linear_regression))\n\n# Tolerance (Greater than .20 Ideal)\n31/vif(linear_regression)\n\n\n1\n\nLess than 10 is solid for this metric\n\n2\n\nYou want the mean value to be around 1.0\n\n3\n\nYou want tolerance to be &gt; .20\n\n\n\n\n   sex   mass \n1.1485 1.1485 \n[1] 1.1485\n      sex      mass \n0.8707007 0.8707007 \n\n\n\n\n\nIndependence of Errors\nLong story short, errors shouldn’t be correlated with each other. We can assess this statistically using the Durbin Watson test. Here we want the test to not be statistically significant. We can use the durbinWatsonTest() function from the car function for this. The code is shown below.\n\nStatistical (Durbin Watson Test)\n\nlibrary(car)\ncar::durbinWatsonTest(linear_regression)\n\n lag Autocorrelation D-W Statistic p-value\n   1      0.09534174      1.807892   0.466\n Alternative hypothesis: rho != 0\n\n\n\n\n\nFinding Outliers & Influential Cases\n\nResiduals\nTypically, residuals outside of the range of -2.5 to 2.5 are a potential problem. However, this doesn’t mean we should discard the data necessarily. We have to feel they are distinctly not in the population we’re hoping to investigate. However, typically as a rule of thumb, having more than 5% of your cases being outside of this range is not ideal.\n\n# Greater than 5% of Data Potentially Problematic\ndata$residuals &lt;- resid(linear_regression)\nsummary(data$residuals)\n\n1problem_residuals &lt;- data %&gt;% filter(residuals &gt; 2.5 | residuals &lt; -2.5)\n\n2nrow(problem_residuals)/nrow(data)\n\n\n1\n\nThis will filter the data set into observations which meet the criteria\n\n2\n\nThis tells me what percent of the cases are potentially problematic\n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-47.431  -3.892   2.007   0.000   8.108  48.526 \n[1] 0.7358491\n\n\n\n\nInfluential Cases\nMaybe a more useful investigation is that involving influential cases (i.e., cases which have undue influence on the results). There are several metrics one can you. I’m just going to focus on Cooks distance. Essentially values over 1 are potentially problematic. Below is the code for this\n\n1data$cooks_dist &lt;- round(cooks.distance(linear_regression),5)\n# Greater Than 1 is Problematic\n2summary(data$cooks_dist)\n\n\n1\n\nThis rounds the values to 5 decimal places for ease of reading in addition to calculating the Cooks distance for each observation.\n\n2\n\nsummary() function says the max value is .36 so there are no individual cases here that are having undue influence on the results.\n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.00016 0.00205 0.03116 0.02715 0.36309"
  },
  {
    "objectID": "regression.html#logistic-regression",
    "href": "regression.html#logistic-regression",
    "title": "R Workshop: Linear & Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nSo logistic regression is something I had to learn in order to do this workshop. Essentially however, logistic regression is used to give odds ratios of whether an observation belongs in one of two categories based on a set of inputs. In industry, this is actually considered a form of machine learning (so is most other regression). You can actually go above and beyond two categories but we’re not going to talk about polynomial regression in this workshop. Below is how we run a logistic regression in R.\n\nRunning a Logistic Regression\n\n# Create Logistic Model w/ Multiple Predictors\n1log_regression &lt;- glm(sex ~ height + mass, family = binomial(link = \"logit\"), data = data)\n2summary(log_regression)\n\n# Determine Chi Square Diff\n3modelChi &lt;- log_regression$null.deviance - log_regression$deviance\nmodeldf &lt;- log_regression$df.null - log_regression$df.residual\nchisq_prob &lt;- 1 - pchisq(modelChi,modeldf)\nprint(chisq_prob)\n\n# Odds Ratios\n4exp(log_regression$coefficients)\n\n# CI\n5exp(confint(log_regression))\n\n# Effect Size\n6effect_size &lt;- modelChi/log_regression$null.deviance\nprint(effect_size)\n\n\n1\n\nThis is your basic logistic regression formula. The family = binomial(link = \"logit\") tells R that we want this model to be a logistic regression.\n\n2\n\nSummarizes the results of the logistic regression\n\n3\n\nCalculates a Chi Square Test to determine if the model is better than chance (less than .05)\n\n4\n\nExponentiation the coefficients will give you the odds ratios\n\n5\n\nWe can also get the confidence intervals of the odds ratios using the confint() function\n\n6\n\nWe can calculate an effect size using this formula\n\n\n\n\n\nCall:\nglm(formula = sex ~ height + mass, family = binomial(link = \"logit\"), \n    data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  3.27693    2.11194   1.552  0.12075   \nheight      -0.06519    0.02458  -2.652  0.00799 **\nmass         0.14554    0.04719   3.084  0.00204 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 48.292  on 52  degrees of freedom\nResidual deviance: 30.705  on 50  degrees of freedom\nAIC: 36.705\n\nNumber of Fisher Scoring iterations: 6\n\n[1] 0.0001517089\n(Intercept)      height        mass \n 26.4943310   0.9368878   1.1566654 \n                2.5 %       97.5 %\n(Intercept) 0.5524946 3056.5379217\nheight      0.8856929    0.9779379\nmass        1.0685581    1.2942792\n[1] 0.3641807"
  },
  {
    "objectID": "regression.html#assumptions-of-logistic-regression",
    "href": "regression.html#assumptions-of-logistic-regression",
    "title": "R Workshop: Linear & Logistic Regression",
    "section": "Assumptions of Logistic Regression",
    "text": "Assumptions of Logistic Regression\nLike regular regression, logistic regression also has assumptions that must be met. They are the linearity of each predictor with the log of the outcome, independence of errors, and multicollinearity. I will show you how to test each of these below.\n\nLinearity w/ Log of Outcome (Each Predictor)\n\nStatistical\n\n1data$massINT &lt;- data$mass*log(data$mass)\ndata$heightINT &lt;- data$height*log(data$height)\n\n2linearity_assumption &lt;- glm(sex ~ height + mass + massINT + heightINT, family = binomial(link = \"logit\"), data = data)\n\nsummary(linearity_assumption)\n\n\n1\n\nYou want the interaction terms of each predictor with the log of the outcome. These need to go into the logistic regression.\n\n2\n\nYou can see them added here. You want none of the interaction terms to be statistically significant\n\n\n\n\n\nCall:\nglm(formula = sex ~ height + mass + massINT + heightINT, family = binomial(link = \"logit\"), \n    data = data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) 1042.3027   703.0356   1.483   0.1382  \nheight       -41.2936    26.8629  -1.537   0.1242  \nmass           5.3989     3.2018   1.686   0.0918 .\nmassINT       -0.9727     0.5926  -1.641   0.1007  \nheightINT      6.7461     4.3829   1.539   0.1238  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 48.2922  on 52  degrees of freedom\nResidual deviance:  8.4973  on 48  degrees of freedom\nAIC: 18.497\n\nNumber of Fisher Scoring iterations: 13\n\n\n\n\nGraphical\nYou can also just plot the logistic regression with the interaction terms and see if you get a roughly straight line.\n\nplot(linearity_assumption,2)\n\n\n\n\n\n\n\nIndependence of Errors\nLike regular regression, we can test this assumption using a Durbin Watson Test. The code here is below.\n\nStatistical\n\ndurbinWatsonTest(log_regression)\n\n lag Autocorrelation D-W Statistic p-value\n   1       0.0998344      1.776247   0.438\n Alternative hypothesis: rho != 0\n\n\n\n\n\nMulticollinearity\nMulticollinearity can also be assessed just like regression by using the vif() function as well as the 1/vif() function. We want these to be less than 10 and greater than .20 respectively.\n\nStatistical\n\nvif(log_regression)\n\n  height     mass \n4.812415 4.812415 \n\n1/vif(log_regression)\n\n   height      mass \n0.2077959 0.2077959 \n\n\n\n\n\nFinding Outliers & Influential Cases\n\nResiduals\nResiduals can be assessed the same was as it is in regular linear regression. The code is below as a refresher.\n\n# Greater than 5% of Data Potentially Problematic\ndata$residuals_log &lt;- resid(log_regression)\nsummary(data$residuals_log)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-2.6335  0.0324  0.2731  0.1136  0.3751  2.1968 \n\nlog_prob &lt;- data %&gt;% filter(residuals_log &gt; 2 | residuals_log &lt; -2)\n\nround(nrow(log_prob)/nrow(data),3)\n\n[1] 0.038\n\n\n\n\nInfluential Cases\nAs with residuals and linear regression compared to logistic regression, the same is true for influential cases. You can see the code for this below.\n\ndata$cooks_dist_log &lt;- cooks.distance(log_regression)\n# Greater Than 1 is Problematic\nsummary(data$cooks_dist_log)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0000000 0.0002653 0.0006915 0.0267272 0.0199929 0.5227436"
  },
  {
    "objectID": "ANOVA.html#running-anova",
    "href": "ANOVA.html#running-anova",
    "title": "R Workshop: ANOVA, ANCOVA & Factorial ANOVA",
    "section": "Running ANOVA",
    "text": "Running ANOVA\n\nlibrary(car)\nlibrary(psych)\nlibrary(multcomp)\nlibrary(effects)\nlibrary(tidyverse)\nlibrary(sjstats)\n\n\ndata &lt;- dplyr::starwars\n\ndata &lt;- data %&gt;% \n  dplyr::select(height,mass,hair_color,species,sex) %&gt;% \n  na.omit() %&gt;% \n1  filter(species == \"Gungan\" | species == \"Human\" | species == \"Wookiee\")\n  \n2height_species_aov &lt;- aov(height ~ species, data = data)\n3summary(height_species_aov)\n\n# Post Hoc Test (Tukey)\n4TukeyHSD(height_species_aov)\n# Bonferroni Adjustment vs Tukey\n5pairwise.t.test(data$height, data$species, p.adjust.method = \"bonferroni\")\n\n\n1\n\nHere we are using the filter() function to filter for “Gungan”, “Human” and “Wookiee”\n\n2\n\nThis is the aov() function. Here we are specifying the ANOVA formula.\n\n3\n\nThe summary() function will give you the output of the ANOVA\n\n4\n\nBecause the species variable has more than 2 conditions, we might want to figure out which comparisons are statistically different from each other. The TukeyHSD() function allows us to perform a post hoc Tukey test\n\n5\n\nMaybe Tukey isn’t your favorite correction. Another route might be to use a pairwise t test that corrects for multiple comparisons using a Bonferroni correction. You can do that with the pairwise.t.test() function.\n\n\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nspecies      2   6118    3059    22.5 3.86e-06 ***\nResiduals   23   3127     136                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = height ~ species, data = data)\n\n$species\n                    diff        lwr       upr     p adj\nHuman-Gungan   -30.45455 -52.022229 -8.886862 0.0048319\nWookiee-Gungan  21.00000  -8.202782 50.202782 0.1915885\nWookiee-Human   51.45455  29.886862 73.022229 0.0000125\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  data$height and data$species \n\n        Gungan Human  \nHuman   0.0053 -      \nWookiee 0.2545 1.3e-05\n\nP value adjustment method: bonferroni"
  },
  {
    "objectID": "ANOVA.html#assumptions-of-anova",
    "href": "ANOVA.html#assumptions-of-anova",
    "title": "R Workshop: ANOVA, ANCOVA & Factorial ANOVA",
    "section": "Assumptions of ANOVA",
    "text": "Assumptions of ANOVA\nAs ANOVA is just a special case of linear regression, the same assumptions exist for ANOVA that exist for linear regression. We will test each assumption below. The code will look awfully similar to the code used for linear regression previously\n\nModel Normality\n\nGraphical\n\ndensity_plot &lt;- data %&gt;% ggplot(aes(x = height_species_aov$residuals)) +\n  geom_histogram(aes(y= after_stat(density)),binwidth = 1) +\n  stat_function(fun = dnorm,\n                args = list(mean = mean(height_species_aov$residuals),\n                            sd = sd(height_species_aov$residuals)),\n                col = \"blue\",\n                linewidth = 1) +\n  labs(title = \"Figure 1. Histogram of Model Residual Scores\",\n       x = \"Model Residuals\",\n       y = \"Density\")\n\ndensity_plot\n\n\n\n# OR\n\nqqnorm(height_species_aov$residuals)\nqqline(height_species_aov$residuals, col = \"blue\")\n\n\n\n\n1.When running the qqnorm() and qqline() functions at the same time, you will get a qq-plot which will graphically assess the normality of the argument given (in this case the model residuals). The col argument simply tells R which color to make the reference line.\n\n\nStatistical\n\n1psych::describe(height_species_aov$residuals)\nshapiro.test(height_species_aov$residuals)\n\n\n1\n\nAnother way to assess normality is to look at the skew and kurtosis of the data. Typically skew and kurtosis between -1 and 1 are acceptable. We can see this using the describe() function within the psych package.\n\n\n\n\n   vars  n mean    sd median trimmed   mad    min   max range  skew kurtosis\nX1    1 26    0 11.18   1.45    0.35 10.38 -29.55 22.45    52 -0.44     0.16\n     se\nX1 2.19\n\n    Shapiro-Wilk normality test\n\ndata:  height_species_aov$residuals\nW = 0.97347, p-value = 0.7143\n\n\n\n\n\nHomogeneity of Variance\nAs with regression, homogeneity of variance is also important for ANOVA. We will graphically and statistically assess this assumption below.\n\nGraphical\n\n1boxplot(data$height ~ data$species)\n\n\n1\n\nThe boxplot() function will give us a visual representation of the DV at each level of the IV. We are looking for boxplots that are roughly the same height across each group.\n\n\n\n\n\n\n\n\n\nStatistical\nStatistically, we can assess homogeneity of variance using Levene’s Test. Here we want the results to not be statistically significant.\n\n1leveneTest(data$height ~ data$species)\n\n\n1\n\nThe leveneTest() function from the car package takes a DV and IV as arguments.\n\n\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  2   1.168 0.3288\n      23"
  },
  {
    "objectID": "ANOVA.html#introduction-to-ancova",
    "href": "ANOVA.html#introduction-to-ancova",
    "title": "R Workshop: ANOVA, ANCOVA & Factorial ANOVA",
    "section": "Introduction to ANCOVA",
    "text": "Introduction to ANCOVA"
  },
  {
    "objectID": "ANOVA.html#running-ancova",
    "href": "ANOVA.html#running-ancova",
    "title": "R Workshop: ANOVA, ANCOVA & Factorial ANOVA",
    "section": "Running ANCOVA",
    "text": "Running ANCOVA\nANCOVA refers to any ANOVA model with 2 or more parameters. These models require similar assumptions to ANOVA with a couple of additional ones. Below we will see how to run an ANCOVA and then test its assumptions.\n\ndata &lt;- data %&gt;% mutate(species = as.factor(species))\n# Type I SS\nancova &lt;- aov(height ~ species + mass, data = data)\n1summary(ancova)\n# Reversed Order\nancova2 &lt;- aov(height ~ mass + species, data = data)\n2summary(ancova2)\n\n3car::Anova(ancova2, type = \"III\")\n\n# Std Means\nsummary_data &lt;- data %&gt;% \n  dplyr::group_by(species) %&gt;% \n  dplyr::summarise(n = n(),\n                   mean = mean(height))\n\n# Adjust means for covariate effect\n4adjustedMeans&lt;- effect(\"species\", ancova, se=TRUE)\nsummary(adjustedMeans)\nadjustedMeans$se\n\n# Post Hoc Tests\n\n5posthoc &lt;- multcomp::glht(ancova, linfct = multcomp::mcp(species = \"Tukey\"))\nsummary(posthoc)\nconfint(posthoc)\n\n# Effect size\n6sjstats::anova_stats(car::Anova(ancova,type = \"III\"))\n\n\n1\n\nThis displays our ANCOVA output with species and then mass.\n\n2\n\nThis displays our ANCOVA output with mass and then species. We can see that 1 and 2 are different. This is because R by default does a sequential ANCOVA so order matters.\n\n3\n\nTo get around this, we can use the Anova() function in the car package to specify that we want Type III sums of squares. This will then generate an ANCOVA where the order of predictors doesn’t matter\n\n4\n\nWhen having covariates, one might wish to report adjusted means. We can do that using the effect() function from the effects package.\n\n5\n\nTo run a post hoc analysis, we need to use the ghlt() function from the multcomp package. There are additional corrections you can do outside of Tukey. To investigate, please see the documentation.\n\n6\n\nTo get basic ANOVA effect size statistics, we can use the anova_stats() function from the sjstats package.\n\n\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nspecies      2   6118  3059.0   37.27 8.61e-08 ***\nmass         1   1322  1321.8   16.10 0.000584 ***\nResiduals   22   1806    82.1                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nmass         1   3737    3737   45.53 8.81e-07 ***\nspecies      2   3702    1851   22.55 4.70e-06 ***\nResiduals   22   1806      82                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnova Table (Type III tests)\n\nResponse: height\n             Sum Sq Df F value    Pr(&gt;F)    \n(Intercept) 28097.6  1 342.337 6.734e-15 ***\nmass         1321.8  1  16.104 0.0005843 ***\nspecies      3702.4  2  22.555 4.699e-06 ***\nResiduals    1805.7 22                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n species effect\nspecies\n  Gungan    Human  Wookiee \n214.4986 180.5408 215.5526 \n\n Lower 95 Percent Confidence Limits\nspecies\n  Gungan    Human  Wookiee \n201.0113 176.5022 200.0533 \n\n Upper 95 Percent Confidence Limits\nspecies\n  Gungan    Human  Wookiee \n227.9859 184.5794 231.0520 \n[1] 6.503427 1.947367 7.473624\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: aov(formula = height ~ species + mass, data = data)\n\nLinear Hypotheses:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \nHuman - Gungan == 0    -33.958      6.748  -5.033   &lt;0.001 ***\nWookiee - Gungan == 0    1.054     10.333   0.102    0.994    \nWookiee - Human == 0    35.012      7.846   4.462   &lt;0.001 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\n     Simultaneous Confidence Intervals\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: aov(formula = height ~ species + mass, data = data)\n\nQuantile = 2.4831\n95% family-wise confidence level\n \n\nLinear Hypotheses:\n                      Estimate lwr      upr     \nHuman - Gungan == 0   -33.9578 -50.7126 -17.2030\nWookiee - Gungan == 0   1.0541 -24.6044  26.7125\nWookiee - Human == 0   35.0118  15.5302  54.4934\n\nterm      |    sumsq |   meansq | df | statistic | p.value | etasq | partial.etasq | omegasq | partial.omegasq | epsilonsq | cohens.f | power\n---------------------------------------------------------------------------------------------------------------------------------------------\nspecies   | 3702.445 | 1851.223 |  2 |    22.555 |  &lt; .001 | 0.542 |         0.672 |   0.512 |           0.624 |     0.518 |    1.432 | 1.000\nmass      | 1321.785 | 1321.785 |  1 |    16.104 |   0.001 | 0.194 |         0.423 |   0.179 |           0.367 |     0.182 |    0.856 | 0.979\nResiduals | 1805.669 |   82.076 | 22 |           |         |       |               |         |                 |           |          |"
  },
  {
    "objectID": "ANOVA.html#assumptions-of-ancova",
    "href": "ANOVA.html#assumptions-of-ancova",
    "title": "R Workshop: ANOVA, ANCOVA & Factorial ANOVA",
    "section": "Assumptions of ANCOVA",
    "text": "Assumptions of ANCOVA\nANCOVA requires the same assumptions as ANOVA with two additional assumptions: 1) That the predictor and covariate are independent 2) The regression slopes are homogeneous\nWe will test each of the traditional parametric tests as well as the two additional assumptions below.\n\nPredictor x Covariate Indepenence\n\nStatistical\n\n1predictor_assumption &lt;- aov(mass ~ species, data = data)\nsummary(predictor_assumption)\n\n\n1\n\nTo assess this statistically, one needs to run an ANOVA looking at each predictor with each additional parameter. We do not want these to be statistically significant.\n\n\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nspecies      2   3390  1695.1   4.694 0.0195 *\nResiduals   23   8306   361.1                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nTip\n\n\n\nHere the statistical test for this assumption is statistically significant (which is bad). It actually means we can’t do this (unless we have random assignment)\n\n\n\n\n\nHomogeneity of Regression Slopes\n\nStatistical\n\nregression_slope_assumption &lt;- aov(height ~ species*mass, data = data)\n1car::Anova(regression_slope_assumption, type = \"III\")\n\n\n1\n\nTo test this assumption, one just needs to test the interaction effects within the model. The code for this is shown here.\n\n\n\n\nAnova Table (Type III tests)\n\nResponse: height\n              Sum Sq Df F value  Pr(&gt;F)  \n(Intercept)   149.72  1  1.9153 0.18163  \nspecies       155.01  2  0.9914 0.38856  \nmass          392.00  1  5.0144 0.03666 *\nspecies:mass  242.18  2  1.5490 0.23689  \nResiduals    1563.48 20                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor the above example, the interaction is NOT significant so assumption is met\n\n\n\n\n\nResidual Normality (DV ~ IV)\nResidual normality is an assumption shared with standard regression. We can assess it the same way we did previously when looking at simple ANOVA. The same is true for the homogeneity of variance assumption.\n\nStatistical\n\n1height_mass_resid &lt;- scale(residuals(aov(height ~ mass, data = data)))\n2psych::describe(height_mass_resid)\n3shapiro.test(height_mass_resid)\n\n\n1\n\nThe scale() function will scale the residuals of the ANOVA. The residuals() function pulls out the residuals within the ANOVA\n\n2\n\nStatistical summary of the residuals using the describe() function in the psych package\n\n3\n\nStatistical test (Shapiro Wilks) of the residuals using the shapiro.test() function\n\n\n\n\n   vars  n mean sd median trimmed  mad   min max range skew kurtosis  se\nX1    1 26    0  1      0   -0.06 0.82 -1.85 2.7  4.55 0.71     0.44 0.2\n\n    Shapiro-Wilk normality test\n\ndata:  height_mass_resid\nW = 0.95143, p-value = 0.2505\n\n\n\n\nGraphical\n\n1hist(height_mass_resid, col = 'beige',\n     main=\"\", xlab = \"ANCOVA Residuals (z-score)\",\n     probability = TRUE)\ncurve(dnorm(x, mean = mean(height_mass_resid),\n            sd = sd(height_mass_resid)),\n            add = TRUE, lwd = 2, col = 'blue')\n\n\n1\n\nA graphical histogram (with normal distribution overlay) of the model residuals\n\n\n\n\n\n\n\n\n\n\nHomogeneity of Variance\n\nStatistical\n\n# Levene's Test to Assess Equal Variance for Species\n1car::leveneTest(data$height ~ data$species)\n\n\n1\n\nA statistical test (Levene’s test) of the homogeneity assumption using the leveneTest() function in the car package.\n\n\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  2   1.168 0.3288\n      23               \n\n\n\n\nGraphical\n\n# Boxplot Height by Species\n1boxplot(height ~ species,data=data, main=\"Height Variance by Species \",\n   xlab=\"Species\", ylab=\"Height\")\n\n\n1\n\nA visual representation of the homogeneity of variance assumption using a boxplot\n\n\n\n\n\n\n\n\n\n\nLinearity of CV & DV\nThe last assumption is that the CV and DV are linearly related. We can test this using the code below graphically.\n\nGraphical\n\n1plot(lm(data$height ~ data$mass,data = data),\n     pch = 16, bty = 'l',2)\n\n\n1\n\nVisual representation of the CV to DV linearity assumption"
  },
  {
    "objectID": "ANOVA.html#introduction-to-factorial-anova",
    "href": "ANOVA.html#introduction-to-factorial-anova",
    "title": "R Workshop: ANOVA, ANCOVA & Factorial ANOVA",
    "section": "Introduction to Factorial ANOVA",
    "text": "Introduction to Factorial ANOVA"
  },
  {
    "objectID": "ANOVA.html#running-factorial-anova",
    "href": "ANOVA.html#running-factorial-anova",
    "title": "R Workshop: ANOVA, ANCOVA & Factorial ANOVA",
    "section": "Running Factorial ANOVA",
    "text": "Running Factorial ANOVA\n\nlibrary(tidyverse)\n1factorial_data &lt;- starwars %&gt;%\n2  select(mass,homeworld,species) %&gt;%\n3  mutate(homeworld = as.factor(homeworld),\n4         species = as.factor(species)) %&gt;%\n5  filter(homeworld == \"Tatooine\" | homeworld == \"Naboo\") %&gt;%\n6  filter(species == \"Human\" | species == \"Droid\") %&gt;% na.omit()\n# Modeling As Factorial ANOVA\n7aov_factorial &lt;- aov(mass ~ species*homeworld, data = factorial_data)\n8Anova(aov_factorial, type = \"III\")\n\n# Modeling As A Regression (For SS Analyses)\n9reg_fanova &lt;- lm(mass ~ homeworld*species,data = factorial_data)\n10summary(reg_fanova)\n\n\n1\n\nStart with the starwars data set\n\n2\n\nUse the select() function to pull out mass, homeworld and specices variables\n\n3\n\nCall the mutate() function to format the homeworld variable using the as.factor() function.\n\n4\n\nFormat the species variables as a factor using the as.factor() function.\n\n5\n\nUse the filter() function to filter for observations with homeworld = “Tatooine” OR “Naboo”\n\n6\n\nUse the filter() function to filter for observations with species = “Human” OR “Droid”\n\n7\n\nCreate an ANOVA object using the aov() function\n\n8\n\nUse the Anova() function from the car() package to specifiy Type III Sums of Squares\n\n9\n\nCreate an Regression object using the lm() function\n\n10\n\nShow output of the regression object using the summary() function.\n\n\n\n\nAnova Table (Type III tests)\n\nResponse: mass\n                  Sum Sq Df F value Pr(&gt;F)\n(Intercept)       1024.0  1  1.6199 0.2388\nspecies            990.1  1  1.5662 0.2461\nhomeworld          308.2  1  0.4875 0.5048\nspecies:homeworld   19.0  1  0.0301 0.8666\nResiduals         5057.2  8               \n\nCall:\nlm(formula = mass ~ homeworld * species, data = factorial_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-23.33 -19.50  -6.00  17.88  40.00 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                      32.000     25.143   1.273    0.239\nhomeworldTatooine                21.500     30.793   0.698    0.505\nspeciesHuman                     36.333     29.032   1.251    0.246\nhomeworldTatooine:speciesHuman    6.167     35.557   0.173    0.867\n\nResidual standard error: 25.14 on 8 degrees of freedom\nMultiple R-squared:  0.5219,    Adjusted R-squared:  0.3426 \nF-statistic:  2.91 on 3 and 8 DF,  p-value: 0.1009"
  },
  {
    "objectID": "ANOVA.html#assumptions-of-factorial-anova",
    "href": "ANOVA.html#assumptions-of-factorial-anova",
    "title": "R Workshop: ANOVA, ANCOVA & Factorial ANOVA",
    "section": "Assumptions of Factorial ANOVA",
    "text": "Assumptions of Factorial ANOVA\n\nLinearity of IV to DV (Regression)\n\nGraphical\n\nplot(lm(mass ~ species, data = factorial_data),2)\n\n\n2\n\nPlot the residuals of the model using the plot() and lm() functions for mass on homeworld\n\n\n\n\n\n\n2plot(lm(mass ~ homeworld, data = factorial_data),2)\n\n\n\n\n\n\nStatistical\n\n1psych::describe(resid(aov_factorial))\n2shapiro.test(resid(aov_factorial))\n\n\n1\n\nStatistical summary of the model residuals using the describe() function in the psych package\n\n2\n\nStatistical test (Shapiro Wilk) of the residuals using the resid() and shapiro.test() functions\n\n\n\n\n   vars  n mean    sd median trimmed   mad    min max range skew kurtosis   se\nX1    1 12    0 21.44     -6   -1.67 22.61 -23.33  40 63.33 0.43     -1.4 6.19\n\n    Shapiro-Wilk normality test\n\ndata:  resid(aov_factorial)\nW = 0.90166, p-value = 0.1666\n\n\n\n\n\nHomogeneity of Variance (Regression)\n\nGraphical\n\nboxplot(factorial_data$mass ~ factorial_data$species)\n\n\n3\n\nFitted object of the ANCOVA\n\n4\n\nPlot of the residual x fitted data using the plot() function\n\n\n\n\n\n\nboxplot(factorial_data$mass ~ factorial_data$homeworld)\n\n\n\n# Residuals\n3f_anova_residuals &lt;- scale(residuals(lm(mass ~ species*homeworld, data = factorial_data)))\nf_anova_fitted &lt;- fitted(lm(mass ~ species*homeworld, data = factorial_data))\n\n4plot(f_anova_residuals ~ f_anova_fitted)\n\n\n\n\n\n\nStatistical\n\n1leveneTest(factorial_data$mass ~ factorial_data$species)\n\n2leveneTest(factorial_data$mass ~ factorial_data$homeworld)\n\n3leveneTest(factorial_data$mass ~ interaction(factorial_data$homeworld,factorial_data$species))\n\n\n1\n\nLevene test of mass x species using leveneTest() function\n\n2\n\nLevene test of mass x homeworld using leveneTest() function\n\n3\n\nLevene test of mass x species/homeworld interaction using leveneTest() function\n\n\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  0.0367  0.852\n      10               \nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1   1e-04 0.9919\n      10               \nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  3  0.3306 0.8037\n       8               \n\n\n\n\n\nNormallity of Residuals\n\nGraphical\n\n1plot(aov_factorial,2)\n\n\n1\n\nPlot ANOVA residuals using plot() function\n\n\n\n\n\n\n\n\n\nStatistical\n\n1fanova_residuals &lt;- aov_factorial$residuals\n\n2shapiro.test(fanova_residuals)\n\n3psych::describe(fanova_residuals)\n\n\n1\n\nCreate residual data set\n\n2\n\nStatistical test of residual normality using shapiro.test() function\n\n3\n\nDescriptive statistics of residuals using describe() function in the psych package\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  fanova_residuals\nW = 0.90166, p-value = 0.1666\n\n   vars  n mean    sd median trimmed   mad    min max range skew kurtosis   se\nX1    1 12    0 21.44     -6   -1.67 22.61 -23.33  40 63.33 0.43     -1.4 6.19"
  },
  {
    "objectID": "factor_analysis.html",
    "href": "factor_analysis.html",
    "title": "R Workshop: Factor Analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(pastecs)\nlibrary(GPArotation)\nlibrary(psych)\nset.seed(10311993)\n\n1data &lt;- psych::bfi\n\nproposed_scale &lt;- psych::bfi[,1:15]\n\nproposed_scale &lt;- proposed_scale %&gt;% na.omit()\n\nproposed_scale &lt;- proposed_scale[sample(nrow(proposed_scale), size=500),]\n\n2cor_proposed_scale &lt;- cor(proposed_scale, use = \"pairwise.complete.obs\")\n\n3apaTables::apa.cor.table(cor_proposed_scale,filename = \"CorTable.doc\")\n\n# For Readability\n4round(cor(proposed_scale, use = \"pairwise.complete.obs\"),2)\n\n\n1\n\nCreate a data set using the bfi dataset in the psych package\n\n2\n\nCreate a correlation matrix of the bfi items using the cor() function\n\n3\n\nCreate an APA Style correlation output within Word\n\n4\n\nRound correlation matrix to 2 decimal places for readability in R\n\n\n\n\n\n\nMeans, standard deviations, and correlations with confidence intervals\n \n\n  Variable M     SD   1            2            3            4           \n  1. A1    0.01  0.32                                                    \n                                                                         \n  2. A2    0.17  0.36 -.77**                                             \n                      [-.92, -.43]                                       \n                                                                         \n  3. A3    0.18  0.37 -.73**       .85**                                 \n                      [-.91, -.35] [.59, .95]                            \n                                                                         \n  4. A4    0.15  0.32 -.60*        .72**        .69**                    \n                      [-.85, -.13] [.32, .90]   [.28, .89]               \n                                                                         \n  5. A5    0.18  0.37 -.65**       .77**        .90**        .70**       \n                      [-.87, -.21] [.43, .92]   [.73, .97]   [.30, .89]  \n                                                                         \n  6. C1    0.12  0.33 -.20         .36          .23          .31         \n                      [-.65, .35]  [-.19, .73]  [-.32, .66]  [-.24, .71] \n                                                                         \n  7. C2    0.15  0.33 -.24         .38          .21          .38         \n                      [-.67, .31]  [-.17, .75]  [-.34, .65]  [-.16, .75] \n                                                                         \n  8. C3    0.12  0.32 -.28         .34          .18          .22         \n                      [-.69, .28]  [-.21, .73]  [-.37, .63]  [-.33, .66] \n                                                                         \n  9. C4    -0.01 0.38 .33          -.57*        -.43         -.54*       \n                      [-.22, .72]  [-.84, -.08] [-.77, .11]  [-.82, -.03]\n                                                                         \n  10. C5   -0.01 0.36 .33          -.57*        -.49         -.61*       \n                      [-.22, .72]  [-.84, -.08] [-.80, .03]  [-.86, -.14]\n                                                                         \n  11. E1   -0.02 0.38 .42          -.71**       -.75**       -.61*       \n                      [-.11, .77]  [-.89, -.30] [-.91, -.40] [-.85, -.13]\n                                                                         \n  12. E2   -0.03 0.41 .43          -.72**       -.77**       -.65**      \n                      [-.10, .77]  [-.90, -.33] [-.92, -.42] [-.87, -.20]\n                                                                         \n  13. E3   0.17  0.37 -.47         .69**        .80**        .62*        \n                      [-.79, .05]  [.28, .89]   [.48, .93]   [.15, .86]  \n                                                                         \n  14. E4   0.14  0.39 -.45         .66**        .75**        .64**       \n                      [-.78, .08]  [.22, .88]   [.39, .91]   [.19, .87]  \n                                                                         \n  15. E5   0.15  0.35 -.35         .67**        .58*         .57*        \n                      [-.73, .19]  [.24, .88]   [.10, .84]   [.08, .84]  \n                                                                         \n  5            6            7            8            9            10          \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n  .24                                                                          \n  [-.31, .67]                                                                  \n                                                                               \n  .18          .78**                                                           \n  [-.37, .63]  [.44, .92]                                                      \n                                                                               \n  .15          .66**        .74**                                              \n  [-.39, .62]  [.22, .87]   [.38, .91]                                         \n                                                                               \n  -.45         -.81**       -.85**       -.77**                                \n  [-.78, .08]  [-.93, -.50] [-.95, -.60] [-.92, -.42]                          \n                                                                               \n  -.49         -.75**       -.76**       -.70**       .86**                    \n  [-.80, .03]  [-.91, -.38] [-.92, -.41] [-.89, -.30] [.63, .95]               \n                                                                               \n  -.79**       -.32         -.20         -.12         .39          .39         \n  [-.93, -.46] [-.71, .23]  [-.65, .35]  [-.60, .42]  [-.15, .75]  [-.15, .75] \n                                                                               \n  -.81**       -.40         -.28         -.20         .49          .54*        \n  [-.93, -.50] [-.76, .14]  [-.70, .27]  [-.65, .35]  [-.03, .80]  [.04, .83]  \n                                                                               \n  .83**        .30          .21          .10          -.41         -.51        \n  [.55, .94]   [-.25, .70]  [-.34, .65]  [-.44, .58]  [-.76, .13]  [-.81, .01] \n                                                                               \n  .84**        .30          .18          .11          -.41         -.49        \n  [.58, .95]   [-.25, .70]  [-.37, .63]  [-.43, .59]  [-.76, .12]  [-.80, .03] \n                                                                               \n  .59*         .52*         .55*         .47          -.70**       -.69**      \n  [.10, .84]   [.01, .81]   [.06, .83]   [-.06, .79]  [-.89, -.30] [-.89, -.27]\n                                                                               \n  11           12           13         14        \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n  .89**                                          \n  [.70, .96]                                     \n                                                 \n  -.86**       -.89**                            \n  [-.95, -.62] [-.96, -.70]                      \n                                                 \n  -.88**       -.93**       .86**                \n  [-.96, -.66] [-.98, -.80] [.61, .95]           \n                                                 \n  -.72**       -.78**       .69**      .64*      \n  [-.90, -.34] [-.92, -.45] [.28, .89] [.18, .87]\n                                                 \n\nNote. M and SD are used to represent mean and standard deviation, respectively.\nValues in square brackets indicate the 95% confidence interval.\nThe confidence interval is a plausible range of population correlations \nthat could have caused the sample correlation (Cumming, 2014).\n * indicates p &lt; .05. ** indicates p &lt; .01.\n \n\n      A1    A2    A3    A4    A5    C1    C2    C3    C4    C5    E1    E2\nA1  1.00 -0.36 -0.31 -0.19 -0.23  0.00 -0.02 -0.08  0.12  0.10  0.13  0.14\nA2 -0.36  1.00  0.51  0.35  0.40  0.14  0.20  0.19 -0.20 -0.16 -0.26 -0.26\nA3 -0.31  0.51  1.00  0.33  0.59  0.08  0.11  0.10 -0.09 -0.14 -0.28 -0.29\nA4 -0.19  0.35  0.33  1.00  0.35  0.10  0.22  0.06 -0.16 -0.23 -0.16 -0.19\nA5 -0.23  0.40  0.59  0.35  1.00  0.09  0.07  0.09 -0.14 -0.13 -0.29 -0.31\nC1  0.00  0.14  0.08  0.10  0.09  1.00  0.44  0.32 -0.38 -0.32 -0.09 -0.13\nC2 -0.02  0.20  0.11  0.22  0.07  0.44  1.00  0.41 -0.42 -0.31  0.02 -0.02\nC3 -0.08  0.19  0.10  0.06  0.09  0.32  0.41  1.00 -0.35 -0.30  0.04  0.01\nC4  0.12 -0.20 -0.09 -0.16 -0.14 -0.38 -0.42 -0.35  1.00  0.52  0.16  0.22\nC5  0.10 -0.16 -0.14 -0.23 -0.13 -0.32 -0.31 -0.30  0.52  1.00  0.08  0.26\nE1  0.13 -0.26 -0.28 -0.16 -0.29 -0.09  0.02  0.04  0.16  0.08  1.00  0.54\nE2  0.14 -0.26 -0.29 -0.19 -0.31 -0.13 -0.02  0.01  0.22  0.26  0.54  1.00\nE3 -0.08  0.32  0.44  0.26  0.47  0.14  0.12  0.02 -0.06 -0.17 -0.38 -0.43\nE4 -0.09  0.27  0.37  0.30  0.50  0.13  0.06  0.04 -0.10 -0.16 -0.44 -0.55\nE5  0.00  0.34  0.25  0.23  0.24  0.21  0.31  0.25 -0.28 -0.24 -0.30 -0.36\n      E3    E4    E5\nA1 -0.08 -0.09  0.00\nA2  0.32  0.27  0.34\nA3  0.44  0.37  0.25\nA4  0.26  0.30  0.23\nA5  0.47  0.50  0.24\nC1  0.14  0.13  0.21\nC2  0.12  0.06  0.31\nC3  0.02  0.04  0.25\nC4 -0.06 -0.10 -0.28\nC5 -0.17 -0.16 -0.24\nE1 -0.38 -0.44 -0.30\nE2 -0.43 -0.55 -0.36\nE3  1.00  0.47  0.36\nE4  0.47  1.00  0.26\nE5  0.36  0.26  1.00"
  },
  {
    "objectID": "factor_analysis.html#creating-data",
    "href": "factor_analysis.html#creating-data",
    "title": "R Workshop: Factor Analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(pastecs)\nlibrary(GPArotation)\nlibrary(psych)\nset.seed(10311993)\n\n1data &lt;- psych::bfi\n\nproposed_scale &lt;- psych::bfi[,1:15]\n\nproposed_scale &lt;- proposed_scale %&gt;% na.omit()\n\nproposed_scale &lt;- proposed_scale[sample(nrow(proposed_scale), size=500),]\n\n2cor_proposed_scale &lt;- cor(proposed_scale, use = \"pairwise.complete.obs\")\n\n3apaTables::apa.cor.table(cor_proposed_scale,filename = \"CorTable.doc\")\n\n# For Readability\n4round(cor(proposed_scale, use = \"pairwise.complete.obs\"),2)\n\n\n1\n\nCreate a data set using the bfi dataset in the psych package\n\n2\n\nCreate a correlation matrix of the bfi items using the cor() function\n\n3\n\nCreate an APA Style correlation output within Word\n\n4\n\nRound correlation matrix to 2 decimal places for readability in R\n\n\n\n\n\n\nMeans, standard deviations, and correlations with confidence intervals\n \n\n  Variable M     SD   1            2            3            4           \n  1. A1    0.01  0.32                                                    \n                                                                         \n  2. A2    0.17  0.36 -.77**                                             \n                      [-.92, -.43]                                       \n                                                                         \n  3. A3    0.18  0.37 -.73**       .85**                                 \n                      [-.91, -.35] [.59, .95]                            \n                                                                         \n  4. A4    0.15  0.32 -.60*        .72**        .69**                    \n                      [-.85, -.13] [.32, .90]   [.28, .89]               \n                                                                         \n  5. A5    0.18  0.37 -.65**       .77**        .90**        .70**       \n                      [-.87, -.21] [.43, .92]   [.73, .97]   [.30, .89]  \n                                                                         \n  6. C1    0.12  0.33 -.20         .36          .23          .31         \n                      [-.65, .35]  [-.19, .73]  [-.32, .66]  [-.24, .71] \n                                                                         \n  7. C2    0.15  0.33 -.24         .38          .21          .38         \n                      [-.67, .31]  [-.17, .75]  [-.34, .65]  [-.16, .75] \n                                                                         \n  8. C3    0.12  0.32 -.28         .34          .18          .22         \n                      [-.69, .28]  [-.21, .73]  [-.37, .63]  [-.33, .66] \n                                                                         \n  9. C4    -0.01 0.38 .33          -.57*        -.43         -.54*       \n                      [-.22, .72]  [-.84, -.08] [-.77, .11]  [-.82, -.03]\n                                                                         \n  10. C5   -0.01 0.36 .33          -.57*        -.49         -.61*       \n                      [-.22, .72]  [-.84, -.08] [-.80, .03]  [-.86, -.14]\n                                                                         \n  11. E1   -0.02 0.38 .42          -.71**       -.75**       -.61*       \n                      [-.11, .77]  [-.89, -.30] [-.91, -.40] [-.85, -.13]\n                                                                         \n  12. E2   -0.03 0.41 .43          -.72**       -.77**       -.65**      \n                      [-.10, .77]  [-.90, -.33] [-.92, -.42] [-.87, -.20]\n                                                                         \n  13. E3   0.17  0.37 -.47         .69**        .80**        .62*        \n                      [-.79, .05]  [.28, .89]   [.48, .93]   [.15, .86]  \n                                                                         \n  14. E4   0.14  0.39 -.45         .66**        .75**        .64**       \n                      [-.78, .08]  [.22, .88]   [.39, .91]   [.19, .87]  \n                                                                         \n  15. E5   0.15  0.35 -.35         .67**        .58*         .57*        \n                      [-.73, .19]  [.24, .88]   [.10, .84]   [.08, .84]  \n                                                                         \n  5            6            7            8            9            10          \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n  .24                                                                          \n  [-.31, .67]                                                                  \n                                                                               \n  .18          .78**                                                           \n  [-.37, .63]  [.44, .92]                                                      \n                                                                               \n  .15          .66**        .74**                                              \n  [-.39, .62]  [.22, .87]   [.38, .91]                                         \n                                                                               \n  -.45         -.81**       -.85**       -.77**                                \n  [-.78, .08]  [-.93, -.50] [-.95, -.60] [-.92, -.42]                          \n                                                                               \n  -.49         -.75**       -.76**       -.70**       .86**                    \n  [-.80, .03]  [-.91, -.38] [-.92, -.41] [-.89, -.30] [.63, .95]               \n                                                                               \n  -.79**       -.32         -.20         -.12         .39          .39         \n  [-.93, -.46] [-.71, .23]  [-.65, .35]  [-.60, .42]  [-.15, .75]  [-.15, .75] \n                                                                               \n  -.81**       -.40         -.28         -.20         .49          .54*        \n  [-.93, -.50] [-.76, .14]  [-.70, .27]  [-.65, .35]  [-.03, .80]  [.04, .83]  \n                                                                               \n  .83**        .30          .21          .10          -.41         -.51        \n  [.55, .94]   [-.25, .70]  [-.34, .65]  [-.44, .58]  [-.76, .13]  [-.81, .01] \n                                                                               \n  .84**        .30          .18          .11          -.41         -.49        \n  [.58, .95]   [-.25, .70]  [-.37, .63]  [-.43, .59]  [-.76, .12]  [-.80, .03] \n                                                                               \n  .59*         .52*         .55*         .47          -.70**       -.69**      \n  [.10, .84]   [.01, .81]   [.06, .83]   [-.06, .79]  [-.89, -.30] [-.89, -.27]\n                                                                               \n  11           12           13         14        \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n                                                 \n  .89**                                          \n  [.70, .96]                                     \n                                                 \n  -.86**       -.89**                            \n  [-.95, -.62] [-.96, -.70]                      \n                                                 \n  -.88**       -.93**       .86**                \n  [-.96, -.66] [-.98, -.80] [.61, .95]           \n                                                 \n  -.72**       -.78**       .69**      .64*      \n  [-.90, -.34] [-.92, -.45] [.28, .89] [.18, .87]\n                                                 \n\nNote. M and SD are used to represent mean and standard deviation, respectively.\nValues in square brackets indicate the 95% confidence interval.\nThe confidence interval is a plausible range of population correlations \nthat could have caused the sample correlation (Cumming, 2014).\n * indicates p &lt; .05. ** indicates p &lt; .01.\n \n\n      A1    A2    A3    A4    A5    C1    C2    C3    C4    C5    E1    E2\nA1  1.00 -0.36 -0.31 -0.19 -0.23  0.00 -0.02 -0.08  0.12  0.10  0.13  0.14\nA2 -0.36  1.00  0.51  0.35  0.40  0.14  0.20  0.19 -0.20 -0.16 -0.26 -0.26\nA3 -0.31  0.51  1.00  0.33  0.59  0.08  0.11  0.10 -0.09 -0.14 -0.28 -0.29\nA4 -0.19  0.35  0.33  1.00  0.35  0.10  0.22  0.06 -0.16 -0.23 -0.16 -0.19\nA5 -0.23  0.40  0.59  0.35  1.00  0.09  0.07  0.09 -0.14 -0.13 -0.29 -0.31\nC1  0.00  0.14  0.08  0.10  0.09  1.00  0.44  0.32 -0.38 -0.32 -0.09 -0.13\nC2 -0.02  0.20  0.11  0.22  0.07  0.44  1.00  0.41 -0.42 -0.31  0.02 -0.02\nC3 -0.08  0.19  0.10  0.06  0.09  0.32  0.41  1.00 -0.35 -0.30  0.04  0.01\nC4  0.12 -0.20 -0.09 -0.16 -0.14 -0.38 -0.42 -0.35  1.00  0.52  0.16  0.22\nC5  0.10 -0.16 -0.14 -0.23 -0.13 -0.32 -0.31 -0.30  0.52  1.00  0.08  0.26\nE1  0.13 -0.26 -0.28 -0.16 -0.29 -0.09  0.02  0.04  0.16  0.08  1.00  0.54\nE2  0.14 -0.26 -0.29 -0.19 -0.31 -0.13 -0.02  0.01  0.22  0.26  0.54  1.00\nE3 -0.08  0.32  0.44  0.26  0.47  0.14  0.12  0.02 -0.06 -0.17 -0.38 -0.43\nE4 -0.09  0.27  0.37  0.30  0.50  0.13  0.06  0.04 -0.10 -0.16 -0.44 -0.55\nE5  0.00  0.34  0.25  0.23  0.24  0.21  0.31  0.25 -0.28 -0.24 -0.30 -0.36\n      E3    E4    E5\nA1 -0.08 -0.09  0.00\nA2  0.32  0.27  0.34\nA3  0.44  0.37  0.25\nA4  0.26  0.30  0.23\nA5  0.47  0.50  0.24\nC1  0.14  0.13  0.21\nC2  0.12  0.06  0.31\nC3  0.02  0.04  0.25\nC4 -0.06 -0.10 -0.28\nC5 -0.17 -0.16 -0.24\nE1 -0.38 -0.44 -0.30\nE2 -0.43 -0.55 -0.36\nE3  1.00  0.47  0.36\nE4  0.47  1.00  0.26\nE5  0.36  0.26  1.00"
  },
  {
    "objectID": "factor_analysis.html#efa-assumptions",
    "href": "factor_analysis.html#efa-assumptions",
    "title": "R Workshop: Factor Analysis",
    "section": "EFA Assumptions",
    "text": "EFA Assumptions\n\n#Barlett Test for New Scale\n1cortest.bartlett(cor_proposed_scale, n = 500)\n\n#KMO for New Scale\n2KMO(cor_proposed_scale)\n\n#Determinent for New Scale\n3det(cor_proposed_scale)\n\n\n1\n\nRun a Bartlett test on the correlation matrix. Ideally, this should have a p value of less than .05\n\n2\n\nRun a KMO on the proposed correlation matrix. Ideally this is greater than KMO = .90\n\n3\n\nFind the determinant of the correlation matrix. This should be less than .00001\n\n\n\n\n$chisq\n[1] 2225.86\n\n$p.value\n[1] 0\n\n$df\n[1] 105\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = cor_proposed_scale)\nOverall MSA =  0.82\nMSA for each item = \n  A1   A2   A3   A4   A5   C1   C2   C3   C4   C5   E1   E2   E3   E4   E5 \n0.73 0.86 0.85 0.86 0.83 0.83 0.77 0.79 0.76 0.78 0.85 0.80 0.88 0.84 0.85 \n[1] 0.0109611"
  },
  {
    "objectID": "factor_analysis.html#efa-factor-structure",
    "href": "factor_analysis.html#efa-factor-structure",
    "title": "R Workshop: Factor Analysis",
    "section": "EFA Factor Structure",
    "text": "EFA Factor Structure\n\npsych::scree(cor_proposed_scale)\n\n\n3\n\nRun an orthogonal rotation factor analysis using the fa() function\n\n4\n\nPrint the output fit measures using the print.psych() function. The SORT = TRUE argument sorts the factor loading by loading magnitude.\n\n5\n\nRun an oblique rotation factor analysis using the fa() function\n\n6\n\nPrint the output again using the print.psych() function\n\n\n\n\n\n\nfa.parallel(cor_proposed_scale, n.obs = 500)\n\n\n\n# Suggests 4 Factor Solution\n\n# Orthogonal (Non Correlated)\n3orthoFA3 &lt;- fa(r = cor_proposed_scale, nfactors = 4,rotate = 'varimax', use = \"pairwise.complete.obs\")\n#Show All Info\n4print.psych(orthoFA3, sort = TRUE)\n\n# Oblique (Correlated)\n5obliqueFA3 &lt;- fa(r = cor_proposed_scale, nfactors = 4,rotate = 'oblimin', use = \"pairwise.complete.obs\")\n6print.psych(obliqueFA3, sort = TRUE)\n\nParallel analysis suggests that the number of factors =  4  and the number of components =  3 \nFactor Analysis using method =  minres\nCall: fa(r = cor_proposed_scale, nfactors = 4, rotate = \"varimax\", \n    use = \"pairwise.complete.obs\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n   item   MR1   MR2   MR3   MR4   h2   u2 com\nA3    3  0.73  0.05  0.17 -0.19 0.60 0.40 1.3\nA5    5  0.67  0.05  0.27 -0.07 0.53 0.47 1.4\nA2    2  0.57  0.21  0.13 -0.28 0.47 0.53 1.9\nE3   13  0.53  0.07  0.45  0.16 0.51 0.49 2.2\nA4    4  0.43  0.19  0.13 -0.11 0.25 0.75 1.7\nC2    7  0.17  0.70 -0.10  0.12 0.54 0.46 1.2\nC4    9  0.03 -0.69 -0.18  0.23 0.57 0.43 1.4\nC1    6  0.07  0.56  0.08  0.08 0.34 0.66 1.1\nC5   10 -0.05 -0.56 -0.20  0.15 0.38 0.62 1.4\nC3    8  0.11  0.55 -0.10 -0.03 0.33 0.67 1.2\nE5   15  0.28  0.38  0.32  0.10 0.33 0.67 3.0\nE2   12 -0.16 -0.10 -0.83  0.11 0.73 0.27 1.1\nE1   11 -0.22 -0.01 -0.60  0.07 0.42 0.58 1.3\nE4   14  0.42  0.05  0.57  0.08 0.51 0.49 1.9\nA1    1 -0.30 -0.02 -0.02  0.56 0.40 0.60 1.5\n\n                       MR1  MR2  MR3  MR4\nSS loadings           2.24 2.15 1.91 0.60\nProportion Var        0.15 0.14 0.13 0.04\nCumulative Var        0.15 0.29 0.42 0.46\nProportion Explained  0.32 0.31 0.28 0.09\nCumulative Proportion 0.32 0.64 0.91 1.00\n\nMean item complexity =  1.6\nTest of the hypothesis that 4 factors are sufficient.\n\ndf null model =  105  with the objective function =  4.51\ndf of  the model are 51  and the objective function was  0.3 \n\nThe root mean square of the residuals (RMSR) is  0.03 \nThe df corrected root mean square of the residuals is  0.04 \n\nFit based upon off diagonal values = 0.99\nMeasures of factor score adequacy             \n                                                   MR1  MR2  MR3  MR4\nCorrelation of (regression) scores with factors   0.87 0.89 0.88 0.71\nMultiple R square of scores with factors          0.76 0.79 0.78 0.50\nMinimum correlation of possible factor scores     0.51 0.57 0.56 0.00\nFactor Analysis using method =  minres\nCall: fa(r = cor_proposed_scale, nfactors = 4, rotate = \"oblimin\", \n    use = \"pairwise.complete.obs\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n   item   MR1   MR2   MR3   MR4   h2   u2 com\nA3    3  0.78 -0.03 -0.01 -0.01 0.60 0.40 1.0\nA5    5  0.66 -0.02 -0.12  0.10 0.53 0.47 1.1\nA2    2  0.63  0.15  0.00 -0.14 0.47 0.53 1.2\nA4    4  0.43  0.15 -0.03  0.00 0.25 0.75 1.2\nE3   13  0.39  0.01 -0.34  0.30 0.51 0.49 2.9\nC2    7  0.10  0.71  0.18  0.17 0.54 0.46 1.3\nC4    9  0.08 -0.68  0.19  0.21 0.57 0.43 1.4\nC1    6 -0.03  0.57 -0.05  0.11 0.34 0.66 1.1\nC3    8  0.09  0.56  0.16  0.00 0.33 0.67 1.2\nC5   10  0.01 -0.55  0.18  0.11 0.38 0.62 1.3\nE5   15  0.15  0.35 -0.26  0.18 0.33 0.67 2.8\nE2   12  0.02 -0.03  0.86  0.04 0.73 0.27 1.0\nE1   11 -0.11  0.05  0.60 -0.01 0.42 0.58 1.1\nE4   14  0.28 -0.01 -0.50  0.20 0.51 0.49 1.9\nA1    1 -0.46  0.03 -0.03  0.48 0.40 0.60 2.0\n\n                       MR1  MR2  MR3  MR4\nSS loadings           2.33 2.15 1.89 0.53\nProportion Var        0.16 0.14 0.13 0.04\nCumulative Var        0.16 0.30 0.42 0.46\nProportion Explained  0.34 0.31 0.27 0.08\nCumulative Proportion 0.34 0.65 0.92 1.00\n\n With factor correlations of \n      MR1   MR2   MR3   MR4\nMR1  1.00  0.21 -0.44  0.07\nMR2  0.21  1.00 -0.17 -0.02\nMR3 -0.44 -0.17  1.00 -0.08\nMR4  0.07 -0.02 -0.08  1.00\n\nMean item complexity =  1.5\nTest of the hypothesis that 4 factors are sufficient.\n\ndf null model =  105  with the objective function =  4.51\ndf of  the model are 51  and the objective function was  0.3 \n\nThe root mean square of the residuals (RMSR) is  0.03 \nThe df corrected root mean square of the residuals is  0.04 \n\nFit based upon off diagonal values = 0.99\nMeasures of factor score adequacy             \n                                                   MR1  MR2  MR3   MR4\nCorrelation of (regression) scores with factors   0.91 0.89 0.91  0.70\nMultiple R square of scores with factors          0.82 0.80 0.83  0.49\nMinimum correlation of possible factor scores     0.64 0.60 0.65 -0.02\n\n\n\n\n\n\n\n\nTip\n\n\n\nMore often than not, an oblique rotation will be the best fit for your data as it assumes that your items are correlated with one another"
  },
  {
    "objectID": "factor_analysis.html#efa-factor-structure-assumptions",
    "href": "factor_analysis.html#efa-factor-structure-assumptions",
    "title": "R Workshop: Factor Analysis",
    "section": "EFA Factor Structure Assumptions",
    "text": "EFA Factor Structure Assumptions\n\n#Standard Residuals \n1obliqueFA3Residuals &lt;- scale(obliqueFA3$residual)\n#Test Normality\n2shapiro.test(obliqueFA3Residuals)\n#Histogram\n3hist(obliqueFA3Residuals, col = 'lightgrey',\n     main=\"\", xlab = \"EFA Model Residuals, FA = 3 (Oblique)\",\n     probability = TRUE)\ncurve(dnorm(x, mean = mean(obliqueFA3Residuals),\n            sd = sd(obliqueFA3Residuals)),\n            add = TRUE, lwd = 2, col = 'blue')\n\n\n1\n\nAssess the residuals of your desired factor loading solution using the scale() function in combination with extracting the residuals using object$residuals notation.\n\n2\n\nStatistical test of the factor solution residuals using the shapiro.test() function.\n\n3\n\nGraphical depiction of the solution residuals with a normal curve overlay in the color blue\n\n\n\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  obliqueFA3Residuals\nW = 0.46308, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "factor_analysis.html#calculating-reliability",
    "href": "factor_analysis.html#calculating-reliability",
    "title": "R Workshop: Factor Analysis",
    "section": "Calculating Reliability",
    "text": "Calculating Reliability\n\n#Items\n1Factor1&lt;- c(\"A1\",\"A2\",\"A3\",\"A4\",\"A5\")\n2Factor2&lt;- c(\"C1\",\"C2\",\"C3\",\"C4\",\"C5\")\n3Factor3&lt;- c(\"E1\",\"E2\",\"E3\",\"E4\",\"E5\")\n4Overall &lt;- c(\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"E1\",\"E2\",\"E3\",\"E4\",\"E5\")\n\n#Reliability Factor 1\n5psych::alpha(proposed_scale[,Factor1], check.keys = TRUE)\n#Reliability Factor 2\n6psych::alpha(proposed_scale[, Factor2], check.keys = TRUE)\n#Reliablity Factor 3\n7psych::alpha(proposed_scale[, Factor3], check.keys = TRUE)\n#Overall Reliability\n8psych::alpha(proposed_scale[, Overall], check.keys = TRUE)\n\n\n1\n\nCreate a subset of items to represent Factor 1\n\n2\n\nCreate a subset of items to represent Factor 2\n\n3\n\nCreate a subset of items to represent Factor 3\n\n4\n\nCreate a subset of items to represent Overall\n\n5\n\nDetermine the reliability of Factor 1 using the alpha() function in the psych package. check.keys ensures that items that load negatively are reverse coded.\n\n6\n\nDetermine the reliability of Factor 2 using the alpha() function in the psych package. check.keys ensures that items that load negatively are reverse coded.\n\n7\n\nDetermine the reliability of Factor 3 using the alpha() function in the psych package. check.keys ensures that items that load negatively are reverse coded.\n\n8\n\nDetermine the reliability of Overall using the alpha() function in the psych package. check.keys ensures that items that load negatively are reverse coded.\n\n\n\n\n\nReliability analysis   \nCall: psych::alpha(x = proposed_scale[, Factor1], check.keys = TRUE)\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n      0.73      0.74    0.72      0.36 2.8 0.019  4.7 0.92     0.35\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.69  0.73  0.77\nDuhachek  0.69  0.73  0.77\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r\nA1-      0.74      0.75    0.70      0.42 2.9    0.019 0.011  0.38\nA2       0.66      0.67    0.63      0.33 2.0    0.025 0.019  0.32\nA3       0.64      0.65    0.59      0.31 1.8    0.027 0.007  0.35\nA4       0.72      0.73    0.69      0.40 2.7    0.021 0.017  0.38\nA5       0.66      0.68    0.62      0.34 2.1    0.025 0.010  0.34\n\n Item statistics \n      n raw.r std.r r.cor r.drop mean  sd\nA1- 500  0.61  0.60  0.42   0.36  4.6 1.4\nA2  500  0.73  0.75  0.66   0.57  4.8 1.1\nA3  500  0.77  0.78  0.74   0.61  4.6 1.3\nA4  500  0.66  0.64  0.48   0.41  4.7 1.5\nA5  500  0.73  0.73  0.66   0.55  4.5 1.3\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nA1 0.33 0.28 0.14 0.13 0.10 0.02    0\nA2 0.01 0.05 0.05 0.19 0.40 0.30    0\nA3 0.03 0.06 0.08 0.19 0.36 0.27    0\nA4 0.05 0.07 0.07 0.16 0.25 0.41    0\nA5 0.03 0.06 0.09 0.23 0.33 0.26    0\n\nReliability analysis   \nCall: psych::alpha(x = proposed_scale[, Factor2], check.keys = TRUE)\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n      0.75      0.75    0.72      0.38   3 0.018  4.2 0.94     0.36\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.71  0.75  0.78\nDuhachek  0.71  0.75  0.78\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nC1       0.71      0.71    0.67      0.38 2.5    0.021 0.0064  0.38\nC2       0.69      0.70    0.64      0.36 2.3    0.022 0.0061  0.33\nC3       0.72      0.73    0.68      0.40 2.6    0.020 0.0060  0.40\nC4-      0.67      0.68    0.63      0.35 2.2    0.024 0.0035  0.32\nC5-      0.72      0.72    0.66      0.39 2.5    0.021 0.0021  0.39\n\n Item statistics \n      n raw.r std.r r.cor r.drop mean  sd\nC1  500  0.67  0.70  0.57   0.49  4.5 1.2\nC2  500  0.72  0.73  0.63   0.54  4.3 1.3\nC3  500  0.66  0.67  0.54   0.46  4.3 1.3\nC4- 500  0.75  0.75  0.67   0.59  4.4 1.3\nC5- 500  0.73  0.69  0.58   0.50  3.6 1.6\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nC1 0.02 0.04 0.11 0.22 0.39 0.21    0\nC2 0.03 0.09 0.10 0.25 0.34 0.18    0\nC3 0.03 0.08 0.11 0.27 0.35 0.16    0\nC4 0.24 0.32 0.18 0.18 0.07 0.02    0\nC5 0.15 0.19 0.13 0.24 0.14 0.13    0\n\nReliability analysis   \nCall: psych::alpha(x = proposed_scale[, Factor3], check.keys = TRUE)\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n      0.78      0.78    0.75      0.41 3.5 0.015  4.2 1.1     0.41\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.75  0.78  0.81\nDuhachek  0.75  0.78  0.81\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nE1-      0.74      0.73    0.69      0.41 2.8    0.019 0.0102  0.40\nE2-      0.70      0.70    0.65      0.37 2.3    0.022 0.0063  0.37\nE3       0.74      0.73    0.69      0.41 2.8    0.018 0.0144  0.40\nE4       0.73      0.72    0.67      0.40 2.6    0.020 0.0065  0.37\nE5       0.78      0.78    0.73      0.47 3.5    0.016 0.0043  0.46\n\n Item statistics \n      n raw.r std.r r.cor r.drop mean  sd\nE1- 500  0.75  0.73  0.63   0.56  4.0 1.6\nE2- 500  0.81  0.79  0.74   0.65  3.8 1.6\nE3  500  0.72  0.73  0.62   0.55  4.1 1.4\nE4  500  0.75  0.75  0.67   0.59  4.4 1.5\nE5  500  0.60  0.63  0.47   0.41  4.5 1.3\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nE1 0.23 0.23 0.15 0.18 0.11 0.10    0\nE2 0.19 0.23 0.12 0.22 0.14 0.09    0\nE3 0.06 0.10 0.13 0.28 0.28 0.15    0\nE4 0.06 0.10 0.09 0.15 0.34 0.26    0\nE5 0.04 0.06 0.10 0.24 0.32 0.24    0\n\nReliability analysis   \nCall: psych::alpha(x = proposed_scale[, Overall], check.keys = TRUE)\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n      0.82      0.82    0.85      0.23 4.5 0.012  4.3 0.73     0.23\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.79  0.82  0.84\nDuhachek  0.79  0.82  0.84\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r\nA1-      0.82      0.82    0.85      0.25 4.5    0.012 0.021  0.25\nA2       0.80      0.80    0.84      0.22 4.0    0.013 0.023  0.22\nA3       0.80      0.80    0.83      0.22 4.0    0.013 0.020  0.22\nA4       0.81      0.81    0.84      0.23 4.2    0.013 0.023  0.23\nA5       0.80      0.80    0.83      0.22 4.0    0.013 0.020  0.22\nC1       0.81      0.81    0.85      0.24 4.4    0.012 0.022  0.24\nC2       0.81      0.81    0.84      0.24 4.3    0.012 0.021  0.23\nC3       0.81      0.82    0.85      0.24 4.5    0.012 0.020  0.23\nC4-      0.81      0.81    0.84      0.23 4.2    0.013 0.022  0.23\nC5-      0.81      0.81    0.84      0.23 4.2    0.013 0.023  0.23\nE1-      0.81      0.81    0.84      0.23 4.2    0.013 0.020  0.23\nE2-      0.80      0.80    0.83      0.22 4.1    0.013 0.020  0.22\nE3       0.80      0.80    0.84      0.22 4.1    0.013 0.020  0.22\nE4       0.80      0.80    0.84      0.22 4.0    0.013 0.020  0.22\nE5       0.80      0.80    0.84      0.23 4.1    0.013 0.023  0.19\n\n Item statistics \n      n raw.r std.r r.cor r.drop mean  sd\nA1- 500  0.36  0.36  0.28   0.24  4.6 1.4\nA2  500  0.60  0.62  0.59   0.53  4.8 1.1\nA3  500  0.61  0.61  0.59   0.52  4.6 1.3\nA4  500  0.52  0.52  0.46   0.42  4.7 1.5\nA5  500  0.61  0.62  0.60   0.53  4.5 1.3\nC1  500  0.43  0.45  0.39   0.33  4.5 1.2\nC2  500  0.44  0.46  0.42   0.34  4.3 1.3\nC3  500  0.37  0.40  0.33   0.27  4.3 1.3\nC4- 500  0.52  0.52  0.49   0.42  4.4 1.3\nC5- 500  0.53  0.52  0.47   0.41  3.6 1.6\nE1- 500  0.54  0.51  0.46   0.42  4.0 1.6\nE2- 500  0.62  0.59  0.57   0.52  3.8 1.6\nE3  500  0.60  0.59  0.56   0.51  4.1 1.4\nE4  500  0.61  0.60  0.57   0.52  4.4 1.5\nE5  500  0.58  0.58  0.54   0.49  4.5 1.3\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nA1 0.33 0.28 0.14 0.13 0.10 0.02    0\nA2 0.01 0.05 0.05 0.19 0.40 0.30    0\nA3 0.03 0.06 0.08 0.19 0.36 0.27    0\nA4 0.05 0.07 0.07 0.16 0.25 0.41    0\nA5 0.03 0.06 0.09 0.23 0.33 0.26    0\nC1 0.02 0.04 0.11 0.22 0.39 0.21    0\nC2 0.03 0.09 0.10 0.25 0.34 0.18    0\nC3 0.03 0.08 0.11 0.27 0.35 0.16    0\nC4 0.24 0.32 0.18 0.18 0.07 0.02    0\nC5 0.15 0.19 0.13 0.24 0.14 0.13    0\nE1 0.23 0.23 0.15 0.18 0.11 0.10    0\nE2 0.19 0.23 0.12 0.22 0.14 0.09    0\nE3 0.06 0.10 0.13 0.28 0.28 0.15    0\nE4 0.06 0.10 0.09 0.15 0.34 0.26    0\nE5 0.04 0.06 0.10 0.24 0.32 0.24    0\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you have more than one factor, your scale is no longer one (or uni) dimensional. As such, the idea of an “overall” reliability is questionable at best. Further, all reliability estimates are sample dependent. For non-sample dependent metrics, one should consider Item Response Theory (IRT)"
  },
  {
    "objectID": "SEM.html",
    "href": "SEM.html",
    "title": "R Workshop: CFA & Structural Equation Modeling",
    "section": "",
    "text": "set.seed(5212023)\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(psych)\nlibrary(semTools)\nlibrary(semPlot)\n\n1data &lt;- psych::bfi[,16:25]\n\n2cfa_data &lt;- data[sample(nrow(data),300),]\n\n3sem_data &lt;- lavaan::PoliticalDemocracy %&gt;% na.omit()\n\n\n1\n\nCreate overall data for CFA\n\n2\n\nRandomly sample 300 observations from data using sample() function\n\n3\n\nCreate data for SEM using the PoliticalDemocracy data set from the lavaan package. Omit missing data using the na.omit() function"
  },
  {
    "objectID": "moderation_mediation.html",
    "href": "moderation_mediation.html",
    "title": "R Workshop: Mediation and Moderation",
    "section": "",
    "text": "set.seed(10311993)\nlibrary(mediation)\nlibrary(psych)\nlibrary(tidyverse)\n\n# Created Toy Data Set\n# Variance Covariance\nsigma &lt;- rbind(c(1,-0.4,-0.3), c(-0.4,1, 0.7), c(-0.3,0.7,1))\n# Variable Mean\nmu &lt;- c(7, 50, 7) \n# Generate the Multivariate Normal Distribution\ndf &lt;- as.data.frame(mvrnorm(n=100, mu=mu, Sigma=sigma))\ndf &lt;- round(df,0)\ncolnames(df) &lt;- c(\"mediator1\",\"outcome\",\"predictor\")\ndf$condition &lt;- rep(1:2,50)"
  },
  {
    "objectID": "moderation_mediation.html#running-a-moderation-analysis-in-r",
    "href": "moderation_mediation.html#running-a-moderation-analysis-in-r",
    "title": "R Workshop: Mediation and Moderation",
    "section": "Running a Moderation Analysis in R",
    "text": "Running a Moderation Analysis in R\n\n1moderation &lt;- lm(outcome ~ condition*predictor, data = df)\n2summary(moderation)\n\n\n1\n\nCreate a mediation object using the lm() function. The condition*predictor syntax gets you both the main effects of condition and predictor as well as the interaction effect between the two\n\n2\n\nShow a summary of the moderation using the summary() function.\n\n\n\n\n\nCall:\nlm(formula = outcome ~ condition * predictor, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.79555 -0.56073 -0.05061  0.55043  1.71457 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         44.85018    1.68125  26.677  &lt; 2e-16 ***\ncondition           -0.01414    1.06533  -0.013  0.98943    \npredictor            0.76026    0.23452   3.242  0.00163 ** \ncondition:predictor -0.01533    0.14964  -0.102  0.91864    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8027 on 96 degrees of freedom\nMultiple R-squared:  0.5089,    Adjusted R-squared:  0.4936 \nF-statistic: 33.16 on 3 and 96 DF,  p-value: 8.49e-15"
  },
  {
    "objectID": "moderation_mediation.html#running-a-mediation-analysis-in-r",
    "href": "moderation_mediation.html#running-a-mediation-analysis-in-r",
    "title": "R Workshop: Mediation and Moderation",
    "section": "Running a Mediation Analysis in R",
    "text": "Running a Mediation Analysis in R\n\n#Regress M on X\n1outcomeM_fit &lt;- lm(mediator1 ~ condition, data = df)\n2summary(outcomeM_fit)\n\n#Regress Y on M and X\n3outcomeY_fit &lt;- lm(outcome ~ mediator1 + condition, data = df)\n4summary(outcomeY_fit)\n\n#Run Mediation with Bootstrap\n5outcome_fit &lt;- mediation::mediate(outcomeM_fit,\n                                  outcomeY_fit,\n                                  treat = \"condition\",\n                                  mediator = \"mediator1\",\n                                  boot = TRUE,\n                                  sims = 5000)\n#Summary of Mediation\n6summary(outcome_fit)\n\n#Path Coefficients\n7plot(outcome_fit)\n\n\n1\n\nRun a regression of the M (mediator) on X using the lm() function\n\n2\n\nShow output of the M on X regression using the summary() function\n\n3\n\nRun a regression of Y on M and X using the lm() function\n\n4\n\nShow output of the Y on M and X regression using the summary() function\n\n5\n\nRun a mediation using the two regressions above. treat is the name of your X condition. mediator is the name of your mediating variable. Setting boot to TRUE will ensure that your mediation is bootstrapped. Lastly, the sims argument tells R how many samples you wish to bootstrap from. Typically you want ~ 5000 or more.\n\n6\n\nFor a summary of your mediation, use the summary() function. The indirect effect is labeled ACME\n\n7\n\nThe plot() function here will give you a graphical representation of the output above with respect to the range of the confidence interval for each metric. Please note by default this is the 95% confidence interval\n\n\n\n\n\n\n\n\nCall:\nlm(formula = mediator1 ~ condition, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.860 -0.755  0.140  1.140  2.280 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   7.0000     0.3412  20.515   &lt;2e-16 ***\ncondition    -0.1400     0.2158  -0.649    0.518    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.079 on 98 degrees of freedom\nMultiple R-squared:  0.004276,  Adjusted R-squared:  -0.005884 \nF-statistic: 0.4209 on 1 and 98 DF,  p-value: 0.518\n\n\nCall:\nlm(formula = outcome ~ mediator1 + condition, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2245 -0.5522 -0.0769  0.4724  3.4724 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 53.53460    0.74376  71.979  &lt; 2e-16 ***\nmediator1   -0.45066    0.09569  -4.709 8.28e-06 ***\ncondition   -0.30309    0.20487  -1.479    0.142    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.022 on 97 degrees of freedom\nMultiple R-squared:  0.1954,    Adjusted R-squared:  0.1788 \nF-statistic: 11.78 on 2 and 97 DF,  p-value: 2.634e-05\n\n\nCausal Mediation Analysis \n\nNonparametric Bootstrap Confidence Intervals with the Percentile Method\n\n               Estimate 95% CI Lower 95% CI Upper p-value\nACME             0.0631      -0.1217         0.29    0.52\nADE             -0.3031      -0.7098         0.08    0.12\nTotal Effect    -0.2400      -0.6849         0.19    0.28\nProp. Mediated  -0.2629      -6.0955         4.66    0.76\n\nSample Size Used: 100 \n\n\nSimulations: 5000"
  },
  {
    "objectID": "moderation_mediation.html#assumptions-of-moderation-analyses",
    "href": "moderation_mediation.html#assumptions-of-moderation-analyses",
    "title": "R Workshop: Mediation and Moderation",
    "section": "Assumptions of Moderation Analyses",
    "text": "Assumptions of Moderation Analyses\n\n# Residual Normality\n1shapiro.test(residuals(moderation))\n\n# Multicollinearity\n2car::vif(moderation, type = c(\"predictor\"))\n\n# Independence of Errors\n3car::durbinWatsonTest(moderation)\n\n\n1\n\nTest of the residual normality of the moderation using the shapiro.test() function\n\n2\n\nTest of the multicollinearity of the moderation analyses using the vif() function in the car package. Because there is an interaction, you must specify an additional argument of type = c(\"predictor\") to properly account for the interaction effect.\n\n3\n\nTo test the independence of errors assumption, you can do so using the durbinWatsonTest() function from the car package.\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(moderation)\nW = 0.98684, p-value = 0.4272\n\n          GVIF Df GVIF^(1/(2*Df)) Interacts With Other Predictors\ncondition    1  3               1      predictor             --  \npredictor    1  3               1      condition             --  \n lag Autocorrelation D-W Statistic p-value\n   1     -0.02268275      2.029087   0.756\n Alternative hypothesis: rho != 0"
  },
  {
    "objectID": "moderation_mediation.html#assumptions-of-mediation-analyses",
    "href": "moderation_mediation.html#assumptions-of-mediation-analyses",
    "title": "R Workshop: Mediation and Moderation",
    "section": "Assumptions of Mediation Analyses",
    "text": "Assumptions of Mediation Analyses\n\n# Linearity\nplot(lm(outcome ~ predictor, data = df),2)\n\n\n2\n\nTo assess multicollinearity, the best course of action is a simple correlation matrix. You can achieve this using the cor() function for a correlation matrix\n\n\n\n\n\n\nplot(lm(outcome ~ mediator1, data = df),2)\n\n\n\nplot(lm(mediator1 ~ predictor, data = df),2)\n\n\n\n# Multicollinearity\n2cor(df)\n\n            mediator1    outcome   predictor   condition\nmediator1  1.00000000 -0.4210068 -0.38328907 -0.06539201\noutcome   -0.42100683  1.0000000  0.71129322 -0.10692147\npredictor -0.38328907  0.7112932  1.00000000 -0.07432941\ncondition -0.06539201 -0.1069215 -0.07432941  1.00000000"
  },
  {
    "objectID": "moderation_mediation.html#a-moderation-example-using-hayes-process-macro",
    "href": "moderation_mediation.html#a-moderation-example-using-hayes-process-macro",
    "title": "R Workshop: Mediation and Moderation",
    "section": "A Moderation Example Using Hayes PROCESS Macro",
    "text": "A Moderation Example Using Hayes PROCESS Macro\n\n1process(data = df,\n2        y = \"outcome\",\n3        x = \"predictor\",\n4        w = \"mediator1\",\n5        model = 1,\n6        stand = 1)\n\n\n1\n\nAssign your data to the data argument\n\n2\n\nAssign your outcome variable to the y argument\n\n3\n\nAssign your predictor variable to the x argument\n\n4\n\nAssign your moderator to the w argument\n\n5\n\nSet your model argument to 1 for simple moderation\n\n6\n\nThe stand = 1 argument standardizes your output\n\n\n\n\n\n********************* PROCESS for R Version 4.3.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n                 \nModel : 1        \n    Y : outcome  \n    X : predictor\n    W : mediator1\n\nSample size: 100\n\n\n*********************************************************************** \nOutcome Variable: outcome\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.7294    0.5320    0.6141   36.3739    3.0000   96.0000    0.0000\n\nModel: \n              coeff        se         t         p      LLCI      ULCI\nconstant    47.3198    3.6872   12.8336    0.0000   40.0008   54.6389\npredictor    0.5567    0.5256    1.0592    0.2922   -0.4866    1.6001\nmediator1   -0.2975    0.5240   -0.5676    0.5716   -1.3377    0.7427\nInt_1        0.0169    0.0761    0.2222    0.8246   -0.1341    0.1679\n\nProduct terms key:\nInt_1  :  predictor  x  mediator1      \n\nTest(s) of highest order unconditional interaction(s):\n      R2-chng         F       df1       df2         p\nX*W    0.0002    0.0494    1.0000   96.0000    0.8246\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n \nNOTE: Standardized coefficients not available for models with moderators. \n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Hayes PROCESS for R requires that all data is numeric in nature. As such, ensure that any potential factor variables are numeric prior to running the analyses. A failure to do so will result in PROCESS not running."
  },
  {
    "objectID": "moderation_mediation.html#a-mediation-example-using-hayes-process-macro",
    "href": "moderation_mediation.html#a-mediation-example-using-hayes-process-macro",
    "title": "R Workshop: Mediation and Moderation",
    "section": "A Mediation Example Using Hayes PROCESS Macro",
    "text": "A Mediation Example Using Hayes PROCESS Macro\n\n1process(data = df,\n2        y = \"outcome\",\n3        x = \"predictor\",\n4        m = \"mediator1\",\n5        model = 4,\n6        stand = 1,\n7        boot = 5000)\n\n\n1\n\nAssign your data to the data argument\n\n2\n\nAssign your outcome variable to the y argument\n\n3\n\nAssign your predictor variable to the x argument\n\n4\n\nAssign your mediator to the m argument\n\n5\n\nSet your model argument to 4 for simple mediation\n\n6\n\nThe stand = 1 argument standardizes your output\n\n7\n\nThe boot argument specifies the number of samples you wish to bootstrap\n\n\n\n\n\n********************* PROCESS for R Version 4.3.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n                 \nModel : 4        \n    Y : outcome  \n    X : predictor\n    M : mediator1\n\nSample size: 100\n\nRandom seed: 818206\n\n\n*********************************************************************** \nOutcome Variable: mediator1\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.3833    0.1469    0.9975   16.8766    1.0000   98.0000    0.0001\n\nModel: \n              coeff        se         t         p      LLCI      ULCI\nconstant     9.4738    0.6609   14.3352    0.0000    8.1623   10.7852\npredictor   -0.3812    0.0928   -4.1081    0.0001   -0.5654   -0.1971\n\nStandardized coefficients:\n              coeff\npredictor   -0.3833\n\n*********************************************************************** \nOutcome Variable: outcome\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.7292    0.5317    0.6081   55.0760    2.0000   97.0000    0.0000\n\nModel: \n              coeff        se         t         p      LLCI      ULCI\nconstant    46.5259    0.9080   51.2386    0.0000   44.7237   48.3281\npredictor    0.6722    0.0784    8.5694    0.0000    0.5165    0.8279\nmediator1   -0.1824    0.0789   -2.3121    0.0229   -0.3389   -0.0258\n\nStandardized coefficients:\n              coeff\npredictor    0.6446\nmediator1   -0.1740\n\n*********************************************************************** \nBootstrapping progress:\n\n  |                                                                    \n  |                                                              |   0%\n  |                                                                    \n  |                                                              |   1%\n  |                                                                    \n  |&gt;                                                             |   1%\n  |                                                                    \n  |&gt;                                                             |   2%\n  |                                                                    \n  |&gt;&gt;                                                            |   2%\n  |                                                                    \n  |&gt;&gt;                                                            |   3%\n  |                                                                    \n  |&gt;&gt;                                                            |   4%\n  |                                                                    \n  |&gt;&gt;&gt;                                                           |   4%\n  |                                                                    \n  |&gt;&gt;&gt;                                                           |   5%\n  |                                                                    \n  |&gt;&gt;&gt;                                                           |   6%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;                                                          |   6%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;                                                          |   7%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;                                                         |   7%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;                                                         |   8%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;                                                         |   9%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;                                                        |   9%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;                                                        |  10%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                       |  10%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                       |  11%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                       |  12%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                      |  12%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                      |  13%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                      |  14%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                     |  14%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                     |  15%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                    |  15%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                    |  16%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                    |  17%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                   |  17%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                   |  18%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                   |  19%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                  |  19%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                  |  20%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                 |  20%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                 |  21%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                 |  22%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                |  22%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                                |  23%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                               |  23%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                               |  24%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                               |  25%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                              |  25%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                              |  26%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                              |  27%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                             |  27%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                             |  28%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                            |  28%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                            |  29%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                            |  30%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                           |  30%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                           |  31%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                          |  31%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                          |  32%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                          |  33%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                         |  33%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                         |  34%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                         |  35%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                        |  35%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                        |  36%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                       |  36%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                       |  37%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                       |  38%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                      |  38%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                      |  39%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                      |  40%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                     |  40%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                     |  41%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                    |  41%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                    |  42%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                    |  43%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                   |  43%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                   |  44%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                  |  44%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                  |  45%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                  |  46%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                 |  46%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                 |  47%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                 |  48%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                |  48%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                                |  49%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                               |  49%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                               |  50%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                               |  51%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                              |  51%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                              |  52%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                             |  52%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                             |  53%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                             |  54%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                            |  54%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                            |  55%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                            |  56%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                           |  56%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                           |  57%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                          |  57%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                          |  58%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                          |  59%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                         |  59%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                         |  60%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                        |  60%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                        |  61%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                        |  62%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                       |  62%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                       |  63%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                       |  64%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                      |  64%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                      |  65%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                     |  65%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                     |  66%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                     |  67%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                    |  67%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                    |  68%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                    |  69%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                   |  69%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                   |  70%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                  |  70%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                  |  71%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                  |  72%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                 |  72%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                 |  73%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                |  73%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                |  74%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;                |  75%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;               |  75%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;               |  76%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;               |  77%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;              |  77%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;              |  78%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;             |  78%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;             |  79%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;             |  80%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;            |  80%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;            |  81%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;           |  81%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;           |  82%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;           |  83%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;          |  83%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;          |  84%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;          |  85%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         |  85%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         |  86%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        |  86%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        |  87%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        |  88%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       |  88%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       |  89%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       |  90%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;      |  90%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;      |  91%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;     |  91%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;     |  92%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;     |  93%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    |  93%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    |  94%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;   |  94%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;   |  95%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;   |  96%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  |  96%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  |  97%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  |  98%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; |  98%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; |  99%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;|  99%\n  |                                                                    \n  |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;| 100%\n\n**************** DIRECT AND INDIRECT EFFECTS OF X ON Y ****************\n\nDirect effect of X on Y:\n     effect        se         t         p      LLCI      ULCI     c'_cs\n     0.6722    0.0784    8.5694    0.0000    0.5165    0.8279    0.6446\n\nIndirect effect(s) of X on Y:\n             Effect    BootSE  BootLLCI  BootULCI\nmediator1    0.0695    0.0353    0.0100    0.1483\n\nCompletely standardized indirect effect(s) of X on Y:\n             Effect    BootSE  BootLLCI  BootULCI\nmediator1    0.0667    0.0339    0.0097    0.1436\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\nNumber of bootstraps for percentile bootstrap confidence intervals: 5000"
  },
  {
    "objectID": "nonparametric.html",
    "href": "nonparametric.html",
    "title": "R Workshop: Non-Parametric Tests",
    "section": "",
    "text": "set.seed(10311993)\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(palmerpenguins)\n\n1data &lt;- palmerpenguins::penguins %&gt;% dplyr::select(island,body_mass_g) %&gt;%\n2  dplyr::filter(island == \"Biscoe\" | island == \"Dream\") %&gt;% na.omit()\n\n3data &lt;- droplevels(data)\n\n\n1\n\nTake the palmerpenguins data set and use the select function to isolate the island and body_mass_g variables\n\n2\n\nUse the filter() function to select for only observations where the island variable is “Biscoe” or “Dream” and omit any missing data after using the na.omit() function.\n\n3\n\nUse the droplevels() function to ensure factor levels no longer present are excluded from analysis."
  },
  {
    "objectID": "nonparametric.html#independent-sample-t-test-i.e.-wilcoxons-rank-sum-test1",
    "href": "nonparametric.html#independent-sample-t-test-i.e.-wilcoxons-rank-sum-test1",
    "title": "R Workshop: Non-Parametric Tests",
    "section": "Independent Sample T Test (i.e., Wilcoxon’s Rank-Sum Test)1",
    "text": "Independent Sample T Test (i.e., Wilcoxon’s Rank-Sum Test)1\n\n\n\n\n\n\nTip\n\n\n\n\nThis is computationally the same as the Mann-Whitley test (Field et al, 2012)\n\n\n\n\n\n\n\n\n\nDanger\n\n\n\nNon-parametric tests are NOT less powerful than their parametric counterpart as long as the distribution of interest is not normally distributed (Field et al, 2012)\n\n\n\n# Two-Tailed (one-tailed \"greater\" diff is + OR \"lesser\" diff is -)\n# Correction for p value (TRUE by default, but can change to FALSE correction = FALSE)\n1ind_t_nonpar &lt;- wilcox.test(body_mass_g ~ island, data = data)\n2ind_t_nonpar\n\n# Effect Size (from Field et al, 2012)\n\n3rFromWilcox&lt;-function(wilcoxModel, N)\n{\nz&lt;- qnorm(wilcoxModel$p.value/2)\nr&lt;- z/ sqrt(N)\nreturn(r)\n}\n\n4rFromWilcox(ind_t_nonpar,291)\n\n# Typical reporting for non-parametric involves median values\n5descriptive_stats &lt;- data %&gt;%\n  group_by(island) %&gt;%\n6  summarize(group_size = n(),\n            median = median(body_mass_g),\n            mean = round(mean(body_mass_g, na.rm = TRUE),2),\n            sd = round(sd(body_mass_g, na.rm = TRUE),2))\n\n7print(descriptive_stats)\n\n\n1\n\nInitiate a non parametric independent samples t test using the wilcox.test() function.\n\n2\n\nShow the output of the independent sample non-parametric t test\n\n3\n\nCreate a pseudo effect size measure for non parametric tests courtesy of Field et al (2012).\n\n4\n\nGenerate an effect size measure r using the function defined above. The function requires you provide the wilcox.test object and the sample size\n\n5\n\nGenerate some simple descriptive statistics for reporting purposes grouped by island affiliation\n\n6\n\nSeveral statistics included such as sample size, mean, median, and standard deviation.\n\n7\n\nShow the output of the above calculations using the print() function.\n\n\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  body_mass_g by island\nW = 17741, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n[1] -0.610081\n# A tibble: 2 × 5\n  island group_size median  mean    sd\n  &lt;fct&gt;       &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Biscoe        167  4775  4716.  783.\n2 Dream         124  3688. 3713.  417."
  },
  {
    "objectID": "nonparametric.html#dependent-sample-t-test-i.e.-wilcoxon-signed-rank-test",
    "href": "nonparametric.html#dependent-sample-t-test-i.e.-wilcoxon-signed-rank-test",
    "title": "R Workshop: Non-Parametric Tests",
    "section": "Dependent Sample T Test (i.e., Wilcoxon Signed-Rank Test)",
    "text": "Dependent Sample T Test (i.e., Wilcoxon Signed-Rank Test)\n\n1data &lt;- psych::sat.act %&gt;%\n  mutate(condition = rep(c(1,2),350),\n         ACT_2 = if_else(age &lt; 39, ACT + 3, ACT -1))\n\n2data_condition1 &lt;- data %&gt;%\n  filter(condition == 1)\n\n3data_condition2 &lt;- data %&gt;%\n  filter(condition == 2)\n\n4condition1_test &lt;- wilcox.test(data_condition1$ACT,\n                           data_condition1$ACT_2,\n                           paired = TRUE,\n                           correct = FALSE)\n\n5condition2_test &lt;- wilcox.test(data_condition2$ACT,\n                           data_condition2$ACT_2,\n                           paired = TRUE,\n                           correct = FALSE)\n\n6print(condition1_test)\nprint(condition2_test)\n\n# Effect Size\n7rFromWilcox(condition1_test,350)\nrFromWilcox(condition2_test,350)\n\n\n1\n\nCreate a dependent samples t test data set\n\n2\n\nFilter data by condition variable (condition = 1) using the filter() function\n\n3\n\nFilter data by condition variable (condition = 2) using the filter() function\n\n4\n\nRun a wilcox.test() function for condition = 1. You want to include the time1 and time2 measures. The paired = TRUE tells R that this is a dependent sample t test\n\n5\n\nDo the same for the condition = 2 condition.\n\n6\n\nThe print() function will give you the results for the non parametric dependent sample t test\n\n7\n\nUse the function shown earlier by Fields et al (2012) to calculate an effect size. It takes the same arguments as before.\n\n\n\n\n\n    Wilcoxon signed rank test\n\ndata:  data_condition1$ACT and data_condition1$ACT_2\nV = 666, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\n    Wilcoxon signed rank test\n\ndata:  data_condition2$ACT and data_condition2$ACT_2\nV = 666, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n[1] -0.9363034\n[1] -0.9363034"
  },
  {
    "objectID": "nonparametric.html#one-way-anova-i.e.-kruskalwallis-test",
    "href": "nonparametric.html#one-way-anova-i.e.-kruskalwallis-test",
    "title": "R Workshop: Non-Parametric Tests",
    "section": "One Way ANOVA (i.e., Kruskal–Wallis Test)",
    "text": "One Way ANOVA (i.e., Kruskal–Wallis Test)\n\n1data2 &lt;- psych::sat.act[1:600,] %&gt;%\n  mutate(condition = as.factor(rep(c(1:3),200)),\n         rank = rank(ACT))\n\n# Condition 3 is control\n\n2kruskal.test(ACT ~ condition, data = data2)\n\n3pgirmess::kruskalmc(ACT ~ condition, data = data2)\n\n# Maybe we want to see the ranked means\n\n4by(data2$rank,data2$condition,mean)\n\n# Maybe we just want to compare each exp group to the control (Lowers Odds of Type II error).\n# We can do that. But we need it to be the first level factor\n\n5data2$condition &lt;- relevel(data2$condition,3)\n\n6levels(data2$condition)\n\n7pgirmess::kruskalmc(ACT ~ condition, data = data2, cont = \"two-tailed\")\n\n\n1\n\nCreate a dummy data set\n\n2\n\nRun a non parametric ANOVA using the kruskal.test() function\n\n3\n\nRun a posthoc multiple comparisons test using the kruskalmc() function\n\n4\n\nShow “means” using the by() function. It takes the rank and condition variables as well as the mean() function\n\n5\n\nSort levels so that the main comparison condition is first using the relevel() function. It takes your condition variable as well as the factor you want first (in this case 3).\n\n6\n\nThe `levels() function confirms that our ordering is correct (i.e., 3,1,2)\n\n7\n\nUse the kuskalmc() function with the cont argument set to “two-tailed” for planned comparison\n\n\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  ACT by condition\nKruskal-Wallis chi-squared = 2.4489, df = 2, p-value = 0.2939\n\nMultiple comparison test after Kruskal-Wallis \nalpha: 0.05 \nComparisons\n    obs.dif critical.dif stat.signif\n1-2 20.7875     41.49949       FALSE\n1-3 25.4050     41.49949       FALSE\n2-3  4.6175     41.49949       FALSE\ndata2$condition: 1\n[1] 285.1025\n------------------------------------------------------------ \ndata2$condition: 2\n[1] 305.89\n------------------------------------------------------------ \ndata2$condition: 3\n[1] 310.5075\n[1] \"3\" \"1\" \"2\"\nMultiple comparison test after Kruskal-Wallis, treatments vs control (two-tailed) \nalpha: 0.05 \nComparisons\n    obs.dif critical.dif stat.signif\n3-1 25.4050     38.85457       FALSE\n3-2  4.6175     38.85457       FALSE"
  },
  {
    "objectID": "nonparametric.html#repeated-measures-anova-i.e.-friedmans-anova",
    "href": "nonparametric.html#repeated-measures-anova-i.e.-friedmans-anova",
    "title": "R Workshop: Non-Parametric Tests",
    "section": "Repeated Measures ANOVA (i.e., Friedman’s ANOVA)",
    "text": "Repeated Measures ANOVA (i.e., Friedman’s ANOVA)\n\n1data3 &lt;- data2 %&gt;%\n  mutate(ACT_2 = if_else(age &lt; 39, ACT + 3, ACT -1)) %&gt;% select(ACT,ACT_2)\n\n# Test\n2friedman.test(as.matrix(data3))\n\n# Post Hoc\n3pgirmess::friedmanmc(as.matrix(data3))\n\n\n1\n\nCreate a repeated measures ANOVA dummy data set\n\n2\n\nRun a repeated measures non parametric ANOVA using the friedman.test() function. Note that the as.matrix() function is required for the test to run properly.\n\n3\n\nRun a posthoc test using the friedmanmc() function.\n\n\n\n\n\n    Friedman rank sum test\n\ndata:  as.matrix(data3)\nFriedman chi-squared = 377.63, df = 1, p-value &lt; 2.2e-16\n\nMultiple comparisons between groups after Friedman test \nalpha: 0.05 \nComparisons\n    obs.dif critical.dif stat.signif     p.value\n1-2     476     48.00912        TRUE 1.22687e-83\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor the Friedman test, the function requires that ONLY the variables needed for the analysis are included in the data frame"
  },
  {
    "objectID": "SEM.html#confirmatory-factor-analysis",
    "href": "SEM.html#confirmatory-factor-analysis",
    "title": "R Workshop: CFA & Structural Equation Modeling",
    "section": "Confirmatory Factor Analysis",
    "text": "Confirmatory Factor Analysis\n\n# Create CFA Model\ncfa_model &lt;- 'nfactor  =~ N1 + N2 + N3 + N4 + N5\n              ofactor =~ O1 + O2 + O3 + O4 + O5'\n\n1fit_cfa &lt;- cfa(cfa_model, data = cfa_data)\n\n2summary(fit_cfa, fit.measures = TRUE)\n\n3semPaths(fit_cfa,'std')\n\n\n1\n\nRun a CFA on the model above using the cfa() function\n\n2\n\nGenerate CFA output and fit measures using the summary() function with the fit.measures argument set to TRUE\n\n3\n\nCreate a basic path diagram of the CFA model using the semPaths() function with standardized coefficients using the std argument\n\n\n\n\n\n\n\nlavaan 0.6.15 ended normally after 39 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n                                                  Used       Total\n  Number of observations                           284         300\n\nModel Test User Model:\n                                                      \n  Test statistic                               126.828\n  Degrees of freedom                                34\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               785.605\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.875\n  Tucker-Lewis Index (TLI)                       0.834\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4737.244\n  Loglikelihood unrestricted model (H1)      -4673.830\n                                                      \n  Akaike (AIC)                                9516.489\n  Bayesian (BIC)                              9593.117\n  Sample-size adjusted Bayesian (SABIC)       9526.525\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.098\n  90 Percent confidence interval - lower         0.080\n  90 Percent confidence interval - upper         0.117\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.952\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.084\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  nfactor =~                                          \n    N1                1.000                           \n    N2                0.979    0.067   14.513    0.000\n    N3                0.809    0.071   11.478    0.000\n    N4                0.794    0.070   11.382    0.000\n    N5                0.746    0.076    9.796    0.000\n  ofactor =~                                          \n    O1                1.000                           \n    O2               -0.580    0.159   -3.635    0.000\n    O3                1.314    0.250    5.249    0.000\n    O4                0.266    0.125    2.134    0.033\n    O5               -0.799    0.158   -5.051    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  nfactor ~~                                          \n    ofactor          -0.070    0.072   -0.967    0.333\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .N1                0.838    0.109    7.720    0.000\n   .N2                0.759    0.101    7.491    0.000\n   .N3                1.442    0.139   10.387    0.000\n   .N4                1.424    0.137   10.427    0.000\n   .N5                1.921    0.175   10.953    0.000\n   .O1                0.879    0.115    7.630    0.000\n   .O2                2.025    0.177   11.468    0.000\n   .O3                0.595    0.158    3.769    0.000\n   .O4                1.415    0.120   11.787    0.000\n   .O5                1.583    0.147   10.733    0.000\n    nfactor           1.779    0.224    7.947    0.000\n    ofactor           0.491    0.125    3.932    0.000\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor SEM and CFA models, the =~ syntax is used. You can interpret it as an “equals” sign more or less"
  },
  {
    "objectID": "SEM.html#structural-equation-modeling",
    "href": "SEM.html#structural-equation-modeling",
    "title": "R Workshop: CFA & Structural Equation Modeling",
    "section": "Structural Equation Modeling",
    "text": "Structural Equation Modeling\n\n# Create SEM Model\nsem_model &lt;- 'ind60 =~ x1 + x2 + x3\n    dem60 =~ y1 + y2 + y3 + y4\n    dem65 =~ y5 + y6 + y7 + y8\n    dem60 ~ ind60\n    dem65 ~ ind60 + dem60\n    y1 ~~ y5\n    y2 ~~ y4 + y6\n    y3 ~~ y7\n    y4 ~~ y8\n    y6 ~~ y8'\n\n1fit_sem &lt;- sem(sem_model, data = sem_data)\n2summary(fit_sem, standardized = TRUE, fit.measures = TRUE)\n3semPaths(fit_sem,'std')\n\n\n1\n\nRun an SEM model using the sem() function\n\n2\n\nGenerate a summary of the SEM model with standardized results and fit measures using the summary() function with the standardized and fit.measures() arguments set to TRUE\n\n3\n\nGenerate a basic path diagram of the SEM model usign the semPaths() function with standardized coefficients using the std argument.\n\n\n\n\n\n\n\nlavaan 0.6.15 ended normally after 68 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        31\n\n  Number of observations                            75\n\nModel Test User Model:\n                                                      \n  Test statistic                                38.125\n  Degrees of freedom                                35\n  P-value (Chi-square)                           0.329\n\nModel Test Baseline Model:\n\n  Test statistic                               730.654\n  Degrees of freedom                                55\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.995\n  Tucker-Lewis Index (TLI)                       0.993\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1547.791\n  Loglikelihood unrestricted model (H1)      -1528.728\n                                                      \n  Akaike (AIC)                                3157.582\n  Bayesian (BIC)                              3229.424\n  Sample-size adjusted Bayesian (SABIC)       3131.720\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.035\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA &lt;= 0.050                    0.611\n  P-value H_0: RMSEA &gt;= 0.080                    0.114\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.044\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  ind60 =~                                                              \n    x1                1.000                               0.670    0.920\n    x2                2.180    0.139   15.742    0.000    1.460    0.973\n    x3                1.819    0.152   11.967    0.000    1.218    0.872\n  dem60 =~                                                              \n    y1                1.000                               2.223    0.850\n    y2                1.257    0.182    6.889    0.000    2.794    0.717\n    y3                1.058    0.151    6.987    0.000    2.351    0.722\n    y4                1.265    0.145    8.722    0.000    2.812    0.846\n  dem65 =~                                                              \n    y5                1.000                               2.103    0.808\n    y6                1.186    0.169    7.024    0.000    2.493    0.746\n    y7                1.280    0.160    8.002    0.000    2.691    0.824\n    y8                1.266    0.158    8.007    0.000    2.662    0.828\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  dem60 ~                                                               \n    ind60             1.483    0.399    3.715    0.000    0.447    0.447\n  dem65 ~                                                               \n    ind60             0.572    0.221    2.586    0.010    0.182    0.182\n    dem60             0.837    0.098    8.514    0.000    0.885    0.885\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .y1 ~~                                                                 \n   .y5                0.624    0.358    1.741    0.082    0.624    0.296\n .y2 ~~                                                                 \n   .y4                1.313    0.702    1.871    0.061    1.313    0.273\n   .y6                2.153    0.734    2.934    0.003    2.153    0.356\n .y3 ~~                                                                 \n   .y7                0.795    0.608    1.308    0.191    0.795    0.191\n .y4 ~~                                                                 \n   .y8                0.348    0.442    0.787    0.431    0.348    0.109\n .y6 ~~                                                                 \n   .y8                1.356    0.568    2.386    0.017    1.356    0.338\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .x1                0.082    0.019    4.184    0.000    0.082    0.154\n   .x2                0.120    0.070    1.718    0.086    0.120    0.053\n   .x3                0.467    0.090    5.177    0.000    0.467    0.239\n   .y1                1.891    0.444    4.256    0.000    1.891    0.277\n   .y2                7.373    1.374    5.366    0.000    7.373    0.486\n   .y3                5.067    0.952    5.324    0.000    5.067    0.478\n   .y4                3.148    0.739    4.261    0.000    3.148    0.285\n   .y5                2.351    0.480    4.895    0.000    2.351    0.347\n   .y6                4.954    0.914    5.419    0.000    4.954    0.443\n   .y7                3.431    0.713    4.814    0.000    3.431    0.322\n   .y8                3.254    0.695    4.685    0.000    3.254    0.315\n    ind60             0.448    0.087    5.173    0.000    1.000    1.000\n   .dem60             3.956    0.921    4.295    0.000    0.800    0.800\n   .dem65             0.172    0.215    0.803    0.422    0.039    0.039\n\n\n\n\n\n\n\n\nTip\n\n\n\nAs stated above, for SEM models we want the =~ syntax. For reference, a regression syntax is simply ~ while residuals syntax are ~~. Each of these can as with SEM, be interpreted as an “equals” sign."
  },
  {
    "objectID": "moderation_mediation.html#using-moderation-and-mediation-usings-hayes-process-macro-for-r",
    "href": "moderation_mediation.html#using-moderation-and-mediation-usings-hayes-process-macro-for-r",
    "title": "R Workshop: Mediation and Moderation",
    "section": "Using Moderation and Mediation Usings Hayes PROCESS Macro (for R)",
    "text": "Using Moderation and Mediation Usings Hayes PROCESS Macro (for R)\nClick on the following link to download the R script for the PROCESS macro for R.\n\nsource(\"process.R\")\n\n\n********************* PROCESS for R Version 4.3.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n \nPROCESS is now ready for use.\nCopyright 2020-2023 by Andrew F. Hayes ALL RIGHTS RESERVED\nWorkshop schedule at http://haskayne.ucalgary.ca/CCRAM"
  }
]